<!doctype html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1, shrink-to-fit=no" />
    <meta name="go-import" content="kubevirt.io/kubevirt git https://github.com/kubevirt/kubevirt">
    <meta name="go-import" content="kubevirt.io/containerized-data-importer git https://github.com/kubevirt/containerized-data-importer">
    <meta name="description" content="Virtual Machine Management on Kubernetes
">
    <title>kubevirt.io</title>

    <link rel="apple-touch-icon" sizes="72x72" href="/assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="/assets/favicon/site.webmanifest">
    <link rel="mask-icon" href="/assets/favicon/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#00aba9">
    <meta name="theme-color" content="#ffffff">

    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.0/css/bootstrap.min.css" integrity="sha384-9gVQ4dYFwwWSjIDZnLEWnxCjeSWFphJiwGPXr1jddIhOegiu1FwO5qRGvFXOdJZ4" crossorigin="anonymous">
    <link rel="stylesheet" href="/css/main.css">
    <link rel="canonical" href="https://www.kubevirt.io//search.html">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,600,700" rel="stylesheet">
</head>


  <body>
    <nav class="navbar navbar-expand-lg navbar-dark fixed-top" role="navigation">
        <a class="navbar-brand" href="/">
    <img src="/assets/images/KubeVirt_logo_color.svg" class="navbar-brand-image d-inline-block align-top" alt="KubeVirt.io">
  </a>
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <i class="fas fa-th-large"></i>
  </button>
  <div class="collapse navbar-collapse" id="navbarSupportedContent">
    <ul class="navbar-nav">
      

      
        <li  class="nav-item" >
          <a class="nav-link text-uppercase" href="/blogs/">Blogs</a>
        </li>
      
        <li  class="nav-item" >
          <a class="nav-link text-uppercase" href="/videos/">Videos</a>
        </li>
      
        <li  class="nav-item" >
          <a class="nav-link text-uppercase" href="/docs/">Docs</a>
        </li>
      
        <li  class="nav-item" >
          <a class="nav-link text-uppercase" href="/community/">Community</a>
        </li>
      

    </ul>
  </div>

    </nav>

    <main role="main" style="margin-top: 60px;">
      <form action="/search.html" method="get">
  <label for="search-box">Search</label>
  <input type="text" id="search-box" name="query">
  <input type="submit" value="search">
</form>

<ul id="search-results"></ul>

<script>
  window.store = {
    
      "2018-kubevirt-v0-7-0-html": {
        "title": "Kubevirt V0.7.0",
        "author": "karmab",
        "category": "news",
        "content": "IntroductionKubevirt v0.7.0 was released a few weeks ago and brings a bunch of new features that this blog post will detail.the full list is visible here but we will pick the ones oriented to the end userFeatureshugepages supportTo use hugepages as backing memory, we need to indicate a desired amount of memory (resources.requests.memory) and size of hugepages to use (memory.hugepages.pageSize)apiVersion: kubevirt.io/v1alpha1kind: VirtualMachinemetadata:  name: myvmspec:  domain:    resources:      requests:        memory: \"64Mi\"    memory:      hugepages:        pageSize: \"2Mi\"    disks:    - name: myimage      volumeName: myimage      disk: {}  volumes:    - name: myimage      persistentVolumeClaim:        claimname: myclaimNote that  a node must have pre-allocated hugepages  hugepages size cannot be bigger than requested memory  requested memory must be divisible by hugepages sizesetting network interface model and MAC addressthe following syntax within interfaces section allows us to set both a mac address and network  modelkind: VMspec:  domain:    devices:      interfaces:        - name: red          macAddress: de:ad:00:00:be:af          model: e1000          bridge: {}  networks:  - name: red    pod: {}alternative network models can be  e1000  e1000e  ne2k_pci  pcnet  rtl8139  virtiosetting a disks serial numberThe new keyword serial in the disks section allows us to specify a serial numberapiVersion: kubevirt.io/v1alpha1kind: VirtualMachinemetadata:  name: myvmspec:  domain:    resources:      requests:        memory: \"64Mi\"    disks:    - name: myimage      volumeName: myimage      serial: sn-11223344      disk: {}  volumes:    - name: myimage      persistentVolumeClaim:        claimname: myclaimspecifying the CPU modelSetting the CPU model is possible via spec.domain.cpu.model. The following VM will have a CPU with the Conroe model:metadata:  name: myvmispec:  domain:    cpu:      # this sets the CPU model      model: Conroe...The available models are listed hereAdditionally, we can also use  host-model  host-passthroughvirtctl exposeTo access services listening within vms, we can expose their ports using standard kubernetes services. Alternatively, we can make use of the virtctl binary to achieve the same result:  to expose a cluster ip servicevirtctl expose virtualmachineinstance vmi-ephemeral --name vmiservice --port 27017 --target-port 22  to expose a node port servicevirtctl expose virtualmachineinstance vmi-ephemeral --name nodeport --type NodePort --port 27017 --target-port 22 --node-port 30000  to expose a load balancer servicevirtctl expose virtualmachineinstance vmi-ephemeral --name lbsvc --type LoadBalancer --port 27017 --target-port 3389Kubernetes compatible networking approach (SLIRP)In slirp mode, virtual machines are connected to the network backend using QEMU user networking mode. In this mode, QEMU allocates internal IP addresses to virtual machines and hides them behind NAT.kind: VMspec:  domain:    devices:      interfaces:        - name: red          slirp: {} # connect using SLIRP mode  networks:  - name: red    pod: {}Role aggregation for our rolesEvery KubeVirt installation after version v0.5.1 comes a set of default RBAC cluster roles that can be used to grant users access to VirtualMachineInstances.The kubevirt.io:admin and kubevirt.io:edit ClusterRoles have console and VNC access permissions built into them.ConclusionThis concludes our review of latest kubevirt features. Enjoy them !",
        "url": "/2018/Kubevirt-v0.7.0.html"
      }
      ,
    
      "2018-unit-test-howto-html": {
        "title": "Unit Test Howto",
        "author": "yuvalif",
        "category": "news",
        "content": "There are way too many reasons to write unit tests, but my favorite one is: the freedom to hack, modify and improve the code without fear, and get quick feedback that you are on the right track.Of course, writing good integration tests (the stuff under the tests directory) is the best way to validate that everything works, but unit tests has great value as:  They are much faster to run (~30 seconds in our case)  You get nice coverage reports with coveralls  No need to: make cluster up/sync  Cover corner cases and easier to debug  Some Notes:      We use same frameworks (ginkgo, gomega) for unit testing and integration testing, which means that with the same learning curve, you can develop much more!    “Bang for the Buck” - it usually takes 20% of the time to get to 80% coverage, and 80% of the time to get to 100%. Which mean that you have to use common sense when improving coverage - some code is just fine with 80% coverage (e.g. large files calling some other APIs with little logic), and other would benefit from getting close to 100% (e.g. complex core functionality handling lots of error cases)    Follow the “boy (or girl) scout rule” - every time you enhance/fix some code, add more testing around the existing code as well    Avoid “white box testing”, as this will cause endless maintenance of the test code. Best way to assure that, is to put the test code under a different package than the code under test    Explore coveralls. Not only it will show you the coverage and the overall trend, it will also help you understand which tests are missing. When drilling down into a file, you can see hits per line, and make better decision on what needs to be covered next  FrameworksThere are several frameworks we use to write unit tests:  The tests themselves are written using ginkgo, which is a Behavior-Driven Development (BDD) framework  The library used for assertions in the tests is gomega. It has a very rich set of matchers, so, before you write you own code around the “equal” matcher, check here to see if there is a more expressive assertion you can use  We use GoMock to generate mocks for the different kubevirt interfaces and objects. The command make generate will (among other things) create a file holding the mocked version of our objects and interfaces          Many examples exist in our code on how to use this framework      Also see here for sample code from GoMock        If you need mocks for k8s objects and interfaces, use their framework. They have a tool called client-gen, which generates both the code and the mocks based on the defined APIs          The generated mock interfaces and objects of the k8s client are here. Note that they a use a different mechanism to control the mocked behavior than the one used in GoMock      Mocked actions are more are here        Unit test utilities are placed under testutils  Some integration test utilities are also useful for unit testing, see this file  When testing interfaces, a mock HTTP server is usually needed. For that we use the golang httptest package          gomega also has a package called ghttp that could be used for same purpose      Best Practices and Tipsginkgo  Don’t mix setup and tests, use BeforeEach/JustBeforeEach for setup and It/Specify for tests  Don’t write setup/cleanup code under Describe/Context clause, which is not inside BeforeEach/AfterEach etc.  Make sure that any state change inside an “It” clause, that may impact other tests, is reverted in “AfterEach”  Don’t assume the “It” clauses, which are at the same level, are invoked in any specific order    gomega    Be verbose and use specific matchers. For example, to check that an array has N elements, you can use:    Expect(len(arr)).To(Equal(N))        But a better way would be:    Expect(arr).To(HaveLen(N))        Function Override    Sometimes the code under test is invoking a function which is not mocked. In most cases, this is an indication that the code needs to be refactored, so this function, or its return values, will be passed as part of the API of the code being tested.However, if this refactoring is not possible (or too costly), you can inject your own implementation of this function. The original function should be defined as a closure, and assigned to a global variable. Since functions are 1st class citizens in go, you can assign your implementation to that function variable. More detailed example is here  ",
        "url": "/2018/Unit-Test-Howto.html"
      }
      ,
    
      "2018-run-istio-with-kubevirt-html": {
        "title": "Run Istio With Kubevirt",
        "author": "SchSeba",
        "category": "news",
        "content": "On this blog post, we are going to deploy virtual machines with the KubeVirt project and insert them into the Istio service mesh.Some information about the technologies we are going to use in this blog post.KubernetesProduction-Grade Container Orchestration.Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.Kubeadmkubeadm helps you bootstrap a minimum viable Kubernetes cluster that conforms to best practices.CalicoCalico provides secure network connectivity for containers and virtual machine workloads.Calico creates and manages a flat layer 3 network, assigning each workload a fully routable IP address. Workloads can communicate without IP encapsulation or network address translation for bare metal performance, easier troubleshooting, and better interoperability. In environments that require an overlay, Calico uses IP-in-IP tunneling or can work with other overlay networking such as flannel.KubeVirtVirtualization API for kubernetes in order to manage virtual machinesKubeVirt technology addresses the needs of development teams that have adopted or want to adopt Kubernetes but possess existing Virtual Machine-based workloads that cannot be easily containerized. More specifically, the technology provides a unified development platform where developers can build, modify, and deploy applications residing in both Application Containers as well as Virtual Machines in a common, shared environment.Benefits are broad and significant. Teams with a reliance on existing virtual machine-based workloads are empowered to rapidly containerize applications. With virtualized workloads placed directly in development workflows, teams can decompose them over time while still leveraging remaining virtualized components as is comfortably desired.IstioAn open platform to connect, manage, and secure microservices.Istio provides an easy way to create a network of deployed services with load balancing, service-to-service authentication, monitoring, and more, without requiring any changes in service code. You add Istio support to services by deploying a special sidecar proxy throughout your environment that intercepts all network communication between microservices, configured and managed using Istio’s control plane functionality.Bookinfo applicationA simple application that displays information about a book, similar to a single catalog entry of an online book store. Displayed on the page is a description of the book, book details (ISBN, number of pages, and so on), and a few book reviews.The Bookinfo application is broken into four separate microservices:productpage. The productpage microservice calls the details and reviews microservices to populate the page.details. The details microservice contains book information.reviews. The reviews microservice contains book reviews. It also calls the ratings microservice.ratings. The ratings microservice contains book ranking information that accompanies a book review.Note: This demo is going to be deployed on a kubernetes 1.10 cluster.Requirements  docker  kubeadmFollow this document to install everything we need for the POCDeploymentFor the POC we clone this repoThe repo contains all the configuration we need to deploy KubeVirt and Istio.  kubevirt.yaml  istio-demo-auth.yamlIt also contains the deployment configuration of our sample application.  bookinfo.yaml  bookinfo-gateway.yamlRun the bash scriptcd kubevirt-istio-poc./deploy-istio-poc.shDemo applicationWe are going to use the bookinfo sample application from the istio webpage.The follow yaml will deploy the bookinfo application with a ‘small’ change the details service will run on a virtual machine inside our kubernetes cluster!Note: it will take like 5 minutes for the application to by running inside the virtual machine because we install git and ruby, then clone the istio repo and start the application.POC detailsLets start with the bash script:#!/bin/bashset -xkubeadm init --pod-network-cidr=192.168.0.0/16yes | cp -i /etc/kubernetes/admin.conf $HOME/.kube/configkubectl apply -f https://docs.projectcalico.org/v3.0/getting-started/kubernetes/installation/hosted/kubeadm/1.7/calico.yamlwhile [[ $(kubectl get po -n kube-system | grep kube-dns | grep Running | wc -l) -eq 0 ]]do        echo Calico deployment is no ready yet.        sleep 5doneecho Calico is ready.echo Taint the master node.kubectl taint nodes --all node-role.kubernetes.io/master-echo Deploy kubevirt.kubectl apply -f https://github.com/kubevirt/kubevirt/releases/download/v0.7.0/kubevirt.yamlecho Deploy istio.kubectl apply -f istio-demo-auth.yamlecho Add istio-injection to the default namespace.kubectl label namespace default istio-injection=enabledwhile [[ $(kubectl get po -n istio-system | grep sidecar-injector | grep Running | wc -l) -eq 0 ]]do        echo Istio deployment is no ready yet.        sleep 5doneecho Istio is ready.sleep 20echo Deploy the bookinfo example applicationkubectl apply -f bookinfo.yamlkubectl apply -f bookinfo-gateway.yamlThe follow script create a kubernetes cluster using the kubeadm command, deploy calico as a network CNI and taint the master node (have only one node in the cluster).After the cluster is up the script deploy both istio with mutual TLS and kubevirt projects, it also add the auto injection to the default namespace.At last the script deploy the bookinfo demo application that we change a bit.Lets take a closer look in the virtual machine part inside the bookinfo.yaml file################################################################################################### Details service##################################################################################################apiVersion: v1kind: Servicemetadata:  name: details  labels:    app: detailsspec:  ports:  - port: 9080    name: http  selector:    app: details---apiVersion: kubevirt.io/v1alpha2kind: VirtualMachineInstancemetadata:  creationTimestamp: null  labels:    special: vmi-details    app: details    version: v1  name: vmi-detailsspec:  domain:    devices:      disks:      - disk:          bus: virtio        name: registrydisk        volumeName: registryvolume      - disk:          bus: virtio        name: cloudinitdisk        volumeName: cloudinitvolume      interfaces:      - name: testSlirp        slirp: {}        ports:        - name: http          port: 9080    machine:      type: \"\"    resources:      requests:        memory: 1024M  networks:  - name: testSlirp    pod: {}  terminationGracePeriodSeconds: 0  volumes:  - name: registryvolume    registryDisk:      image: kubevirt/fedora-cloud-registry-disk-demo:latest  - cloudInitNoCloud:      userData: |-        #!/bin/bash        echo \"fedora\" |passwd fedora --stdin        yum install git ruby -y        git clone https://github.com/istio/istio.git        cd istio/samples/bookinfo/src/details/        ruby details.rb 9080 &amp;    name: cloudinitvolumestatus: {}---..........Details:  Create a network of type podnetworks:- name: testSlirp  pod: {}  Create an interface of type slirp and connect it to the pod network by matching the pod network name  Add our application portinterfaces:- name: testSlirp  slirp: {}  ports:  - name: http    port: 9080  Use the cloud init script to download install and run the details application- cloudInitNoCloud:    userData: |-        #!/bin/bash        echo \"fedora\" |passwd fedora --stdin        yum install git ruby -y        git clone https://github.com/istio/istio.git        cd istio/samples/bookinfo/src/details/        ruby details.rb 9080 &amp;    name: cloudinitvolumePOC CheckAfter running the bash script the environment should look like thisNAME                              READY     STATUS    RESTARTS   AGEproductpage-v1-7bbdd59459-w6nwq   2/2       Running   0          1hratings-v1-76dc7f6b9-6n6s9        2/2       Running   0          1hreviews-v1-64545d97b4-tvgl2       2/2       Running   0          1hreviews-v2-8cb9489c6-wjp9x        2/2       Running   0          1hreviews-v3-6bc884b456-hr5bm       2/2       Running   0          1hvirt-launcher-vmi-details-94pb6   3/3       Running   0          1hLets find the istio ingress service port# kubectl get service -n istio-system  | grep istio-ingressgatewayistio-ingressgateway       LoadBalancer   10.97.163.91     &lt;pending&gt;     80:31380/TCP,443:31390/TCP,31400:31400/TCP                            3hThen browse the follow urlhttp://&lt;k8s-node-ip-address&gt;:&lt;istio-ingress-service-port-exposed-by-k8s&gt;/productpageExample:http://10.0.0.1:31380/productpageConclusionsThis POC show how we can use KubeVirt with Istio to integrate the Istio service mesh to virtual machine workloads running inside our kubernetes cluster.",
        "url": "/2018/Run-Istio-with-kubevirt.html"
      }
      ,
    
      "2018-kvm-using-device-plugins-html": {
        "title": "Kvm Using Device Plugins",
        "author": "stu-gott",
        "category": "news",
        "content": "As of Kubernetes 1.10, the Device Plugins API is now in beta! KubeVirt is nowusing this framework to provide hardware acceleration and network devices tovirtual machines. The motivation behind this is that virt-launcher pods are nolonger responsible for creating their own device nodes. Or stated another way:virt-launcher pods no longer require excess privileges just for the purpose ofcreating device nodes.Kubernetes Device Plugin BasicsDevice Plugins consist of two main parts: a server that provides devices andpods that consume them. Each plugin server is used to share a preconfiguredlist of devices local to the node with pods scheduled on that node. Kubernetesmarks each node with the devices it’s capable of sharing, and uses the presenceof such devices when scheduling pods.Device Plugins In KubeVirtProviding DevicesIn KubeVirt virt-handler takes on the role of the device plugin server. When itstarts up on each node, it registers with the Kubernetes Device Plugin API andadvertises KVM and TUN devices.apiVersion: v1kind: Nodemetadata:  ...spec:  ...status:  allocatable:    cpu: \"2\"    devices.kubevirt.io/kvm: \"110\"    devices.kubevirt.io/tun: \"110\"    pods: \"110\"    ...  capacity:    cpu: \"2\"    devices.kubevirt.io/kvm: \"110\"    devices.kubevirt.io/tun: \"110\"    pods: \"110\"    ...In this case advertising 110 KVM or TUN devices is simply an arbitrary defaultbased on the number of pods that node is limited to.Consuming DevicesNow any pod that requests a devices.kubevirt.io/kvm ordevices.kubevirt.io/tun device can only be scheduled on nodes which providethem. On clusters where KubeVirt is deployed this conveniently happens to beall nodes in the cluster that have these physical devices, which normally meansall nodes in the cluster.Here’s an excerpt of what the pod spec looks like in this case.apiVersion: v1kind: Podmetadata:  ...spec:  containers:  - command:    - /entrypoint.sh      ...    name: compute      ...    resources:      limits:        devices.kubevirt.io/kvm: \"1\"        devices.kubevirt.io/tun: \"1\"      requests:        devices.kubevirt.io/kvm: \"1\"        devices.kubevirt.io/tun: \"1\"        memory: \"161679432\"    securityContext:      capabilities:        add:        - NET_ADMIN      privileged: false      runAsUser: 0    ...Of special note is the securityContext stanza. The only special privilegerequired is the NET_ADMIN capability! This is needed by libvirt to set up thedomain’s networking stack.",
        "url": "/2018/KVM-Using-Device-Plugins.html"
      }
      ,
    
      "2018-hello-kubevirt-on-minikube-html": {
        "title": "Hello Kubevirt On Minikube",
        "author": "mhenriks",
        "category": "",
        "content": "In this blog post, we will demonstrate the process for creating and managing virtual machines in Kubernetes with KubeVirt.  We will also go through the process of installing Minikube and KubeVirt on a Fedora 28 workstation.Install KVMMiniKube will create a single node Kubernetes cluster in a KVM virtual machine on our Fedora host.  KVM is also the virtualization technology used by KubeVirt so we have to make sure that the host is configured to support nested virtual machines.  Fedora does not have that feature enabled by default.# install packages$ sudo yum install libvirt-daemon-kvm qemu-kvm# enable nested virtualization# substitute 'kvm_intel' with 'kvm_amd' if your system has an AMD processor$ sudo modprobe -r kvm_intel$ sudo vi /etc/modprobe.d/kvm.conf# uncomment 'options kvm_intel nested=1' and save$ sudo modprobe kvm_intel#verify nested virtualization enabled$ cat /sys/module/kvm_intel/parameters/nestedYInstall KVM2 driver for MinikubeMinikube requires a special driver to manage Docker Machine VMs running in KVM.  KVM2 is the latest iteration of the driver.  Read more about it here# install driver to /usr/local/bin$ curl -LO https://storage.googleapis.com/minikube/releases/latest/docker-machine-driver-kvm2 &amp;&amp; chmod +x docker-machine-driver-kvm2 &amp;&amp; sudo mv docker-machine-driver-kvm2 /usr/local/bin/Install MinikubeMinikube is responsible for creating and managing a local single-node Kubernetes cluster.  It is installed as a single executable.#install minikube to /usr/local/bin$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/Start Minikube$ minikube start --vm-driver kvm2 --network-plugin cniInstall kubectlNow that we have a Kubernetes cluster running, we need some way to communicate with it.  That is where the kubectl CLI comes in.# install kubectl to /usr/local/bin$ curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kubectl &amp;&amp; chmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/# try out the cli# should see similar output$ kubectl get allNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGEkubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   5mYou may be wondering how kubectl knows where to look for the Kubernetes API endpoint.  minikube start actually takes care of creating the kubectl configuration file.  Take a look at ~/.kube/configDeploy KubeVirtGet KubeVirt components running on the Kubernetes cluster.$ kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/v0.7.0/kubevirt.yaml# watch for Kubevirt components to start (bebin with 'virt-')# may take awhile as containers are downloaded and started$ watch kubectl get --all-namespaces pods# eventually output should look something like this (everything with Running status)NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGEkube-system   etcd-minikube                           1/1\tRunning   0          16mkube-system   kube-addon-manager-minikube             1/1\tRunning   0          15mkube-system   kube-apiserver-minikube                 1/1\tRunning   0          15mkube-system   kube-controller-manager-minikube        1/1\tRunning   0          16mkube-system   kube-dns-86f4d74b45-ppp5p               3/3\tRunning   0          16mkube-system   kube-proxy-rjkxl                        1/1\tRunning   0          16mkube-system   kube-scheduler-minikube                 1/1\tRunning   0          16mkube-system   kubernetes-dashboard-5498ccf677-8zmnk   1/1\tRunning   0          16mkube-system   storage-provisioner                     1/1\tRunning   0          16mkube-system   virt-api-7797f95869-dwrrc               1/1\tRunning   0          2mkube-system   virt-api-7797f95869-fqnhk               1/1\tRunning   1          2mkube-system   virt-controller-69cc6b4897-nlffm        1/1\tRunning   0          2mkube-system   virt-controller-69cc6b4897-xsxmt        1/1\tRunning   0          2mkube-system   virt-handler-f7str                      1/1\tRunning   0          2mInstall virtctlVirtctl is the CLI for creating and managing KubeVirt virtual machines.# install virtctl to /usr/local/bin$ curl -Lo virtctl https://github.com/kubevirt/kubevirt/releases/download/v0.7.0/virtctl-v0.7.0-linux-amd64 &amp;&amp; chmod +x virtctl &amp;&amp; sudo mv virtctl /usr/local/binCreate a VMApply manifest for VM.  If you’re curious, download the manifest file locally and take a look.$ kubectl apply -f https://raw.githubusercontent.com/kubevirt/demo/master/manifests/vm.yaml# check that VM successfully created$ kubectl get vms NAME      AGEtestvm    7s# for more detailed info, run$ kubectl get vms -o yaml testvmStart a VMTime to try out virtctl.$ virtctl start testvm# wait for VM to be running$ watch kubectl get pods# confirm with kubectl$ kubectl get vmisNAME      AGEtestvm    4s# for more detailed info$ kubectl get vmis -o yaml testvmConnect to VMThe VM is running CirrOS, which is “a Tiny OS that specializes in running on a cloud.”  Don’t expect anything fancy.  Look here for more info on CirrOS$ virtctl console testvm# note escape sequence of '^]'# login with creds provided and poke around# you are connected to a VM running in Kubernetes!  Pretty cool!# 'exit' to logout# quit virtctl by providing escape sequence '^]Stop a VM$ virtctl stop testvm# wait for termination to complete$ watch kubectl get pods# confirm with kubectl$ kubectl get vmisNo resources found.Delete a VM$ kubectl delete vms testvm# confirm with kubectl$ kubectl get vmsNo resources found.Nest StepsTake a look at the user guide and get involved with the community.",
        "url": "/2018/Hello-KubeVirt-on-Minikube.html"
      }
      ,
    
      "2018-proxy-vm-conclusion-html": {
        "title": "Proxy VM Conclusion",
        "author": "SchSeba",
        "category": "uncategorized",
        "content": "This blog post follow my previous reseach on how to allow vms inside a k8s cluster tp play nice with istio and other sidecars.Research conclusions and network roadmapAfter the deep research about different options/ways to connect VM to pods, we find that all the solution have different pros and cons.All the represented solution need access to kernel modules and have the risk of conflicting with other networking tools.We decided to implement a 100% Kubernetes compatible network approach on the kubevirt project by using the slirp interface qemu provides.This approach let the VM (from a networking perspective) behave like a process. Thus all traffic is going in and out of TCP or UDP sockets. The approach especially needs to avoid to rely on any specific Kernel configurations (like iptables, ebtables, tc, …) in order to not conflict with other Kubernetes networking tools like Istio or multus.This is just an intermediate solution, because it’s shortcomings (unmaintained, unsafe, not performing well)Slirp interfacePros:  vm ack like a process  No external modules needed  No external process needed  Works with any sidecar solution  no rely on any specific Kernel configurations  pod can run without privilegeCons:  poor performance  use userspace network stackIptables onlyPros:  No external modules needed  No external process needed  All the traffic is handled by the kernel user space not involvedCons:  Istio dedicated solution!  Not other process can change the iptables rulesIptables with a nat-proxyPros:  No external modules needed  Works with any sidecar solutionCons:  Not other process can change the iptables rules  External process needed  The traffic is passed to user space  Only support ingress TCP connectionIptables with a trasperent-proxyPros:  other process can change the nat table (this solution works on the mangle table)  better preformance comparing to nat-proxy  Works with any sidecar solutionCons:  Need NET_ADMIN capability for the docker  External process needed  The traffic is passed to user space  Only support ingress TCP connection",
        "url": "/2018/Proxy-vm-conclusion.html"
      }
      ,
    
      "2018-non-dockerized-build-html": {
        "title": "Non Dockerized Build",
        "author": "yuvalif",
        "category": "uncategorized",
        "content": "In this post we will set up an alternative to the existing containerized build system used in KubeVirt.A new makefile will be presented here, which you can for experimenting (if you are brave enough…)Why?Current build system for KubeVirt is done inside docker. This ensures a robust and consistent build environment:  No need to install system dependencies  Controlled versions of these dependencies  Agnostic of local golang environmentSo, in general, you should just use the dockerized build system.Still, there are some drawbacks there:  Tool integration:          Since your tools are not running in the dockerized environment, they may give different outcome than the ones running in the dockerized environment      Invoking any of the dockerized scripts (under hack directory) may be inconsistent with the outside environment (e.g. file path is different than the one on your machine)        Build time: the dockerized build has some small overheads, and some improvements are still needed to make sure that caching work properly and build is optimized  And last, but not least, sometimes it is just hard to resist the tinkering…How?Currently, the Makefile includes targets that address different things: building, dependencies, cluster management, testing etc. - here I tried to modify the minimum which is required for non-containerized build. Anything not related to it, should just be done using the existing Makefile.  Note that cross compilation is not covered here (e.g. building virtctl for mac and windows)PrerequisitesBest place to look for that is in the docker file definition for the build environment: hack/docker-builder/DockerfileNote that not everything from there is needed for building, so the bare minimum on Fedora27 would be:sudo dnf install -y gitsudo dnf install -y libvirt-develsudo dnf install -y golangsudo dnf install -y dockersudo dnf install -y qemu-imgSimilarly to the containerized case, docker is still needed (e.g. all the cluster stuff is done via docker),  and therefore, any docker related preparations are needed as well. This would include running docker on startup and making sure that docker commands does not need root privileges. On Fedora27 this would mean:sudo groupadd dockersudo usermod -aG docker $USERsudo systemctl enable dockersudo systemctl start dockerNow, getting the actual code could be done either via go get (don’t forget to set the GOPATH environment variable):go get -d kubevirt.io/kubevirt/...Or git clone:mkdir -p $GOPATH/src/kubevirt.io/ &amp;&amp; cd $GOPATH/src/kubevirt.io/git clone https://github.com/kubevirt/kubevirtMakefile.nocontainerall: buildbootstrap:    go get -u github.com/onsi/ginkgo/ginkgo    go get -u mvdan.cc/sh/cmd/shfmt    go get -u -d k8s.io/code-generator/cmd/deepcopy-gen    go get -u -d k8s.io/code-generator/cmd/defaulter-gen    go get -u -d k8s.io/code-generator/cmd/openapi-gen    cd ${GOPATH}/src/k8s.io/code-generator/cmd/deepcopy-gen &amp;&amp; git checkout release-1.9 &amp;&amp; go install    cd ${GOPATH}/src/k8s.io/code-generator/cmd/defaulter-gen &amp;&amp; git checkout release-1.9 &amp;&amp; go install    cd ${GOPATH}/src/k8s.io/code-generator/cmd/openapi-gen &amp;&amp; git checkout release-1.9 &amp;&amp; go installgenerate:    ./hack/generate.shapidocs: generate    ./hack/gen-swagger-doc/gen-swagger-docs.sh v1 htmlbuild: check    go install -v ./cmd/... ./pkg/...    ./hack/copy-cmd.shtest: build    go test -v -cover ./pkg/...check:    ./hack/check.shOUT_DIR=./_outTESTS_OUT_DIR=${OUT_DIR}/testsfunctest: build    go build -v ./tests/...    ginkgo build ./tests    mkdir -p ${TESTS_OUT_DIR}/    mv ./tests/tests.test ${TESTS_OUT_DIR}/    ./hack/functests.shcluster-sync: build    ./hack/build-copy-artifacts.sh    ./hack/build-manifests.sh    ./hack/build-docker.sh build    ./cluster/clean.sh    ./cluster/deploy.sh.PHONY: bootstrap generate apidocs build test check functest cluster-syncTargetsTo execute any of the targets use:make -f Makefile.nocontainer &lt;target&gt;File has the following targets:  bootstrap: this is actually part of the prerequisites, but added all golang tool dependencies here, since this is agnostic of the running platform Should be called once          Note that the k8s code generators use specific version      Note that these are not code dependencies, as they are handled by using a vendor directory, as well as the distclean,  deps-install and deps-update targets in the standard makefile        generate: Calling hack/generate.sh script similarly to the standard makefile. It builds all generators (under the tools directory) and use them to generate: test mocks, KubeVirt resources and test yamls  apidocs: this is similar to apidocs target in the standard makefile  build: this is building all product binaries, and then using a script (copy-cmd.sh, should be placed under: hack) to copy the binaries from their standard location into the _out directory, where the cluster management scripts expect them  test: building and running unit testscheck: using similar code to the one used in the standard makefile: formatting files, fixing package imports and calling go vet  functest: building and running integration tests. After tests are built , they are moved to the _out directory so that the standard script for running integration tests would find them  cluster-sync: this is the only “cluster management” target that had to be modified from the standard makefile",
        "url": "/2018/Non-Dockerized-Build.html"
      }
      ,
    
      "2018-research-run-vms-with-istio-service-mesh-html": {
        "title": "Research Run Vms With Istio Service Mesh",
        "author": "SchSeba",
        "category": "uncategorized",
        "content": "In this blog post we are going to talk about istio and virtual machines on top of Kubernetes. Some of the components we are going to use are istio, libvirt, ebtables, iptables, and tproxy. Please review the links provided for an overview and deeper dive into each technologyResearch explanationOur research goal was to give virtual machines running inside pods (kubevirt project) all the benefits kubernetes have to offer, one of them is a service mesh like istio.Iptables only with dnat and source nat configurationThis configuration is istio only!For this solution we created the following architectureWith the follow yaml configurationapiVersion: v1kind: Servicemetadata:  name: application-devel  labels:    app: libvirtd-develspec:  ports:  - port: 9080    name: http  selector:    app: libvirtd-devel---apiVersion: v1kind: Servicemetadata:  name: libvirtd-client-devel  labels:    app: libvirtd-develspec:  ports:  - port: 16509    name: client-connection  - port: 5900    name: spice  - port: 22    name: ssh  selector:    app: libvirtd-devel  type: LoadBalancer---apiVersion: extensions/v1beta1kind: Deploymentmetadata:  creationTimestamp: null  name: libvirtd-develspec:  replicas: 1  strategy: {}  template:    metadata:      annotations:        sidecar.istio.io/status: '{\"version\":\"43466efda2266e066fb5ad36f2d1658de02fc9411f6db00ccff561300a2a3c78\",\"initContainers\":[\"istio-init\",\"enable-core-dump\"],\"containers\":[\"istio-proxy\"],\"volumes\":[\"istio-envoy\",\"istio-certs\"]}'      creationTimestamp: null      labels:        app: libvirtd-devel    spec:      containers:      - image: docker.io/sebassch/mylibvirtd:devel        imagePullPolicy: Always        name: compute        ports:        - containerPort: 9080        - containerPort: 16509        - containerPort: 5900        - containerPort: 22        securityContext:          capabilities:            add:            - ALL          privileged: true          runAsUser: 0        volumeMounts:          - mountPath: /var/lib/libvirt/images            name: test-volume          - mountPath: /host-dev            name: host-dev          - mountPath: /host-sys            name: host-sys        resources: {}        env:          - name: LIBVIRTD_DEFAULT_NETWORK_DEVICE            value: \"eth0\"      - args:        - proxy        - sidecar        - --configPath        - /etc/istio/proxy        - --binaryPath        - /usr/local/bin/envoy        - --serviceCluster        - productpage        - --drainDuration        - 45s        - --parentShutdownDuration        - 1m0s        - --discoveryAddress        - istio-pilot.istio-system:15005        - --discoveryRefreshDelay        - 1s        - --zipkinAddress        - zipkin.istio-system:9411        - --connectTimeout        - 10s        - --statsdUdpAddress        - istio-mixer.istio-system:9125        - --proxyAdminPort        - \"15000\"        - --controlPlaneAuthPolicy        - MUTUAL_TLS        env:        - name: POD_NAME          valueFrom:            fieldRef:              fieldPath: metadata.name        - name: POD_NAMESPACE          valueFrom:            fieldRef:              fieldPath: metadata.namespace        - name: INSTANCE_IP          valueFrom:            fieldRef:              fieldPath: status.podIP        image: docker.io/istio/proxy:0.7.1        imagePullPolicy: IfNotPresent        name: istio-proxy        resources: {}        securityContext:          privileged: false          readOnlyRootFilesystem: true          runAsUser: 1337        volumeMounts:        - mountPath: /etc/istio/proxy          name: istio-envoy        - mountPath: /etc/certs/          name: istio-certs          readOnly: true      initContainers:      - args:        - -p        - \"15001\"        - -u        - \"1337\"        image: docker.io/istio/proxy_init:0.7.1        imagePullPolicy: IfNotPresent        name: istio-init        resources: {}        securityContext:          capabilities:            add:            - NET_ADMIN      - args:        - -c        - sysctl -w kernel.core_pattern=/etc/istio/proxy/core.%e.%p.%t &amp;&amp; ulimit -c          unlimited        command:        - /bin/sh        image: alpine        imagePullPolicy: IfNotPresent        name: enable-core-dump        resources: {}        securityContext:          privileged: true      volumes:      - emptyDir:          medium: Memory        name: istio-envoy      - name: istio-certs        secret:          optional: true          secretName: istio.default      - name: host-dev        hostPath:          path: /dev          type: Directory      - name: host-sys        hostPath:          path: /sys          type: Directory      - name: test-volume        hostPath:          # directory location on host          path: /bricks/brick1/volume/Images          # this field is optional          type: Directorystatus: {}---apiVersion: extensions/v1beta1kind: Ingressmetadata:  name: gateway-devel  annotations:    kubernetes.io/ingress.class: \"istio\"spec:  rules:  - http:      paths:      - path: /devel-myvm        backend:          serviceName: application-devel          servicePort: 9080When the my-libvirt container starts it runs an entry point script for iptables configuration.1. iptables -t nat -D PREROUTING 12. iptables -t nat -A PREROUTING -p tcp -m comment --comment \"Kubevirt Spice\"  --dport 5900 -j ACCEPT3. iptables -t nat -A PREROUTING -p tcp -m comment --comment \"Kubevirt virt-manager\"  --dport 16509 -j ACCEPT4. iptables -t nat  -A PREROUTING -d 10.96.0.0/12 -m comment --comment \"istio/redirect-ip-range-10.96.0.0/12-service cidr\" -j ISTIO_REDIRECT5. iptables -t nat  -A PREROUTING -d 192.168.0.0/16 -m comment --comment \"istio/redirect-ip-range-192.168.0.0/16-Pod cidr\" -j ISTIO_REDIRECT6. iptables -t nat  -A OUTPUT -d 127.0.0.1/32 -p tcp -m comment --comment \"Kubevirt mesh application port\" --dport 9080 -j DNAT --to-destination 10.0.0.27. iptables -t nat  -A POSTROUTING -s 127.0.0.1/32 -d 10.0.0.2/32 -m comment --comment \"Kubevirt VM Forward\" -j SNAT --to-source `ifconfig eth0 | grep inet | awk '{print $2}'Now lets explain every one of this lines:  Remove istio ingress connection rule that send all the ingress traffic directly to the envoy proxy (our vm traffic is ingress traffic for our pod)  Allow ingress connection with spice port to get our libvirt process running in the pod  Allow ingress connection with virt-manager port to get our libvirt process running in the pod  Redirect all the traffic that came from the k8s clusters services to the envoy process  Redirect all the traffic that came from the k8s clusters pods to the envoy process  Send all the traffic that came from envoy process to our vm by changing the destination ip address to ur vm ip address  Change the source ip address of the packet send by envoy from localhost to the pod ip address so the virtual machine can return the connectionIptables configuration conclusionsWith this configuration all the traffic that exit the virtual machine to a k8s service will pass the envoy process and will enter the istio service mash.Also all the traffic that came into the pod will be pass to envoy and after that it will be send to our virtual machineEgress data flow in this solution:Ingress data flow in this solution:Pros:  No external modules needed  No external process needed  All the traffic is handled by the kernel user space not involvedCons:  Istio dedicated solution!  Not other process can change the iptables rulesIptables with a nat-proxy processFor this solution a created the following architectureWith the follow yaml configurationapiVersion: v1kind: Servicemetadata:  name: application-nat-proxt  labels:    app: libvirtd-nat-proxtspec:  ports:  - port: 9080    name: http  selector:    app: libvirtd-nat-proxt  type: LoadBalancer---apiVersion: v1kind: Servicemetadata:  name: libvirtd-client-nat-proxt  labels:    app: libvirtd-nat-proxtspec:  ports:  - port: 16509    name: client-connection  - port: 5900    name: spice  - port: 22    name: ssh  selector:    app: libvirtd-nat-proxt  type: LoadBalancer---apiVersion: extensions/v1beta1kind: Deploymentmetadata:  creationTimestamp: null  name: libvirtd-nat-proxtspec:  replicas: 1  strategy: {}  template:    metadata:      annotations:        sidecar.istio.io/status: '{\"version\":\"43466efda2266e066fb5ad36f2d1658de02fc9411f6db00ccff561300a2a3c78\",\"initContainers\":[\"istio-init\",\"enable-core-dump\"],\"containers\":[\"istio-proxy\"],\"volumes\":[\"istio-envoy\",\"istio-certs\"]}'      creationTimestamp: null      labels:        app: libvirtd-nat-proxt    spec:      containers:      - image: docker.io/sebassch/mylibvirtd:devel        imagePullPolicy: Always        name: compute        ports:        - containerPort: 9080        - containerPort: 16509        - containerPort: 5900        - containerPort: 22        securityContext:          capabilities:            add:            - ALL          privileged: true          runAsUser: 0        volumeMounts:          - mountPath: /var/lib/libvirt/images            name: test-volume          - mountPath: /host-dev            name: host-dev          - mountPath: /host-sys            name: host-sys        resources: {}        env:          - name: LIBVIRTD_DEFAULT_NETWORK_DEVICE            value: \"eth0\"      - image: docker.io/sebassch/mynatproxy:devel        imagePullPolicy: Always        name: proxy        resources: {}        securityContext:          privileged: true          capabilities:            add:            - NET_ADMIN      - args:        - proxy        - sidecar        - --configPath        - /etc/istio/proxy        - --binaryPath        - /usr/local/bin/envoy        - --serviceCluster        - productpage        - --drainDuration        - 45s        - --parentShutdownDuration        - 1m0s        - --discoveryAddress        - istio-pilot.istio-system:15005        - --discoveryRefreshDelay        - 1s        - --zipkinAddress        - zipkin.istio-system:9411        - --connectTimeout        - 10s        - --statsdUdpAddress        - istio-mixer.istio-system:9125        - --proxyAdminPort        - \"15000\"        - --controlPlaneAuthPolicy        - MUTUAL_TLS        env:        - name: POD_NAME          valueFrom:            fieldRef:              fieldPath: metadata.name        - name: POD_NAMESPACE          valueFrom:            fieldRef:              fieldPath: metadata.namespace        - name: INSTANCE_IP          valueFrom:            fieldRef:              fieldPath: status.podIP        image: docker.io/istio/proxy:0.7.1        imagePullPolicy: IfNotPresent        name: istio-proxy        resources: {}        securityContext:          privileged: false          readOnlyRootFilesystem: true          runAsUser: 1337        volumeMounts:        - mountPath: /etc/istio/proxy          name: istio-envoy        - mountPath: /etc/certs/          name: istio-certs          readOnly: true      initContainers:      - args:        - -p        - \"15001\"        - -u        - \"1337\"        - -i        - 10.96.0.0/12,192.168.0.0/16        image: docker.io/istio/proxy_init:0.7.1        imagePullPolicy: IfNotPresent        name: istio-init        resources: {}        securityContext:          capabilities:            add:            - NET_ADMIN      - args:        - -c        - sysctl -w kernel.core_pattern=/etc/istio/proxy/core.%e.%p.%t &amp;&amp; ulimit -c          unlimited        command:        - /bin/sh        image: alpine        imagePullPolicy: IfNotPresent        name: enable-core-dump        resources: {}        securityContext:          privileged: true      volumes:      - emptyDir:          medium: Memory        name: istio-envoy      - name: istio-certs        secret:          optional: true          secretName: istio.default      - name: host-dev        hostPath:          path: /dev          type: Directory      - name: host-sys        hostPath:          path: /sys          type: Directory      - name: test-volume        hostPath:          # directory location on host          path: /bricks/brick1/volume/Images          # this field is optional          type: Directorystatus: {}---apiVersion: extensions/v1beta1kind: Ingressmetadata:  name: gateway-nat-proxt  annotations:    kubernetes.io/ingress.class: \"istio\"spec:  rules:  - http:      paths:      - path: /nat-proxt-myvm        backend:          serviceName: application-nat-proxt          servicePort: 9080When the mynatproxy container starts it runs an entry point script for iptables configuration.1. iptables -t nat -I PREROUTING 1 -p tcp -s 10.0.1.2 -m comment --comment \"nat-proxy redirect\" -j REDIRECT --to-ports 80802. iptables -t nat -I OUTPUT 1 -p tcp -s 10.0.1.2 -j ACCEPT3. iptables -t nat -I POSTROUTING 1 -s 10.0.1.2 -p udp -m comment --comment \"nat udp connections\" -j MASQUERADENow lets explain every one of this lines:  Redirect all the tcp traffic that came from the virtual machine to our proxy on port 8080  Accept all the traffic that go from the pod to the virtual machine  Nat all the udp praffic that came from the virtual machineThis solution uses a container I created that has two processes inside, one for the egress traffic of the virtual machine and one for the ingress traffic.For the egress traffic i used a program writed in golang, and for the ingress traffic I used haproxy.The nat-proxy used a system call to get the original destination address and port that its been redirected to us from the iptable rules I created.The extract function:func getOriginalDst(clientConn *net.TCPConn) (ipv4 string, port uint16, newTCPConn *net.TCPConn, err error) {    if clientConn == nil {        log.Printf(\"copy(): oops, dst is nil!\")        err = errors.New(\"ERR: clientConn is nil\")        return    }    // test if the underlying fd is nil    remoteAddr := clientConn.RemoteAddr()    if remoteAddr == nil {        log.Printf(\"getOriginalDst(): oops, clientConn.fd is nil!\")        err = errors.New(\"ERR: clientConn.fd is nil\")        return    }    srcipport := fmt.Sprintf(\"%v\", clientConn.RemoteAddr())    newTCPConn = nil    // net.TCPConn.File() will cause the receiver's (clientConn) socket to be placed in blocking mode.    // The workaround is to take the File returned by .File(), do getsockopt() to get the original    // destination, then create a new *net.TCPConn by calling net.Conn.FileConn().  The new TCPConn    // will be in non-blocking mode.  What a pain.    clientConnFile, err := clientConn.File()    if err != nil {        log.Printf(\"GETORIGINALDST|%v-&gt;?-&gt;FAILEDTOBEDETERMINED|ERR: could not get a copy of the client connection's file object\", srcipport)        return    } else {        clientConn.Close()    }    // Get original destination    // this is the only syscall in the Golang libs that I can find that returns 16 bytes    // Example result: &amp;{Multiaddr:[2 0 31 144 206 190 36 45 0 0 0 0 0 0 0 0] Interface:0}    // port starts at the 3rd byte and is 2 bytes long (31 144 = port 8080)    // IPv4 address starts at the 5th byte, 4 bytes long (206 190 36 45)    addr, err := syscall.GetsockoptIPv6Mreq(int(clientConnFile.Fd()), syscall.IPPROTO_IP, SO_ORIGINAL_DST)    log.Printf(\"getOriginalDst(): SO_ORIGINAL_DST=%+v\\n\", addr)    if err != nil {        log.Printf(\"GETORIGINALDST|%v-&gt;?-&gt;FAILEDTOBEDETERMINED|ERR: getsocketopt(SO_ORIGINAL_DST) failed: %v\", srcipport, err)        return    }    newConn, err := net.FileConn(clientConnFile)    if err != nil {        log.Printf(\"GETORIGINALDST|%v-&gt;?-&gt;%v|ERR: could not create a FileConn fron clientConnFile=%+v: %v\", srcipport, addr, clientConnFile, err)        return    }    if _, ok := newConn.(*net.TCPConn); ok {        newTCPConn = newConn.(*net.TCPConn)        clientConnFile.Close()    } else {        errmsg := fmt.Sprintf(\"ERR: newConn is not a *net.TCPConn, instead it is: %T (%v)\", newConn, newConn)        log.Printf(\"GETORIGINALDST|%v-&gt;?-&gt;%v|%s\", srcipport, addr, errmsg)        err = errors.New(errmsg)        return    }    ipv4 = itod(uint(addr.Multiaddr[4])) + \".\" +        itod(uint(addr.Multiaddr[5])) + \".\" +        itod(uint(addr.Multiaddr[6])) + \".\" +        itod(uint(addr.Multiaddr[7]))    port = uint16(addr.Multiaddr[2])&lt;&lt;8 + uint16(addr.Multiaddr[3])    return}After we get the original destination address and port we start a connection to it and copy all the packets.var streamWait sync.WaitGroupstreamWait.Add(2)streamConn := func(dst io.Writer, src io.Reader) {    io.Copy(dst, src)    streamWait.Done()}go streamConn(remoteConn, VMconn)go streamConn(VMconn, remoteConn)streamWait.Wait()The Haproxy help us with the ingress traffic with the follow configurationdefaults  mode tcpfrontend main  bind *:9080  default_backend guestbackend guest  server guest 10.0.1.2:9080 maxconn 2048It send all the traffic to our virtual machine on the service port the machine is listening.Code repositorynat proxy conclusionsThis solution is a general solution, not a dedicated solution to istio only. Its make the vm traffic look like a regular process inside the pod so it will work with any sidecars projectsEgress data flow in this solution:Ingress data flow in this solution:Pros:  No external modules needed  Works with any sidecar solutionCons:  Not other process can change the iptables rules  External process needed  The traffic is passed to user space  Only support ingress TCP connectionIptables with a trasperent-proxy processThis is the last solution I used in my research, it use a kernel module named TPROXY The official documentation from the linux kernel documentation.For this solution a created the following architectureWith the follow yaml configurationapiVersion: v1kind: Servicemetadata:  name: application-devel  labels:    app: libvirtd-develspec:  ports:  - port: 9080    name: http  selector:    app: libvirtd-devel  type: LoadBalancer---apiVersion: v1kind: Servicemetadata:  name: libvirtd-client-devel  labels:    app: libvirtd-develspec:  ports:  - port: 16509    name: client-connection  - port: 5900    name: spice  - port: 22    name: ssh  selector:    app: libvirtd-devel  type: LoadBalancer---apiVersion: extensions/v1beta1kind: Deploymentmetadata:  creationTimestamp: null  name: libvirtd-develspec:  replicas: 1  strategy: {}  template:    metadata:      annotations:        sidecar.istio.io/status: '{\"version\":\"43466efda2266e066fb5ad36f2d1658de02fc9411f6db00ccff561300a2a3c78\",\"initContainers\":[\"istio-init\",\"enable-core-dump\"],\"containers\":[\"istio-proxy\"],\"volumes\":[\"istio-envoy\",\"istio-certs\"]}'      creationTimestamp: null      labels:        app: libvirtd-devel    spec:      containers:      - image: docker.io/sebassch/mylibvirtd:devel        imagePullPolicy: Always        name: compute        ports:        - containerPort: 9080        - containerPort: 16509        - containerPort: 5900        - containerPort: 22        securityContext:          capabilities:            add:            - ALL          privileged: true          runAsUser: 0        volumeMounts:          - mountPath: /var/lib/libvirt/images            name: test-volume          - mountPath: /host-dev            name: host-dev          - mountPath: /host-sys            name: host-sys        resources: {}        env:          - name: LIBVIRTD_DEFAULT_NETWORK_DEVICE            value: \"eth0\"      - image: docker.io/sebassch/mytproxy:devel        imagePullPolicy: Always        name: proxy        resources: {}        securityContext:          privileged: true          capabilities:            add:            - NET_ADMIN      - args:        - proxy        - sidecar        - --configPath        - /etc/istio/proxy        - --binaryPath        - /usr/local/bin/envoy        - --serviceCluster        - productpage        - --drainDuration        - 45s        - --parentShutdownDuration        - 1m0s        - --discoveryAddress        - istio-pilot.istio-system:15005        - --discoveryRefreshDelay        - 1s        - --zipkinAddress        - zipkin.istio-system:9411        - --connectTimeout        - 10s        - --statsdUdpAddress        - istio-mixer.istio-system:9125        - --proxyAdminPort        - \"15000\"        - --controlPlaneAuthPolicy        - MUTUAL_TLS        env:        - name: POD_NAME          valueFrom:            fieldRef:              fieldPath: metadata.name        - name: POD_NAMESPACE          valueFrom:            fieldRef:              fieldPath: metadata.namespace        - name: INSTANCE_IP          valueFrom:            fieldRef:              fieldPath: status.podIP        image: docker.io/istio/proxy:0.7.1        imagePullPolicy: IfNotPresent        name: istio-proxy        resources: {}        securityContext:          privileged: false          readOnlyRootFilesystem: true          runAsUser: 1337        volumeMounts:        - mountPath: /etc/istio/proxy          name: istio-envoy        - mountPath: /etc/certs/          name: istio-certs          readOnly: true      initContainers:      - args:        - -p        - \"15001\"        - -u        - \"1337\"        - -i        - 10.96.0.0/12,192.168.0.0/16        image: docker.io/istio/proxy_init:0.7.1        imagePullPolicy: IfNotPresent        name: istio-init        resources: {}        securityContext:          capabilities:            add:            - NET_ADMIN      - args:        - -c        - sysctl -w kernel.core_pattern=/etc/istio/proxy/core.%e.%p.%t &amp;&amp; ulimit -c          unlimited        command:        - /bin/sh        image: alpine        imagePullPolicy: IfNotPresent        name: enable-core-dump        resources: {}        securityContext:          privileged: true      volumes:      - emptyDir:          medium: Memory        name: istio-envoy      - name: istio-certs        secret:          optional: true          secretName: istio.default      - name: host-dev        hostPath:          path: /dev          type: Directory      - name: host-sys        hostPath:          path: /sys          type: Directory      - name: test-volume        hostPath:          # directory location on host          path: /bricks/brick1/volume/Images          # this field is optional          type: Directorystatus: {}---apiVersion: extensions/v1beta1kind: Ingressmetadata:  name: gateway-devel  annotations:    kubernetes.io/ingress.class: \"istio\"spec:  rules:  - http:      paths:      - path: /devel-myvm        backend:          serviceName: application-devel          servicePort: 9080When the tproxy container starts it runs an entry point script for iptables configuration but this time the proxy redirect came in the mangle table and not in the nat table that because TPROXY module avilable only in the mangle table.TPROXYThis target is only valid in the mangle table, in thePREROUTING chain and user-defined chains which are onlycalled from this chain.  It redirects the packet to a localsocket without changing the packet header in any way. It canalso change the mark value which can then be used inadvanced routing rules.iptables rules:iptables -t mangle -vLiptables -t mangle -N KUBEVIRT_DIVERTiptables -t mangle -A KUBEVIRT_DIVERT -j MARK --set-mark 8iptables -t mangle -A KUBEVIRT_DIVERT -j ACCEPTtable=mangleiptables -t ${table} -N KUBEVIRT_INBOUNDiptables -t ${table} -A PREROUTING -p tcp -m comment --comment \"Kubevirt Spice\"  --dport 5900 -j RETURNiptables -t ${table} -A PREROUTING -p tcp -m comment --comment \"Kubevirt virt-manager\"  --dport 16509 -j RETURNiptables -t ${table} -A PREROUTING -p tcp -i vnet0 -j KUBEVIRT_INBOUNDiptables -t ${table} -N KUBEVIRT_TPROXYiptables -t ${table} -A KUBEVIRT_TPROXY ! -d 127.0.0.1/32 -p tcp -j TPROXY --tproxy-mark 8/0xffffffff --on-port 9401#iptables -t mangle -A KUBEVIRT_TPROXY ! -d 127.0.0.1/32 -p udp -j TPROXY --tproxy-mark 8/0xffffffff --on-port 8080# If an inbound packet belongs to an established socket, route it to the# loopback interface.iptables -t ${table} -A KUBEVIRT_INBOUND -p tcp -m socket -j KUBEVIRT_DIVERT#iptables -t mangle -A KUBEVIRT_INBOUND -p udp -m socket -j KUBEVIRT_DIVERT# Otherwise, it's a new connection. Redirect it using TPROXY.iptables -t ${table} -A KUBEVIRT_INBOUND -p tcp -j KUBEVIRT_TPROXY#iptables -t mangle -A KUBEVIRT_INBOUND -p udp -j KUBEVIRT_TPROXYiptables -t ${table} -I OUTPUT 1 -d 10.0.1.2 -j ACCEPTtable=nat# Remove vm Connection from iptables rulesiptables -t ${table} -I PREROUTING 1 -s 10.0.1.2 -j ACCEPTiptables -t ${table} -I OUTPUT 1 -d 10.0.1.2 -j ACCEPT# Allow guest -&gt; world -- using nat for UDPiptables -t ${table} -I POSTROUTING 1 -s 10.0.1.2 -p udp -j MASQUERADEFor this solution we also need to load the bridge kernel modulemodprobe bridgeAnd create some ebtables rules so egress and ingress traffict from the virtial machine will exit the l2 rules and pass to the l3 rules:  ebtables -t broute -F # Flush the table    # inbound traffic    ebtables -t broute -A BROUTING -p IPv4 --ip-dst 10.0.1.2 \\    -j redirect --redirect-target DROP    # returning outbound traffic    ebtables -t broute -A BROUTING -p IPv4 --ip-src 10.0.1.2 \\    -j redirect --redirect-target DROPWe also need to disable rp_filter on the virtual machine interface and the libvirt bridge interfaceecho 0 &gt; /proc/sys/net/ipv4/conf/virbr0/rp_filterecho 0 &gt; /proc/sys/net/ipv4/conf/virbr0-nic/rp_filterecho 0 &gt; /proc/sys/net/ipv4/conf/vnet0/rp_filterAfter this configuration the container start the semi-tproxy process for egress traffic and the haproxy process for the ingress traffic.The semi-tproxy program is a golag program,binding a listener socket with the IP_TRANSPARENT socket optionPreparing a socket to receive connections with TProxy is really no different than what is normally done when setting up a socket to listen for connections. The only difference in the process is before the socket is bound, the IP_TRANSPARENT socket option.syscall.SetsockoptInt(fileDescriptor, syscall.SOL_IP, syscall.IP_TRANSPARENT, 1)About IP_TRANSPARENTIP_TRANSPARENT (since Linux 2.6.24)Setting this boolean option enables transparent proxying onthis socket.  This socket option allows the calling applica‐tion to bind to a nonlocal IP address and operate both as aclient and a server with the foreign address as the localend‐point.  NOTE: this requires that routing be set up ina way that packets going to the foreign address are routedthrough the TProxy box (i.e., the system hosting theapplication that employs the IP_TRANSPARENT socket option).Enabling this socket option requires superuser privileges(the CAP_NET_ADMIN capability).TProxy redirection with the iptables TPROXY target alsorequires that this option be set on the redirected socket.Then we set the IP_TRANSPARENT socket option on outbound connectionsSame goes for making connections to a remote host pretending to be the client, the IP_TRANSPARENT socket option is set and the Linux kernel will allow the bind so along as a connection was intercepted with those details being used for the bind.When the process get a new connection we start a connection to the real destination address and copy the traffic between both socketsvar streamWait sync.WaitGroupstreamWait.Add(2)streamConn := func(dst io.Writer, src io.Reader) {    io.Copy(dst, src)    streamWait.Done()}go streamConn(remoteConn, VMconn)go streamConn(VMconn, remoteConn)streamWait.Wait()The Haproxy helps us with the ingress traffic with the follow configurationdefaults  mode tcpfrontend main  bind *:9080  default_backend guestbackend guest  server guest 10.0.1.2:9080 maxconn 2048It send all the traffic to our virtual machine on the service port the machine is listening.Code repositorytproxy conclusionsThis solution is a general solution, not a dedicated solution to istio only. Its make the vm traffic look like a regular process inside the pod so it will work with any sidecars projectsEgress data flow in this solution:Ingress data flow in this solution:Pros:  other process can change the nat table (this solution works on the mangle table)  better preformance comparing to nat-proxy  Works with any sidecar solutionCons:  Need NET_ADMIN capability for the docker  External process needed  The traffic is passed to user space  Only support ingress TCP connectionResearch ConclustionKubevirt shows it is possible to run virtual machines inside a kubernetes cluster, and this post shows that the virtual machine can also get the benefit of it.",
        "url": "/2018/Research-run-VMs-with-istio-service-mesh.html"
      }
      ,
    
      "2018-use-vs-code-for-kube-virt-development-html": {
        "title": "Use Vs Code For Kube Virt Development",
        "author": "SchSeba",
        "category": "uncategorized",
        "content": "In this post we will install and configure Visual studio code (vscode) for KubeVirt development and debug.Visual Studio Code is a source code editor developed by Microsoft for Windows, Linux and macOS.It includes support for debugging, embedded Git control, syntax highlighting, intelligent code completion, snippets, and code refactoring.Golang InstallationGO installation is required, We can find the binaries in golang page.Golang Linux InstallationAfter downloading the binaries extract them with the following command:tar -C /usr/local -xzf go$VERSION.$OS-$ARCH.tar.gzNow lets Add /usr/local/go/bin to the PATH environment variable.You can do this by adding this line to your /etc/profile (for a system-wide installation) or $HOME/.profile:export PATH=$PATH:/usr/local/go/binGolang Windows InstallationOpen the MSI file and follow the prompts to install the Go tools.By default, the installer puts the Go distribution in C:\\Go.The installer should put the C:\\Go\\bin directory in your PATH environment variable.You may need to restart any open command prompts for the change to take effect.VSCODE InstallationNow we will install Visual Studio Code in our system.For linux machinesWe need to choose our linux distribution.For RHEL/Centos/FedoraThe following script will install the key and repository:sudo rpm --import https://packages.microsoft.com/keys/microsoft.ascsudo sh -c 'echo -e \"[code]\\nname=Visual Studio Code\\nbaseurl=https://packages.microsoft.com/yumrepos/vscode\\nenabled=1\\ngpgcheck=1\\ngpgkey=https://packages.microsoft.com/keys/microsoft.asc\" &gt; /etc/yum.repos.d/vscode.repo'Then update the package cache and install the package using dnf (Fedora 22 and above):dnf check-updatesudo dnf install codeOr on older versions using yum:yum check-updatesudo yum install codeFor Debian/UbuntuWe need to download the .deb package from the vscode download page,and from the command line run the package management.sudo dpkg -i &lt;file&gt;.debsudo apt-get install -f # Install dependenciesFor Windows machinesDownload the Visual Studio Code installer, and then run the installer (VSCodeSetup-version.exe)Go Project structLets create the following structure for our kubevirt project development environment:├── &lt;Go-projects-folder&gt; # Your Golang projects root folder│   ├── bin│   ├── pkg│   ├── src│   │   ├── kubevirt.ioNow navigate to kubevirt.io folder and run:git clone &lt;kubevirt-fork&gt;Install VSCODE ExtensionsNow we are going to install some extensions for a better development experience with the IDE.Open vscode and select your go project root folder you created in the last step.On the extensions tab (Ctrl+Shift+X), search for golang and install it.Now open the command palette (Ctrl+Shift+P) view-&gt;Command Palette and type “Go: install/update tools”, this will install all the requirements for example: delve debugger, etc…(optional) We can install docker extension for syntax highlighting, commands, etc..GOPATH and GOROOT configurationOpen the vscode configuration file (ctrl+,) file-&gt;preferences-&gt;settings.Now on the right file we need to add this configuration:\"go.gopath\": \"&lt;Go-projects-folder&gt;\",\"go.goroot\": \"/usr/local/go\",Create debug configurationFor the last part we are going to configure the debugger file, open it by Debug-&gt;Open Configurations and add to the configuration list the following structure** Change the  parameter to your golang projects root directory{            \"name\": \"Kubevirt\",            \"type\": \"go\",            \"request\": \"launch\",            \"mode\": \"debug\",            \"remotePath\": \"\",            \"port\": 2345,            \"host\": \"127.0.0.1\",            \"program\": \"${fileDirname}\",            \"env\": {},            \"args\": [\"--kubeconfig\", \"cluster/k8s-1.9.3/.kubeconfig\",                     \"--port\", \"1234\"],            \"showLog\": true,            \"cwd\": \"${workspaceFolder}/src/kubevirt.io/kubevirt\",            \"output\": \"&lt;Go-projects-folder&gt;/bin/${fileBasenameNoExtension}\"}Debug ProcessFor debug we need to open the main package we want to debug.For example if we want to debug the virt-api component, open the main package:kubevirt.io/cmd/virt-api/virt-api.goNow change to debug view (ctrl+shift+D), check that we are using the kubevirt configuration and hit the play buttonMore InformationFor more information, keyboard shortcuts and advance vscode usage please refer the following linkeditor code basics",
        "url": "/2018/Use-VS-Code-for-Kube-Virt-Development.html"
      }
      ,
    
      "2018-ovn-multi-network-plugin-for-kubernetes-kubetron-html": {
        "title": "Ovn Multi Network Plugin For Kubernetes Kubetron",
        "author": "phoracek",
        "category": "uncategorized",
        "content": "Kubernetes networking model is suited for containerized applications, based mostly around L4 and L7 services, where all pods are connected to one big network. This is perfectly ok for most use cases. However, sometimes there is a need for fine-grained network configuration with better control. Use-cases such as L2 networks, static IP addresses, interfaces dedicated for storage traffic etc. For such needs there is ongoing effort in Kubernetes sig-network to support multiple networks (see Kubernetes Network CRD De-Facto Standard. There exist many prototypes of plugins providing such functionality. You are reading about one of them.Kubetron (working name, kubernetes + neutron, quite misleading since we want to support setup without Neutron involved too), allows users to connect their pods to multiple networks configured on OVN. Important part here is, that such networks are configured by an external tool, be it OVN Northbound Database client or higher level tool such as Neutron or oVirt Provider OVN. This allows administrators to configure complicated networks, Kubernetes then only knows enough about the known networks to be able to connect to them - but not all the complexity involved to manage them. Kubetron does not affect default Kubernetes networking at all, default networks will be left intact.In order to enable the use-cases outlined above, Kubetron can be used to provide multiple interfaces to a pod, further more KubeVirt will then use those interfaces to pass them to its virtual machines via the in progress VirtualMachine networking API.You can find source code in Kubetron GitHub repository.Contents  Desired Model and Usage  Proof of Concept  Demo  Try it Yourself  Looking for Help  DisclaimerDesired Model and UsageLet’s talk about how Kubetron looks from administrator’s and user’s point of view. Please note that following examples are still for the desired state and some of them might not be implemented in PoC yet. If you want to learn more about deployment and architecture, check Kubetron slide deck.Configure OVN NetworksFirst of all, administrator must create and configure networks in OVN. That could be done either directly on OVN northbound database (e.g. using ovn-nbctl) or via OVN manager (e.g. Neutron or oVirt Provider OVN, using ansible).Expose Available NetworksOnce the networks are configured, there are two options how to expose available networks to a user. First one is providing some form of access to OVN or Neutron API, this one is completely out of Kubernetes’ and Kubetron’sscope. Second option is to enable Network object support (as described in Kubernetes Network CRD De-Facto standard). With this option, administrator must create a Network object per each OVN network is allowed to be used by a user. This object allows administrator to expose only limited subset of networks or to limit access per Namespace. This process could be automated, e.g. via a service that monitors available logical switches and exposes them as Networks.# List networks (Logical Switches) directly from OVN Northbound databaseovn-nbctl ls-list# List networks available on Neutronneutron net-list# List networks as Network objects created in Kuberneteskubectl get networksAttach pod to a NetworkOnce user selects a desired network based on options described in previous section, he or she can request them for a pod using an annotation. This annotation is compatible with earlier mentioned Kubernetes Network CRD De-Facto Standard.apiVersion: v1kind: podmetadata:  name: network-consumer  annotations:    kubernetes.v1.cni.cncf.io/networks: red  # requested networksspec:  containers:  - name: busybox    image: busyboxAccess the Network from the podOnce the pod is created, a user can list its interfaces and their assigned IP addresses:$ kubectl exec -it network-consumer -- ip address...10: red-bcxoeffrsw: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1442 qdisc noqueue state UNKNOWN qlen 1000    link/ether 4e:71:3b:ee:a5:f4 brd ff:ff:ff:ff:ff:ff    inet 10.1.0.3/24 brd 10.1.0.255 scope global dynamic red-bcxoeffrsw       valid_lft 86371sec preferred_lft 86371sec    inet6 fe80::4c71:3bff:feee:a5f4/64 scope link       valid_lft forever preferred_lft forever...In order to make it easier to obtain the network’s interface name inside pod’s containers, environment variables with network-interface mapping are created:$ echo $NETWORK_INTERFACE_REDred-bcxoeffrswProof of ConceptAs for now, current implementation does not completely implement the desired model yet:  Only Neutron mode is implemented, Kubetron can not be used with OVN alone  Network object handling is not implemented, Kubetron obtains networks directly from Neutron  Interface names are not exposed as environment variablesIt might be unstable and there are some missing parts. However, basic scenario works, at least in development environment.DemoIn the following recording we create two networks red and blue using Neutron API via Ansible. Then we create two pods and connect them to both mentioned networks. And then we ping.Try it YourselfI encourage you to try Kubetron yourself. It has not yet been tested on regular Kubernetes deployment (and it likely won’t work without some tuning). Fortunately, Kubetron repository contains Vagrant file and set of scripts that will help you deploy multi-node Kuberneteswith OVN and Kubetron installed. On top of that it describes how to create networks and connect pods to them. Check out Kubetron README.md and give it a try!Looking for HelpIf you are interested in contributing to Kubetron, please follow its GitHub repository. There are many missing features and possible improvements, I will open issues to track them soon. Stay tuned!DisclaimerKubetron is in early development stage, both it’s architecture and tools to use it will change.",
        "url": "/2018/ovn-multi-network-plugin-for-kubernetes-kubetron.html"
      }
      ,
    
      "2018-use-glusterfs-cloning-with-kubevirt-html": {
        "title": "Use Glusterfs Cloning With Kubevirt",
        "author": "karmab",
        "category": "uncategorized",
        "content": "Gluster seems like a good fit for storage in kubernetes and in particular in kubevirt. Still, as for other storage backends, we will likely need to use a golden set of images and deploy vms from them.That’s where cloning feature of gluster comes at rescue!Contents  Prerequisites  Installing Gluster provisioner  Using The cloning feature  ConclusionPrerequisitesI assume you already have a running instance of openshift and kubevirt along with gluster and an already existing pvc where you copied a base operating system ( you can get those from here)For reference, I used the following components and versions:  3 baremetal servers with Rhel 7.4 as base OS  Openshift and Cns 3.9  KubeVirt latestInstalling Gluster provisionerinitial deploymentWe will deploy the custom provisioner using this template, along with cluster rules located in this fileNote that we also patch the image to use an existing one from gluster org located at docker.io instead of quay.io, as the corresponding repository is private by the time of this writing, and the heketi one, to make sure it has the code required to handle cloningNAMESPACE=\"app-storage\"oc create -f openshift-clusterrole.yamloc process -f glusterfile-provisioner-template.yml | oc apply -f - -n $NAMESPACEoc adm policy add-cluster-role-to-user cluster-admin -z glusterfile-provisioner -n $NAMESPACEoc adm policy add-scc-to-user privileged -z glusterfile-provisioneroc set image dc/heketi-storage heketi=gluster/heketiclone:latest  -n $NAMESPACEoc set image dc/glusterfile-provisioner glusterfile-provisioner=gluster/glusterfileclone:latest  -n $NAMESPACEAnd you will see something similar to this in your storage namespace[root@master01 ~]# NAMESPACE=\"app-storage\"[root@master01 ~]# kubectl get pods -n $NAMESPACENAME                              READY     STATUS    RESTARTS   AGEglusterfile-provisioner-3-vhkx6   1/1       Running   0          1dglusterfs-storage-b82x4           1/1       Running   1          23dglusterfs-storage-czthc           1/1       Running   0          23dglusterfs-storage-z68hm           1/1       Running   0          23dheketi-storage-2-qdrks            1/1       Running   0          6hadditional configurationfor the custom provisioner to work, we need two additional things:  a storage class pointing to it, but also containing the details of the current heketi installation  a secret similar to the one used by the current heketi installation, but using a different typeYou can use the followingNAMESPACE=\"app-storage\"oc get sc glusterfs-storage -o yamloc get secret heketi-storage-admin-secret -n $NAMESPACE-o yamlthen, create the following objects:  glustercloning-heketi-secret secret in your storage namespace  glustercloning storage classfor reference, here are samples of those files.Note how we change the type for the secret and add extra options for our storage class (in particular, enabling smartclone).apiVersion: v1data:  key: eEt0NUJ4cklPSmpJb2RZcFpqVExSSjUveFV5WHI4L0NxcEtMME1WVlVjQT0=kind: Secretmetadata:  name: glustercloning-heketi-secret  namespace: app-storagetype: gluster.org/glusterfileapiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: glustercloningparameters:  restsecretname: glustercloning-heketi-secret  restsecretnamespace: app-storage  resturl: http://heketi-storage.192.168.122.10.xip.io  restuser: admin  smartclone: \"true\"  snapfactor: \"10\"  volumeoptions: group virtprovisioner: gluster.org/glusterfilereclaimPolicy: DeleteThe full set of supported parameters can be found hereUsing the cloning featureOnce deployed, you can now provision pvcs from a base originCloning single pvcsFor instance, provided you have an existing pvc named cirros containing this base operating system, and that this PVC contains an annotion of the following(...)metadata: annotations:  gluster.org/heketi-volume-id: f0cbbb29ef4202c5226f87708da57e5c(...)you can create a cloned pvc with the following yaml ( note that we simply indicate a clone request in the annotations)apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: testclone1  namespace: default  annotations:    k8s.io/CloneRequest: cirrosspec:  accessModes:  - ReadWriteOnce  resources:    requests:      storage: 1Gi  storageClassName: glustercloningstatus:  accessModes:  - ReadWriteOnce  capacity:    storage: 1GiOnce provisioned, the pvc will contain this additional annotation created by the provisioner(...)metadata: annotations:      k8s.io/CloneOf: cirros(...)Leveraging the feature in openshift templatesWe can make direct use of the feature in this openshift template which would create the following objects:  a persistent volume claim as a clone of an existing pvc (cirros by default)  an offline virtual machine object  additional services for ssh and http accessyou can use it with something likeoc process -f template.yml -p Name=myvm | oc process -f - -n defaultConclusioncloning features in the storage backend allow us to simply use a given set of pvcs as base os for the deployment of our vms. this feature is growing in gluster, worth giving it a try!",
        "url": "/2018/Use-GlusterFS-Cloning-with-KubeVirt.html"
      }
      ,
    
      "2018-kubevirt-api-access-control-html": {
        "title": "Kubevirt Api Access Control",
        "author": "davidvossel",
        "category": "uncategorized",
        "content": "Access to KubeVirt resources are controlled entirely by Kubernete’s ResourceBased Access Control (RBAC) system. This system allows KubeVirt to tie directlyinto the existing authentication and authorization mechanisms Kubernetesalready provides to its core api objects.KubeVirt RBAC Role BasicsTypically when people think of Kubernetes RBAC system, they’re thinking aboutgranting users access to create/delete kubernetes objects (like Pods,deployments, etc), however those same RBAC mechanisms work naturally withKubeVirt objects as well.When we look at KubeVirt’s objects, we can see they are structured just likethe objects that come predefined in the Kubernetes core.For example, look here’s an example of a VirtualMachine spec.apiVersion: kubevirt.io/v1alpha1kind: VirtualMachinemetadata:  name: vm-ephemeralspec:  domain:    devices:      disks:      - disk:          bus: virtio        name: registrydisk        volumeName: registryvolume    resources:      requests:        memory: 64M  volumes:  - name: registryvolume    registryDisk:      image: kubevirt/cirros-registry-disk-demo:develIn the spec above, we see the KubeVirt VirtualMachine object has an apiVersionfield and a kind field just like a Pod spec does. The kubevirt.io portionof the apiVersion field represents KubeVirt apiGroup the resource is a part of.The kind field reflects the resource type.Using that information, we can create an RBAC role that gives a user permissionto create, delete, and view all VirtualMachine objects.apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata:  name: vm-access  labels:    kubevirt.io: \"\"rules:  - apiGroups:      - kubevirt.io    resources:      - virtualmachines    verbs:      - get      - delete      - create      - update      - patch      - list      - watchThis same logic can be applied when creating RBAC roles for other KubeVirtobjects as well. If we wanted to extend this RBAC role to grant similarpermissions for VirtualMachinePreset objects, we’d just have to add a secondresource kubevirt.io resource list. The result would look like this.apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata:  name: vm-access  labels:    kubevirt.io: \"\"rules:  - apiGroups:      - kubevirt.io    resources:      - virtualmachines      - virtualmachinepresets    verbs:      - get      - delete      - create      - update      - patch      - list      - watchKubeVirt Subresource RBAC RolesAccess to a VirtualMachines’s VNC and console stream using KubeVirt’svirtctl tool is managed by the Kubernetes RBAC system as well. Permissionsfor these resources work slightly different than the other KubeVirt objectsthough.Console and VNC access is performed using the KubeVirt Stream API, which hasits own api group called subresources.kubevirt.io. Below is an example ofhow to create a role that grants access to the VNC and console streams APIs.apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata:  name: vm-vnc-access  labels:    kubevirt.io: \"\"rules:  - apiGroups:      - subresources.kubevirt.io    resources:      - virtualmachines/console      - virtualmachines/vnc    verbs:      - getLimiting RBAC To a Single Namespace.A ClusterRole can be bound to a user in two different ways.When a ClusterRoleBinding is used, a user is permitted access to all resourcesdefined in the ClusterRole across all namespaces in the cluster.When a RoleBinding is used, a user is limited to accessing only the resourcesdefined in the ClusterRole within the namespace RoleBinding exists in.Limiting RBAC To a Single Resource.A user can also be limit to accessing only a single resource within a resourcetype. Below is an example that only grants VNC access to the VirtualMachinenamed ‘bobs-vm’apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata:  name: vm-vnc-access  labels:    kubevirt.io: \"\"rules:  - apiGroups:      - subresources.kubevirt.io    resources:      - virtualmachines/console      - virtualmachines/vnc    resourceName:      - bobs-vm    verbs:      - getDefault Kubevirt RBAC RolesThe next release of KubeVirt is coming with three default ClusterRoles thatadmins can use to grant users access to KubeVirt resources. In most cases,these roles will prevent admins from ever having to create their own customKubeVirt RBAC roles.More information about these default roles can be found in the KubeVirtuser guide here",
        "url": "/2018/KubeVirt-API-Access-Control.html"
      }
      ,
    
      "2018-kubevirt-objects-html": {
        "title": "Kubevirt Objects",
        "author": "jcpowermac",
        "category": "uncategorized",
        "content": "The KubeVirt project provides extensions to Kubernetes via custom resources.These resources are a collection a API objects that definesa virtual machine within Kubernetes.I think it’s important to point out the two great resources that I used tocompile information for this post:  user-guide  api-referenceWith that let’s take a look at the objects that are available.KubeVirt top-level objectsBelow is a list of the top level API objects and descriptions that KubeVirt provides.      VirtualMachine (vm[s]) - represents a virtual machine in the runtime environment of Kubernetes.        OfflineVirtualMachine (ovm[s]) - handles the virtual machines that are not running or are in a stopped state.        VirtualMachinePreset (vmpreset[s]) - is an extension to general VirtualMachine configuration behaving much like PodPresets from Kubernetes. When a VirtualMachine is created, any applicable VirtualMachinePresets will be applied to the existing spec for the VirtualMachine. This allows for re-use of common settings that should apply to multiple VirtualMachines.        VirtualMachineReplicaSet (vmrs[s]) - tries to ensures that a specified number of VirtualMachine replicas are running at any time.  DomainSpec is listed as a top-level object but is only used within all of the objects above. Currently the DomainSpec is a subset of what is configurable via libvirt domain XML.VirtualMachineVirtualMachine is mortal object just like aPod within Kubernetes.It only runs once and cannot be resurrected. This might seem problematic especiallyto an administrator coming from a traditional virtualization background. Fortunatelylater we will discuss OfflineVirtualMachines which will address this.First let’s use kubectl to retrieve a list of VirtualMachine objects.$ kubectl get vms -n nodejs-exNAME      AGEmongodb   5dnodejs    5dWe can also use kubectl describe$ kubectl describe vms -n testName:         testvmNamespace:    testLabels:       guest=testvm              kubevirt.io/nodeName=kn2.virtomation.com              kubevirt.io/size=small...output...Events:  Type    Reason              Age                From                               Message  ----    ------              ----               ----                               -------  Normal  SuccessfulCreate    59m                virtualmachine-controller          Created virtual machine pod virt-launcher-testvm-8h927  Normal  SuccessfulHandOver  59m                virtualmachine-controller          Pod owner ship transfered to the node virt-launcher-testvm-8h927  Normal  Created             59m (x2 over 59m)  virt-handler, kn2.virtomation.com  VM defined.  Normal  Started             59m                virt-handler, kn2.virtomation.com  VM started.And just in case if you want to return the yaml definition of a VirtualMachine object here is an example.$ kubectl -o yaml get vms mongodb -n nodejs-exapiVersion: kubevirt.io/v1alpha1kind: VirtualMachine...output...The first object we will annotate is VirtualMachine. The important sections .spec for VirtualMachineSpec and .spec.domain for DomainSpec will be annotated only in this section then referred to in the other object sections.apiVersion: kubevirt.io/v1alpha1kind: VirtualMachinemetadata:  annotations: {}  labels: {}  name: string  namespace: stringspec: {}Node PlacementKubernetes has the ability to schedule a pod to specific nodes based on affinity and anti-affinity rules.Node affinity is also possible with KubeVirt. To constrain a virtual machine to run on a node define a matching expressions using node labels.  affinity:    nodeAffinity:      preferredDuringSchedulingIgnoredDuringExecution:      - preference:          matchExpressions:          - key: string            operator: string            values:            - string        weight: 0      requiredDuringSchedulingIgnoredDuringExecution:        nodeSelectorTerms:        - matchExpressions:          - key: string            operator: string            values:            - stringA virtual machine can also more easily be constrained by using nodeSelector which is defined by node’s label and value. Here is an example  nodeSelector:    kubernetes.io/hostname: kn1.virtomation.comClocks and TimersConfigures the virtualize hardware clock provided by QEMU.  domain:    clock:      timezone: string      utc:        offsetSeconds: 0The timer defines the type and policy attribute that determines what action is take when QEMU misses a deadline for injecting a tick to the guest.  domain:    clock:      timer:        hpet:          present: true          tickPolicy: string        hyperv:          present: true        kvm:          present: true        pit:          present: true          tickPolicy: string        rtc:          present: true          tickPolicy: string          track: stringCPU and MemoryThe number of CPU cores a virtual machine will be assigned. .spec.domain.cpu.cores will not be used for scheduling use .spec.domain.resources.requests.cpu instead.    cpu:      cores: 1There are two supported resource limits and requests: cpu and memory. A .spec.domain.resources.requests.memory should be defined to determine the allocation of memory provided to the virtual machine. These values will be used to in scheduling decisions.    resources:      limits: {}      requests: {}Watchdog Devices.spec.domain.watchdog automatically triggers an action via Libvirt and QEMU when the virtual machine operating system hangs or crashes.      watchdog:        i6300esb:          action: string        name: stringFeatures.spec.domain.featuresare hypervisor cpu or machine features that can be enabled.After reviewing both Linux and Microsoft QEMU virtual machines managed byLibvirtboth acpi andapicshould be enabled.The hyperv features should be enabled only for Windows-based virtual machines. For additional information regarding features please visit the virtual hardware configuration in the kubevirt user guide.    features:      acpi:        enabled: true      apic:        enabled: true        endOfInterrupt: true      hyperv:        relaxed:          enabled: true        reset:          enabled: true        runtime:          enabled: true        spinlocks:          enabled: true          spinlocks: 0        synic:          enabled: true        synictimer:          enabled: true        vapic:          enabled: true        vendorid:          enabled: true          vendorid: string        vpindex:          enabled: trueQEMU Machine Type.spec.domain.machine.type is the emulated machine architecture provided by QEMU.    machine:      type: stringHere is an example how to retrieve the supported QEMU machine types.$ qemu-system-x86_64 --machine helpSupported machines are:...output...pc                   Standard PC (i440FX + PIIX, 1996) (alias of pc-i440fx-2.10)pc-i440fx-2.10       Standard PC (i440FX + PIIX, 1996) (default)...output...q35                  Standard PC (Q35 + ICH9, 2009) (alias of pc-q35-2.10)pc-q35-2.10          Standard PC (Q35 + ICH9, 2009)Disks and Volumes.spec.domain.devices.disks configures a QEMU type of disk to the virtual machine and assigns a specific volume and its type to that disk via the volumeName.    devices:      disks:      - cdrom:          bus: string          readonly: true          tray: string        disk:          bus: string          readonly: true        floppy:          readonly: true          tray: string        lun:          bus: string          readonly: true        name: string        volumeName: stringcloudInitNoCloudinjects scripts and configuration into a virtual machine operating system.There are three different parameters that can be used to provide thecloud-init coniguration: secretRef, userData or userDataBase64.See the user-guide for examples of how to use .spec.volumes.cloudInitNoCloud.  volumes:  - cloudInitNoCloud:      secretRef:        name: string      userData: string      userDataBase64: stringAn emptyDisk volume creates an extra qcow2 disk that is created with the virtual machine. It will be removed if the VirtualMachine object is deleted.    emptyDisk:      capacity: stringEphemeral volume creates a temporary local copy on write image storage that will be discarded when the VirtualMachine is removed.    ephemeral:      persistentVolumeClaim:        claimName: string        readOnly: true    name: stringpersistentVolumeClaim volume persists after the VirtualMachine is deleted.    persistentVolumeClaim:      claimName: string      readOnly: trueregistryDisk volume type uses a virtual machine disk that is stored in a container image registry.    registryDisk:      image: string      imagePullSecret: stringVirtual Machine StatusOnce the VirtualMachine object has been created the VirtualMachineStatus will be available. VirtualMachineStatus can be used in automation tools such as Ansible to confirm running state, determine where a VirtualMachine is running via nodeName or the ipAddress of the virtual machine operating system.kubectl -o yaml get vm mongodb -n nodejs-ex# ...output...status:  interfaces:  - ipAddress: 10.244.2.7  nodeName: kn2.virtomation.com  phase: RunningExample using --template to retrieve the .status.phase of the VirtualMachine.kubectl get vm mongodb --template {{.status.phase}} -n nodejs-exRunningExamples  https://www.kubevirt.io/user-guide/#/workloads/virtual-machines/creation?id=virtualmachine-apiOfflineVirtualMachineAn OfflineVirtualMachine is an immortal object within KubeVirt. The VirtualMachinedescribed within the spec will be recreated with a start power operation, host issueor simply a accidental deletion of the VirtualMachine object.For a traditional virtual administrator this object might be appropriate formost use-cases.Just like VirtualMachine we can retrieve the OfflineVirtualMachine objects.$ kubectl get ovms -n nodejs-exNAME      AGEmongodb   5dnodejs    5dAnd display the object in yaml.$ kubectl -o yaml get ovms mongodb -n nodejs-exapiVersion: kubevirt.io/v1alpha1kind: OfflineVirtualMachinemetadata:...output...We continue by annotating OfflineVirtualMachine object.apiVersion: kubevirt.io/v1alpha1kind: OfflineVirtualMachinemetadata:  annotations: {}  labels: {}  name: string  namespace: stringspec:What is Running in OfflineVirtualMachine?.spec.running controls whether the associated VirtualMachine object is created. In other words this changes the power status of the virtual machine.  running: trueThis will create a VirtualMachine object which will instantiate and power on a virtual machine.kubectl patch offlinevirtualmachine mongodb --type merge -p '{\"spec\":{\"running\":true }}' -n nodejs-exThis will delete the VirtualMachine object which will power off the virtual machine.kubectl patch offlinevirtualmachine mongodb --type merge -p '{\"spec\":{\"running\":false }}' -n nodejs-exAnd if you would rather not have to remember the kubectl patch command abovethe KubeVirt team has provided a cli tool virtctl that can start and stopa guest../virtctl start mongodb -n nodejs-ex./virtctl stop mongodb -n nodejs-exOffline Virtual Machine StatusOnce the OfflineVirtualMachine object has been created the OfflineVirtualMachineStatus will be available. Like VirtualMachineStatus OfflineVirtualMachineStatus can be used for automation tools such as Ansible.kubectl -o yaml get ovms mongodb -n nodejs-ex# ...output...status:  created: true  ready: trueExample using --template to retrieve the .status.conditions[0].type of OfflineVirtualMachine.kubectl get ovm mongodb --template \"{{.status.ready}}\" -n nodejs-extrueExamples  https://www.kubevirt.io/user-guide/#/workloads/controllers/offline-virtual-machine?id=exampleVirtualMachineReplicaSetVirtualMachineReplicaSet is great when you want to run multiple identical virtual machines.Just like the other top-level objects we can retrieve VirtualMachineReplicaSet.$ kubectl get vmrs -n nodejs-exNAME      AGEreplica   1mWith the replicas parameter set to 2 the command below displays the two VirtualMachine objects that were created.$ kubectl get vms -n nodejs-exNAME           AGEreplicanmgjl   7mreplicarjhdz   7mPause rolloutThe .spec.paused parameter if true pauses the deployment of the VirtualMachineReplicaSet.  paused: trueReplica quantityThe .spec.replicas number of VirtualMachine objects that should be created.  replicas: 0The selector must be defined and match labels defined in the template. It is used by the controller to keep track of managed virtual machines.  selector:    matchExpressions:    - key: string      operator: string      values:      - string    matchLabels: {}Virtual Machine Template SpecThe VMTemplateSpec is the definition of a VirtualMachine objects that will be created.In the VirtualMachine section the .spec VirtualMachineSpec describes the available parameters for that object.  template:    metadata:      annotations: {}      labels: {}      name: string      namespace: string    spec: {}Replica StatusLike the other objects we already have discussed VMReplicaSetStatus is an important object to use for automation.status:  readyReplicas: 0  replicas: 0Example using --template to retrieve the .status.readyReplicas and .status.replicas of VirtualMachineReplicaSet.$ kubectl get vmrs replica --template \"{{.status.readyReplicas}}\" -n nodejs-ex2$ kubectl get vmrs replica --template \"{{.status.replicas}}\" -n nodejs-ex2Examples  https://www.kubevirt.io/user-guide/#/workloads/controllers/virtual-machine-replica-set?id=exampleVirtualMachinePresetThis is used to define a DomainSpec that can be used for multiple virtual machines.To configure a DomainSpec for multiple VirtualMachine objects the selector defines which VirtualMachine the VirtualMachinePreset should be applied to.$ kubectl get vmpreset -n nodejs-exNAME       AGEm1.small   17sDomain SpecSee the VirtualMachine section above for annotated details of the DomainSpec object.spec:  domain: {}Preset SelectorThe selector is optional but if not defined will be applied to all VirtualMachine objects; which is probably not the intended purpose so I recommend always including a selector.  selector:    matchExpressions:    - key: string      operator: string      values:      - string    matchLabels: {}Examples  https://www.kubevirt.io/user-guide/#/workloads/virtual-machines/presets?id=examplesWe provided an annotated view into the KubeVirt objects - VirtualMachine, OfflineVirtualMachine, VirtualMachineReplicaSet and VirtualMachienPreset. Hopefully this will help a user of KubeVirt to understand the options and parameters that are currently available when creating a virtual machine on Kubernetes.",
        "url": "/2018/KubeVirt-objects.html"
      }
      ,
    
      "2018-deploying-vms-on-kubernetes-glusterfs-kubevirt-html": {
        "title": "Deploying Vms On Kubernetes Glusterfs Kubevirt",
        "author": "rwsu",
        "category": "uncategorized",
        "content": "Kubernetes is traditionally used to deploy and manage containerized applications. Did you know Kubernetes can also be used to deploy and manage virtual machines? This guide will walk you through installing a Kubernetes environment backed by GlusterFS for storage and the KubeVirt add-on to enable deployment and management of VMs.Contents  Prerequisites  Known Issues  Installing Kubernetes  Installing GlusterFS and Heketi using gk-deploy  Installing KubeVirt  Deploying Virtual MachinesPrerequisitesYou should have access to at least three baremetal servers. One server will be the master Kubernetes node and other two servers will be the worker nodes. Each server should have a block device attached for GlusterFS, this is in addition to the ones used by the OS.You may use virtual machines in lieu of baremetal servers. Performance may suffer and you will need to ensure your hardware supports nested virtualization and that the relevant kernel modules are loaded in the OS.For reference, I used the following components and versions:  baremetal servers with CentOS version 7.4 as the base OS  latest version of Kubernetes (at the time v1.10.1)  Weave Net as the Container Network Interface (CNI), v2.3.0  gluster-kubernetes master commit 2a2a68ce5739524802a38f3871c545e4f57fa20a  KubeVirt v0.4.1.Known Issues  You may need to set SELinux to permissive mode prior to running “kubeadm init” if you see failures attributed to etcd in /var/log/audit.log.  Prior to installing GlusterFS, you may need to disable firewalld until this issue is resolved: https://github.com/gluster/gluster-kubernetes/issues/471  kubevirt-ansible install may fail in storage-glusterfs role: https://github.com/kubevirt/kubevirt-ansible/issues/219Installing KubernetesCreate the Kubernetes cluster by using kubeadm. Detailed instructions can be found at https://kubernetes.io/docs/setup/independent/install-kubeadm/.Use Weave Net as the CNI. Other CNIs may work, but I have only tested Weave Net.If you are using only 2 servers as workers, then you will need to allow scheduling of pods on the master node because GlusterFS requires at least three nodes. To schedule pods on the master node, see “Master Isolation” in the kubeadm guide or execute this command:kubectl taint nodes --all node-role.kubernetes.io/master-Move onto the next step when your master and worker nodes are Ready.[root@master ~]# kubectl get nodesNAME                     STATUS    ROLES     AGE       VERSIONmaster.somewhere.com     Ready     master    6d        v1.10.1worker1.somewhere.com    Ready     &lt;none&gt;    6d        v1.10.1worker2.somewhere.com    Ready     &lt;none&gt;    6d        v1.10.1And all of the pods in the kube-system namespace are Running.[root@master ~]# kubectl get pods -n kube-systemNAME                                           READY     STATUS    RESTARTS   AGEetcd-master.somewhere.com                      1/1       Running   0          6dkube-apiserver-master.somewhere.com            1/1       Running   0          6dkube-controller-manager-master.somewhere.com   1/1       Running   0          6dkube-dns-86f4d74b45-glv4k                      3/3       Running   0          6dkube-proxy-b6ksg                               1/1       Running   0          6dkube-proxy-jjxs5                               1/1       Running   0          6dkube-proxy-kw77k                               1/1       Running   0          6dkube-scheduler-master.somewhere.com            1/1       Running   0          6dweave-net-ldlh7                                2/2       Running   0          6dweave-net-pmhlx                                2/2       Running   1          6dweave-net-s4dp6                                2/2       Running   0          6dInstalling GlusterFS and Heketi using gluster-kubernetesThe next step is to deploy GlusterFS and Heketi onto Kubernetes.GlusterFS provides the storage system on which the virtual machine images are stored. Heketi provides the REST API that Kubernetes uses to provision GlusterFS volumes. The gk-deploy tool is used to deploy both of these components as pods in the Kubernetes cluster.There is a detailed setup guide for gk-deploy. Note each node must have a raw block device that is reserved for use by heketi and they must not contain any data or be pre-formatted. You can reset your block device to a useable state by running:wipefs -a &lt;path to device&gt;To aid you, below are the commands you will need to run if you are following the setup guide.On all nodes:# Open ports for GlusterFS communicationssudo iptables -I INPUT 1 -p tcp --dport 2222 -j ACCEPTsudo iptables -I INPUT 1 -p tcp --dport 24007 -j ACCEPTsudo iptables -I INPUT 1 -p tcp --dport 24008 -j ACCEPTsudo iptables -I INPUT 1 -p tcp --dport 49152:49251 -j ACCEPT# Load kernel modulessudo modprobe dm_snapshotsudo modprobe dm_thin_poolsudo modprobe dm_mirror# Install glusterfs-fuse and git packagessudo yum install -y glusterfs-fuse gitOn the master node:# checkout gluster-kubernetes repogit clone https://github.com/gluster/gluster-kubernetescd gluster-kubernetes/deployBefore running the gk-deploy script, we need to first create a topology.json file that maps the nodes present in the GlusterFS cluster and the block devices attached to each node. The block devices should be raw and unformatted. Below is a sample topology.json file for a 3 node cluster all operating in the same zone. The gluster-kubernetes/deploy directory also contains a sample topology.json file.# topology.json{  \"clusters\": [    {      \"nodes\": [        {          \"node\": {            \"hostnames\": {              \"manage\": [                \"master.somewhere.com\"              ],              \"storage\": [                \"192.168.10.100\"              ]            },            \"zone\": 1          },          \"devices\": [            \"/dev/vdb\"          ]        },        {          \"node\": {            \"hostnames\": {              \"manage\": [                \"worker1.somewhere.com\"              ],              \"storage\": [                \"192.168.10.101\"              ]            },            \"zone\": 1          },          \"devices\": [            \"/dev/vdb\"          ]        },        {          \"node\": {            \"hostnames\": {              \"manage\": [                \"worker2.somewhere.com\"              ],              \"storage\": [                \"192.168.10.102\"              ]            },            \"zone\": 1          },          \"devices\": [            \"/dev/vdb\"          ]        }      ]    }  ]}Under “hostnames”, the node’s hostname is listed under “manage” and its IP address is listed under “storage”. Multiple block devices can be listed under “devices”. If you are using VMs, the second block device attached to the VM will usually be /dev/vdb. For multi-path, the device path will usually be /dev/mapper/mpatha. If you are using a second disk drive, the device path will usually be /dev/sdb.Once you have your topology.json file and saved it in gluster-kubernetes/deploy, we can execute gk-deploy to create the GlusterFS and Heketi pods. You will need to specify an admin-key which will be used in the next step and will be discovered during the KubeVirt installation.# from gluster-kubernetes/deploy./gk-deploy -g -v -n kube-system --admin-key my-admin-keyAdd the end of the installation, you will see:heketi is now running and accessible via http://10.32.0.4:8080 . To runadministrative commands you can install 'heketi-cli' and use it as follows:  # heketi-cli -s http://10.32.0.4:8080 --user admin --secret '&lt;ADMIN_KEY&gt;' cluster listYou can find it at https://github.com/heketi/heketi/releases . Alternatively,use it from within the heketi pod:  # /usr/bin/kubectl -n kube-system exec -i heketi-b96c7c978-dcwlw -- heketi-cli -s http://localhost:8080 --user admin --secret '&lt;ADMIN_KEY&gt;' cluster listFor dynamic provisioning, create a StorageClass similar to this:\\Take note of the URL for Heketi which will be used next step.If successful, 4 additional pods will be shown as Running in the kube-system namespace.[root@master deploy]# kubectl get pods -n kube-systemNAME                                                              READY     STATUS    RESTARTS   AGE...snip...glusterfs-h4nwf                                                   1/1       Running   0          6dglusterfs-kfvjk                                                   1/1       Running   0          6dglusterfs-tjm2f                                                   1/1       Running   0          6dheketi-b96c7c978-dcwlw                                            1/1       Running   0          6d...snip...Installing KubeVirt and setting up storageThe final component to install and which will enable us to deploy VMs on Kubernetes is KubeVirt.We will use kubevirt-ansible to deploy KubeVirt which will also help us configure a Secret and a StorageClass that will allow us to provision Persistent Volume Claims (PVCs) on GlusterFS.Let’s first clone the kubevirt-ansible repo.git clone https://github.com/kubevirt/kubevirt-ansiblecd kubevirt-ansibleEdit the inventory file in the kubevirt-ansible checkout. Modify the section that starts with “#BEGIN CUSTOM SETTINGS”. As an example using the servers from above:# BEGIN CUSTOM SETTINGS[masters]# Your master FQDNmaster.somewhere.com[etcd]# Your etcd FQDNmaster.somewhere.com[nodes]# Your nodes FQDN'sworker1.somewhere.comworker2.somewhere.com[nfs]# Your nfs server FQDN[glusterfs]# Your glusterfs nodes FQDN# Each node should have the \"glusterfs_devices\" variable, which# points to the block device that will be used by gluster.master.somewhere.comworker1.somewhere.comworker1.somewhere.com## If you run openshift deployment# You can add your master as schedulable node with option openshift_schedulable=true# Add at least one node with lable to run on it router and docker containers# openshift_node_labels=\"{'region': 'infra','zone': 'default'}\"# END CUSTOM SETTINGSNow let’s run the kubevirt.yml playbook:ansible-playbook -i inventory playbooks/kubevirt.yml -e cluster=k8s -e storage_role=storage-glusterfs -e namespace=kube-system -e glusterfs_namespace=kube-system -e glusterfs_name= -e heketi_url=http://10.32.0.4:8080 -vIf successful, we should see 7 additional pods as Running in the kube-system namespace.[root@master kubevirt-ansible]# kubectl get pods -n kube-systemNAME                                                              READY     STATUS    RESTARTS   AGEvirt-api-785fd6b4c7-rdknl                                         1/1       Running   0          6dvirt-api-785fd6b4c7-rfbqv                                         1/1       Running   0          6dvirt-controller-844469fd89-c5vrc                                  1/1       Running   0          6dvirt-controller-844469fd89-vtjct                                  0/1       Running   0          6dvirt-handler-78wsb                                                1/1       Running   0          6dvirt-handler-csqbl                                                1/1       Running   0          6dvirt-handler-hnlqn                                                1/1       Running   0          6dDeploying Virtual MachinesTo deploy a VM, we must first grab a VM image in raw format, place the image into a PVC, define the VM in a yaml file, source the VM definition into Kubernetes, and then start the VM.The containerized data importer (CDI) is usually used to import VM images into Kubernetes, but there are some patches and additional testing to be done before the CDI can work smoothly with GlusterFS. For now, we will be placing the image into the PVC using a Pod that curls the image from the local filesystem using httpd.On master or on a node where kubectl is configured correctly install and start httpd.sudo yum install -y httpdsudo systemctl start httpdDownload the cirros cloud image and convert it into raw format.curl http://download.cirros-cloud.net/0.4.0/cirros-0.4.0-x86_64-disk.img -o /var/www/html/cirros-0.4.0-x86_64-disk.imgsudo yum install -y qemu-imgqemu-img convert /var/www/html/cirros-0.4.0-x86_64-disk.img /var/www/html/cirros-0.4.0-x86_64-disk.rawCreate the PVC to store the cirros image.cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: PersistentVolumeClaimmetadata: name: gluster-pvc-cirros annotations:   volume.beta.kubernetes.io/storage-class: kubevirtspec: accessModes:  - ReadWriteOnce resources:   requests:     storage: 5GiEOFCheck the PVC was created and has “Bound” status.[root@master ~]# kubectl get pvcNAME                 STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGEgluster-pvc-cirros   Bound     pvc-843bd508-4dbf-11e8-9e4e-149ecfc53021   5Gi        RWO            kubevirt       2mCreate a Pod to curl the cirros image into the PVC.Note: You will need to substitute  with actual hostname or IP address.cat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: Podmetadata:  name: image-importer-cirrosspec:  restartPolicy: OnFailure  containers:  - name: image-importer-cirros    image: kubevirtci/disk-importer    env:      - name: CURL_OPTS        value: \"-L\"      - name: INSTALL_TO        value: /storage/disk.img      - name: URL        value: http://&lt;hostname&gt;/cirros-0.4.0-x86_64-disk.raw    volumeMounts:    - name: storage      mountPath: /storage  volumes:  - name: storage    persistentVolumeClaim:      claimName: gluster-pvc-cirrosEOFCheck and wait for the image-importer-cirros Pod to complete.[root@master ~]# kubectl get podsNAME                         READY     STATUS      RESTARTS   AGEimage-importer-cirros        0/1       Completed   0          28sCreate a Virtual Machine definition for your VM and source it into Kubernetes.Note the PVC containing the cirros image must be listed as the first disk under spec.domain.devices.disks.cat &lt;&lt;EOF | kubectl create -f -apiVersion: kubevirt.io/v1alpha2kind: VirtualMachinemetadata:  creationTimestamp: null  labels:    kubevirt.io/ovm: cirros  name: cirrosspec:  running: false  template:    metadata:      creationTimestamp: null      labels:        kubevirt.io/ovm: cirros    spec:      domain:        devices:          disks:          - disk:              bus: virtio            name: pvcdisk            volumeName: cirros-pvc          - disk:              bus: virtio            name: cloudinitdisk            volumeName: cloudinitvolume        machine:          type: \"\"        resources:          requests:            memory: 64M      terminationGracePeriodSeconds: 0      volumes:      - cloudInitNoCloud:          userDataBase64: IyEvYmluL3NoCgplY2hvICdwcmludGVkIGZyb20gY2xvdWQtaW5pdCB1c2VyZGF0YScK        name: cloudinitvolume      - name: cirros-pvc        persistentVolumeClaim:          claimName: gluster-pvc-cirrosstatus: {}Finally start the VM.export VERSION=v0.4.1curl -L -o virtctl https://github.com/kubevirt/kubevirt/releases/download/$VERSION/virtctl-$VERSION-linux-amd64chmod +x virtctl./virtctl start cirrosWait for the VM pod to be in “Running” status.[root@master ~]# kubectl get podsNAME                         READY     STATUS      RESTARTS   AGEimage-importer-cirros        0/1       Completed   0          28svirt-launcher-cirros-krvv2   0/1       Running     0          13sOnce it is running, we can then connect to its console../virtctl console cirrosPress enter if a login prompt doesn’t appear.",
        "url": "/2018/Deploying-VMs-on-Kubernetes-GlusterFS-KubeVirt.html"
      }
      ,
    
      "2018-deploying-kubevirt-on-a-single-ovirt-vm-html": {
        "title": "Deploying Kubevirt On A Single Ovirt Vm",
        "author": "awels",
        "category": "uncategorized",
        "content": "In this blog post we are exploring the possibilities of deploying Kube Virt on top of Open Shift which is running inside an oVirt VM. First we must prepare the environment. In my testing I created a VM with 4 cpus, 14G memory and a 100G disk. I then installed Centos 7.4 minimal on it. I also have nested virtualizationenabled on my hosts, so any VMs I create can run VMs inside them. These instructions are specific to oVirt, however if you are running another virtualizationplatform that can nested virtualization this will also work.For this example I chose to use a single VM for everything, but I could have done different VMs for my master/nodes/storage/etc, for simplicity I used a singleVM.Preparing the VMFirst we will need to enable epel and install some needed tools, like git to get at the source, and ansible to do the deploy:As root:$ yum -y install epel-release$ yum -y install ansible git wgetoptionalInstall ovirt-guest-agent so you can see information in your oVirt admin view.As root:$ yum -y install ovirt-guest-agent$ systemctl start ovirt-guest-agent$ systemctl enable ovirt-guest-agentMake a template out of the VM, so if something goes wrong you have a good starting point to try again.Make sure the VM has a fully qualified domain name, using either DNS or editing /etc/hosts.As we are going to install openshift we will need to install the openshift client tooling from openshift githubin this article I opted to simply copy the oc command into /usr/bin, but anywhere in your path will do. Alternatively you can add oc to your PATH.As root:$ wget https://github.com/openshift/origin/releases/download/v3.9.0/openshift-origin-client-tools-v3.9.0-191fece-linux-64bit.tar.gz$ tar zxvf openshift-origin-client-tools-v3.9.0-191fece-linux-64bit.tar.gz$ cp openshift-origin-client-tools-v3.9.0-191fece-linux-64bit/oc /usr/binNext we will install docker and configure it for use with open shift.As root:$ yum -y install dockerWe need to setup an insecure registry in docker before we can start open shift. To do this we must add:INSECURE_REGISTRY=”–insecure-registry 172.30.0.0/16”to the end of /etc/sysconfig/dockerNow we can start docker.As root:$ systemctl start docker$ systemctl enable dockerNow we are ready to test if we can bring our cluster to up.As root:$ oc cluster upInstalling kube virt with ansibleNow that we have everything configured we can the rest as a regular user. Also note that if you had an existing cluster you can could have skipped the previous section.Clone the kube-virt ansible repo, and setup the ansible galaxy roles needed to deploy.As user:$ git clone https://github.com/kubevirt/kubevirt-ansible$ cd kubevirt-ansible$ mkdir $HOME/galaxy-roles$ ansible-galaxy install -p $HOME/galaxy-roles -r requirements.yml$ export ANSIBLE_ROLES_PATH=$HOME/galaxy-rolesNow that we are in the kubevirt-ansible directory, we have to edit the inventory file on where we are going to deploy the different open shift nodes.Because we opted to install everything on a single VM the FQDN we enter is the same as the one we defined for our VM. Had we had different nodes we wouldenter the FQDN of each in the inventory file. Lets assume our VMs FQDN is kubevirt.demo, we would changed the inventory file as follows:As user:[masters]kubevirt.demo[etcd]kubevirt.demo[nodes]kubevirt.demo openshift_node_labels=\"{'region': 'infra','zone': 'default'}\" openshift_schedulable=true[nfs]kubevirt.demoIn order to allow ansible to ssh into the box using ssh keys instead of a password we will need to generate some, assuming we don’t have theseconfigured already:As root:$ ssh-keygen -t rsaFill out the information in the questions, which will generate two files in /root/.ssh, id_rsa and id_rsa.pub. The id_rsa.pub is the public key which will allowssh to verify your identify when you ssh into a machine. Since we are doing all of this on the same machine, we can simply append the contents ofid_rsa.pub to authorized_keys in /root/.ssh. If that file doesn’t exist you can simply copy id_rsa.pub to authorized_keys. If you are deploying to multiple hostsyou need to append the contents of id_rsa.pub on each host.Next we need to configure docker storage, one can write a whole book about how to do that, so I will post a link to the installation document and for now go with the defaults which are not recommended for production, but since this is an introduction its fine.As root:$ docker-storage-setupLets double check the cluster is up before we start running the ansible play books.As root:$ oc cluster upInstall kubernetes.As root:$ ansible-playbook -i inventory playbooks/cluster/kubernetes/config.ymlDisable selinux on all hosts, this hopefully won’t be needed in the future.As root:$ ansible-playbook -i inventory playbooks/selinux.ymllog in as admin to give developer user rights.As root:$ oc login -u system:admin$ oc adm policy add-cluster-role-to-user cluster-admin developerLog in as the developer user.As user:$ oc login -u developerThe password for the developer user is developer. Now finally deploy kubevirt.As user:$ ansible-playbook -i localhost playbooks/kubevirt.yml -e@vars/all.ymlVerify that the pods are running, you should be in the kube-system namespace, if not switch with oc project kube-system.As user:$ kubectl get podsNAME                               READY     STATUS    RESTARTS   AGEvirt-api-747745669-mswk8           1/1       Running   0          10mvirt-api-747745669-t9dsp           1/1       Running   0          10mvirt-controller-648945bbcb-ln7dv   1/1       Running   0          10mvirt-controller-648945bbcb-nxrj8   0/1       Running   0          10mvirt-handler-6zh77                 1/1       Running   0          10mNow that we have kube virt up and running we are ready to try and start a VM. Lets install virtctl to make it easier tostart and stop VMs. The latest available version while writing this was 0.4.1.As user:$ export VERSION=v0.4.1$ curl -L -o virtctl \\    https://github.com/kubevirt/kubevirt/releases/download/$VERSION/virtctl-$VERSION-linux-amd64$ chmod +x virtctlLets grab the demo VM specification from the kubevirt github page.As user:$ kubectl apply -f https://raw.githubusercontent.com/kubevirt/demo/master/manifests/vm.yamlNow we can start the VM.As user:$ ./virtctl start testvmNow a new pod will be running that is controlling the VM.As user:$ kubectl get podsNAME                               READY     STATUS    RESTARTS   AGEvirt-api-747745669-mswk8           1/1       Running   0          15mvirt-api-747745669-t9dsp           1/1       Running   0          15mvirt-controller-648945bbcb-ln7dv   1/1       Running   0          15mvirt-controller-648945bbcb-nxrj8   0/1       Running   0          15mvirt-handler-6zh77                 1/1       Running   0          15mvirt-launcher-testvm-gv5nt         2/2       Running   0          23sCongratulations you now have a VM running in openshift using kubevirt inside an oVirt VM.Usefule resources  KubeVirt  KubeVirt Ansible  Minikube kubevirt Demo  Kubectl installation",
        "url": "/2018/Deploying-KubeVirt-on-a-Single-oVirt-VM.html"
      }
      ,
    
      "2018-this-week-in-kube-virt-23-html": {
        "title": "This Week In Kube Virt 23",
        "author": "fabiand",
        "category": "updates",
        "content": "This is a close-to weekly update from the KubeVirt team.In general there is now more work happening outside of the core kubevirtrepository.We are currently driven by      Closing a lot of loose ends        Stepping back to identify gaps for 1.0  Within the last two weeks we achieved to:      Release KubeVirt v0.4.1 to address some shutdown issues          https://github.com/kubevirt/kubevirt/releases/tag/v0.4.1            Many VM life-cycle and guarantee fixes (@rmohr @vossel)                  https://github.com/kubevirt/kubevirt/pull/951                    https://github.com/kubevirt/kubevirt/pull/948                    https://github.com/kubevirt/kubevirt/pull/935                    https://github.com/kubevirt/kubevirt/pull/838                    https://github.com/kubevirt/kubevirt/pull/907                    https://github.com/kubevirt/kubevirt/pull/883                  Pass labels from VM to pod for better Service integration (@rmohr)          https://github.com/kubevirt/kubevirt/pull/939            Packaging preparations (@rmohr)                  https://github.com/kubevirt/kubevirt/pull/941                    https://github.com/kubevirt/kubevirt/issues/924                    https://github.com/kubevirt/kubevirt/pull/950                  Controller readiness clarifications (@rmohr)          https://github.com/kubevirt/kubevirt/pull/901            Validation improvements using CRD scheme and webhooks (@vossel)                  Webhook: https://github.com/kubevirt/kubevirt/pull/911                    Scheme: https://github.com/kubevirt/kubevirt/pull/850                    https://github.com/kubevirt/kubevirt/pull/917                  Add Windows tests (@alukiano)          https://github.com/kubevirt/kubevirt/pull/809            Improve PVC tests (@petrkotas)          https://github.com/kubevirt/kubevirt/pull/862            Enable SELinux in OpenShift CI environment        Tests to run KubeVirt on Kubernetes 1.10  In addition to this, we are also working on:      virtctl expose convenience verb (@yuvalif)          https://github.com/kubevirt/kubevirt/pull/962            CRIO support in CI        virtctl bash/zsh completion (@rmohr)          https://github.com/kubevirt/kubevirt/pull/916            Improved error messages from virtctl (@fromanirh)          https://github.com/kubevirt/kubevirt/pull/934            Improved validation feedback (@vossel)          https://github.com/kubevirt/kubevirt/pull/960      Take a look at the pulse, to get an overview over all changes of thisweek: https://github.com/kubevirt/kubevirt/pulseFinally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesAnd keep track of events at our calendar18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com“&gt;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&lt;/link&gt;If you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;",
        "url": "/2018/This-Week-in-Kube-Virt-23.html"
      }
      ,
    
      "2018-kubevirt-network-deep-dive-html": {
        "title": "Kubevirt Network Deep Dive",
        "author": "jcpowermac, booxter",
        "category": "uncategorized",
        "content": "In this post we will research and discover how KubeVirt networking functions along with Kubernetes objects services and ingress. This should also provide enough technical details to start troubleshooting your own environment if a problem should arise. So with that let’s get started.Component InstallationWe are going to walk through the installation that assisted me to write this post. I have created three CentOS 7.4 with nested virtualization enabled where Kubernetes will be installed, which is up next.KubernetesI am rehashing what is available in Kubernetes documentation just to make it easier to follow along and provide an identical environment that I used to research KubeVirt networking.PackagesAdd the Kubernetes repositorycat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$basearchenabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgEOFUpdate and install prerequisites.yum update -yyum install kubelet-1.9.4 \\            kubeadm-1.9.4 \\            kubectl-1.9.4 \\            docker \\            ansible \\            git \\            curl \\            wget -yDocker prerequisitesFor docker storage we will use a new disk vdb formatted XFS using the Overlay driver.cat &lt;&lt;EOF &gt; /etc/sysconfig/docker-storage-setupSTORAGE_DRIVER=overlay2DEVS=/dev/vdbCONTAINER_ROOT_LV_NAME=dockerlvCONTAINER_ROOT_LV_SIZE=100%FREECONTAINER_ROOT_LV_MOUNT_PATH=/var/lib/dockerVG=dockervgEOFStart and enable Dockersystemctl start dockersystemctl enable dockerAdditional prerequisitesIn this section we continue with the required prerequistes. This is also described in the install kubeadm kubernetes documentation.systemctl enable kubeletThis is a requirement for Flannel - pass bridged IPv4 traffic to iptables’ chainscat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --systemTemporarily disable selinux so we can run kubeadm initsetenforce 0And let’s also permanently disable selinux - yes I know. If this isn’t done once you reboot your node kubernetes won’t start and then you will be wondering what happened :)cat &lt;&lt;EOF &gt; /etc/selinux/config# This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:#     enforcing - SELinux security policy is enforced.#     permissive - SELinux prints warnings instead of enforcing.#     disabled - No SELinux policy is loaded.SELINUX=disabled# SELINUXTYPE= can take one of three two values:#     targeted - Targeted processes are protected,#     minimum - Modification of targeted policy. Only selected processes are protected.#     mls - Multi Level Security protection.SELINUXTYPE=targetedEOFInitialize clusterNow we are ready to create our cluster starting with the first and only master.  Note  --pod-network-cidr is required for Flannelkubeadm init --pod-network-cidr=10.244.0.0/16...output...mkdir -p $HOME/.kube  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config  sudo chown $(id -u):$(id -g) $HOME/.kube/configThere are multiple CNI providers in this example environment just going to use Flannel since its simple to deploy and configure.kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.ymlAfter Flannel is deployed join the nodes to the cluster.kubeadm join --token 045c1c.04765c236e1bd8da 172.31.50.221:6443 \\             --discovery-token-ca-cert-hash sha256:redactedOnce all the nodes have been joined check the status.$ kubectl get nodeNAME                  STATUS    ROLES     AGE       VERSIONkm1.virtomation.com   Ready     master    11m       v1.9.4kn1.virtomation.com   Ready     &lt;none&gt;    10m       v1.9.4kn2.virtomation.com   Ready     &lt;none&gt;    10m       v1.9.4Additional ComponentsKubeVirtThe recommended installation method is to use kubevirt-ansible. For this example I don’t require storage so just deploying using kubectl create.For additional information regarding KubeVirt install see the installation readme.$ kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/v0.4.1/kubevirt.yamlserviceaccount \"kubevirt-apiserver\" created... output ...customresourcedefinition \"offlinevirtualmachines.kubevirt.io\" createdLet’s make sure that all the pods are running.$ kubectl get pod -n kube-system -l 'kubevirt.io'NAME                               READY     STATUS    RESTARTS   AGEvirt-api-747745669-62cww           1/1       Running   0          4mvirt-api-747745669-qtn7f           1/1       Running   0          4mvirt-controller-648945bbcb-dfpwm   0/1       Running   0          4mvirt-controller-648945bbcb-tppgx   1/1       Running   0          4mvirt-handler-xlfc2                 1/1       Running   0          4mvirt-handler-z5lsh                 1/1       Running   0          4mSkydiveI have used Skydive in the past. It is a great tool to understand the topology of software-defined-networking. The only caveat is that Skydive doesn’t create a complete topology when using Flannel but there is still a good picture of what is going on. So with that let’s go ahead and install.kubectl create ns skydivekubectl create -n skydive -f https://raw.githubusercontent.com/skydive-project/skydive/master/contrib/kubernetes/skydive.yamlCheck the status of Skydive agent and analyzer$ kubectl get pod -n skydiveNAME                                READY     STATUS    RESTARTS   AGEskydive-agent-5hh8k                 1/1       Running   0          5mskydive-agent-c29l7                 1/1       Running   0          5mskydive-analyzer-5db567b4bc-m77kq   2/2       Running   0          5mingress-nginxTo provide external access our example NodeJS application we need to an ingress controller. For this example we are going to using ingress-nginxI created a simple script ingress.sh that follows the installation documentation for ingress-nginx with a couple minor modifications:      Patch the nginx-configuration ConfigMap to enable vts status        Add an additional containerPort to the deployment and an additional port to the service.        Create an ingress to access nginx status page  The script and additional files are available in the github repo listed below.git clone https://github.com/jcpowermac/kubevirt-network-deepdivecd kubevirt-network-deepdive/kubernetes/ingressbash ingress.shAfter the script is complete confirm that ingress-nginx pods are running.$ kubectl get pod -n ingress-nginxNAME                                        READY     STATUS    RESTARTS   AGEdefault-http-backend-55c6c69b88-jpl95       1/1       Running   0          1mnginx-ingress-controller-85c8787886-vf5tp   1/1       Running   0          1mKubeVirt Virtual MachinesNow we are at a point where we can deploy our first KubeVirt virtual machines. These instances are where we will install our simple NodeJS and MongoDB application.Create objectsLet’s create a clean new namespace to use.$ kubectl create ns nodejs-exnamespace \"nodejs-ex\" createdThe nodejs-ex.yaml contains multiple objects. The definitions for our two virtual machines - mongodb and nodejs. Two Kubernetes Services and a one Kubernetes Ingress object. These instances will be created as offline virtual machines so after kubectl create we will start them up.$ kubectl create -f https://raw.githubusercontent.com/jcpowermac/kubevirt-network-deepdive/master/kubernetes/nodejs-ex.yaml -n nodejs-exofflinevirtualmachine \"nodejs\" createdofflinevirtualmachine \"mongodb\" createdservice \"mongodb\" createdservice \"nodejs\" createdingress \"nodejs\" createdStart the nodejs virtual machine$ kubectl patch offlinevirtualmachine nodejs --type merge -p '{\"spec\":{\"running\":true}}' -n nodejs-exofflinevirtualmachine \"nodejs\" patchedStart the mongodb virtual machine$ kubectl patch offlinevirtualmachine mongodb --type merge -p '{\"spec\":{\"running\":true}}' -n nodejs-exofflinevirtualmachine \"mongodb\" patchedReview kubevirt virtual machine objects$ kubectl get ovms -n nodejs-exNAME      AGEmongodb   7mnodejs    7m$ kubectl get vms -n nodejs-exNAME      AGEmongodb   4mnodejs    5mWhere are the virtual machines and what is their IP address?$ kubectl get pod -o wide -n nodejs-exNAME                          READY     STATUS    RESTARTS   AGE       IP           NODEvirt-launcher-mongodb-qdpmg   2/2       Running   0          4m        10.244.2.7   kn2.virtomation.comvirt-launcher-nodejs-5r59c    2/2       Running   0          4m        10.244.1.8   kn1.virtomation.com  Note  To test virtual machine to virtual machine network connectivity I purposely set the host where which instance would run by using a nodeSelector.Installing the NodeJS Example ApplicationTo quickly deploy our example application Ansible project is included in the repository. Two inventory files need to be modified before executing ansible-playbook. Within all.yml change the analyzers IP address to what is listed in the command below.$ kubectl get endpoints -n skydiveNAME               ENDPOINTS                                                      AGEskydive-analyzer   10.244.1.2:9200,10.244.1.2:12379,10.244.1.2:8082 + 1 more...   18hAnd finally use the IP Addresses from the kubectl get pod -o wide -n nodejs-ex command (example above) to modify inventory/hosts.ini. Now we can run ansible-playbook.cd kubevirt-network-deepdive/ansiblevim inventory/group_vars/all.ymlvim inventory/hosts.iniansible-playbook -i inventory/hosts.ini playbook/main.yml... output ...Determine Ingress URLFirst let’s find the host. This is defined within the Ingress object. In this case it is nodejs.ingress.virtomation.com.$ kubectl get ingress -n nodejs-exNAME      HOSTS                            ADDRESS   PORTS     AGEnodejs    nodejs.ingress.virtomation.com             80        22mWhat are the NodePorts? For this installation Service spec was modified to include nodePort for http (30000) and http-mgmt (32000).  Note  When deploying ingress-nginx using the provided Service definition the nodePort is undefined. Kubernetes will assign a random port to ports defined in the spec.$ kubectl get service ingress-nginx -n ingress-nginxNAME            TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                                      AGEingress-nginx   NodePort   10.110.173.97   &lt;none&gt;        80:30000/TCP,443:30327/TCP,18080:32000/TCP   52mWhat node is the nginx-ingress controller running on? This is needed to configure DNS.$ kubectl get pod -n ingress-nginx -o wideNAME                                        READY     STATUS    RESTARTS   AGE       IP           NODEdefault-http-backend-55c6c69b88-jpl95       1/1       Running   0          53m       10.244.1.3   kn1.virtomation.comnginx-ingress-controller-85c8787886-vf5tp   1/1       Running   0          53m       10.244.1.4   kn1.virtomation.comConfigure DNSIn my homelab I am using dnsmasq. To support ingress add the host where the controller is running as an A record.[root@dns1 ~]# cat /etc/dnsmasq.d/virtomation.conf... output ...address=/km1.virtomation.com/172.31.50.221address=/kn1.virtomation.com/172.31.50.231address=/kn2.virtomation.com/172.31.50.232# Needed for nginx-ingressaddress=/.ingress.virtomation.com/172.31.50.231... output ...Restart dnsmasq for the new configsystemctl restart dnsmasqTesting our applicationThis application uses MongoDB to store the views of the website. Listing the count-value shows that the database is connected and networking is functioning correctly.$ curl http://nodejs.ingress.virtomation.com:30000/&lt;!doctype html&gt;&lt;html lang=\"en\"&gt;...output...&lt;p&gt;Page view count:&lt;span class=\"code\" id=\"count-value\"&gt;7&lt;/span&gt;&lt;/p&gt;...output...KubeVirt NetworkingNow that we shown that kubernetes, kubevirt, ingress-nginx and flannel work together how is it accomplished? First let’s go over what is going on in kubevirt specifically.virt-launcher - virtwrapvirt-launcher is the pod that runs the necessary components instantiate and run a virtual machine. We are only going to concentrate on the network portion in this post.virtwrap managerBefore the virtual machine is started the preStartHook will run SetupPodNetwork.SetupPodNetwork → SetupDefaultPodNetworkThis function calls three functions that are detailed below discoverPodNetworkInterface, preparePodNetworkInterface and StartDHCPdiscoverPodNetworkInterfaceThis function gathers the following information about the pod interface:      IP Address        Routes        Gateway        MAC address  This is stored for later use in configuring DHCP.preparePodNetworkInterfacesOnce the current details of the pod interface have been stored following operations are performed:      Delete the IP address from the pod interface        Set the pod interface down        Change the pod interface MAC address        Set the pod interface up        Create the bridge        Add the pod interface to the bridge  This will provide libvirt a bridge to use for the virtual machine that will be created.StartDHCP → DHCPServer → SingleClientDHCPServerThis DHCP server only provides a single address to a client in this case the virtual machine that will be started. The network details - the IP address, gateway, routes, DNS servers and suffixes are taken from the pod which will be served to the virtual machine.Networking in detailNow that we have a clearier picture of kubevirt networking we will continue with details regarding kubernetes objects, host, pod and virtual machine networking components. Then we will finish up with two scenarios: virtual machine to virtual machine communication and ingress to virtual machine.Kubernetes-levelservicesThere are two services defined in the manifest that was deployed above. One each for mongodb and nodejs applications. This allows us to use the hostname mongodb to connect to MongoDB. Review DNS for Services and Pods for additional information.$ kubectl get services -n nodejs-exNAME      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)     AGEmongodb   ClusterIP   10.108.188.170   &lt;none&gt;        27017/TCP   3hnodejs    ClusterIP   10.110.233.114   &lt;none&gt;        8080/TCP    3hendpointsThe endpoints below were automatically created because there was a selectorspec:  selector:    kubevirt.io: virt-launcher    kubevirt.io/domain: nodejsdefined in the Service object.$ kubectl get endpoints -n nodejs-exNAME      ENDPOINTS          AGEmongodb   10.244.2.7:27017   1hnodejs    10.244.1.8:8080    1hingressAlso defined in the manifest was the ingress object. This will allow us to contact the NodeJS example application using a URL.$ kubectl get ingress -n nodejs-exNAME      HOSTS                            ADDRESS   PORTS     AGEnodejs    nodejs.ingress.virtomation.com             80        3hHost-levelinterfacesA few important interfaces to note. The flannel.1 interface is type vxlan for connectivity between hosts. I removed from the ip a output the veth interfaces but the details are shown further below with bridge link show.[root@kn1 ~]# ip a...output...2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000    link/ether 52:54:00:97:a6:ee brd ff:ff:ff:ff:ff:ff    inet 172.31.50.231/24 brd 172.31.50.255 scope global eth0       valid_lft forever preferred_lft forever    inet6 fe80::5054:ff:fe97:a6ee/64 scope link       valid_lft forever preferred_lft forever...output...4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN    link/ether ce:4e:fb:41:1d:af brd ff:ff:ff:ff:ff:ff    inet 10.244.1.0/32 scope global flannel.1       valid_lft forever preferred_lft forever    inet6 fe80::cc4e:fbff:fe41:1daf/64 scope link       valid_lft forever preferred_lft forever5: cni0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP qlen 1000    link/ether 0a:58:0a:f4:01:01 brd ff:ff:ff:ff:ff:ff    inet 10.244.1.1/24 scope global cni0       valid_lft forever preferred_lft forever    inet6 fe80::341b:eeff:fe06:7ec/64 scope link       valid_lft forever preferred_lft forever...output...cni0 is a bridge where one side of the veth interface pair is attached.[root@kn1 ~]# bridge link show6: vethb4424886 state UP @docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 master cni0 state forwarding priority 32 cost 27: veth1657737b state UP @docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 master cni0 state forwarding priority 32 cost 28: vethdfd32c87 state UP @docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 master cni0 state forwarding priority 32 cost 29: vethed0f8c9a state UP @docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 master cni0 state forwarding priority 32 cost 210: veth05e4e005 state UP @docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 master cni0 state forwarding priority 32 cost 211: veth25933a54 state UP @docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 master cni0 state forwarding priority 32 cost 212: vethe3d701e7 state UP @docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 master cni0 state forwarding priority 32 cost 2routesThe pod network subnet is 10.244.0.0/16 and broken up per host:      km1 - 10.244.0.0/24        kn1 - 10.244.1.0/24        kn2 - 10.244.2.0/24  So the table will route the packets to correct interface.[root@kn1 ~]# ip rdefault via 172.31.50.1 dev eth010.244.0.0/24 via 10.244.0.0 dev flannel.1 onlink10.244.1.0/24 dev cni0 proto kernel scope link src 10.244.1.110.244.2.0/24 via 10.244.2.0 dev flannel.1 onlink172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1172.31.50.0/24 dev eth0 proto kernel scope link src 172.31.50.231iptablesTo also support kubernetes services kube-proxy writes iptables rules for those services. In the output below you can see our mongodb and nodejs services with destination NAT rules defined. For more information regarding iptables and services refer to debug-service in the kubernetes documentation.[root@kn1 ~]# iptables -n -L -t nat | grep nodejs-exKUBE-MARK-MASQ  all  --  10.244.1.8           0.0.0.0/0            /* nodejs-ex/nodejs: */DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            /* nodejs-ex/nodejs: */ tcp to:10.244.1.8:8080KUBE-MARK-MASQ  all  --  10.244.2.7           0.0.0.0/0            /* nodejs-ex/mongodb: */DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            /* nodejs-ex/mongodb: */ tcp to:10.244.2.7:27017KUBE-MARK-MASQ  tcp  -- !10.244.0.0/16        10.108.188.170       /* nodejs-ex/mongodb: cluster IP */ tcp dpt:27017KUBE-SVC-Z7W465PEPK7G2UVQ  tcp  --  0.0.0.0/0            10.108.188.170       /* nodejs-ex/mongodb: cluster IP */ tcp dpt:27017KUBE-MARK-MASQ  tcp  -- !10.244.0.0/16        10.110.233.114       /* nodejs-ex/nodejs: cluster IP */ tcp dpt:8080KUBE-SVC-LATB7COHB4ZMDCEC  tcp  --  0.0.0.0/0            10.110.233.114       /* nodejs-ex/nodejs: cluster IP */ tcp dpt:8080KUBE-SEP-JOPA2J4R76O5OVH5  all  --  0.0.0.0/0            0.0.0.0/0            /* nodejs-ex/nodejs: */KUBE-SEP-QD4L7MQHCIVOWZAO  all  --  0.0.0.0/0            0.0.0.0/0            /* nodejs-ex/mongodb: */Pod-levelinterfacesThe bridge br1 is the main focus in the pod level. It contains the eth0 and vnet0 ports. eth0 becomes the uplink to the bridge which is the other side of the veth pair which is a port on the host’s cni0 bridge.  Important  Since eth0 has no IP address and br1 is in the self-assigned range the pod has no network access. There are also no routes in the pod. This can be resolved for troubleshooting by creating a veth pair, adding one of the interfaces to the bridge and assigning an IP address in the pod subnet for the host. Routes are also required to be added. This is performed for running skydive in the pod see skydive.sh for more details.$ kubectl exec -n nodejs-ex -c compute virt-launcher-nodejs-5r59c -- ip a...output...3: eth0@if12: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue master br1 state UP group default    link/ether a6:97:da:96:cf:07 brd ff:ff:ff:ff:ff:ff link-netnsid 0    inet6 fe80::a497:daff:fe96:cf07/64 scope link       valid_lft forever preferred_lft forever4: br1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default    link/ether 32:8a:f5:59:10:02 brd ff:ff:ff:ff:ff:ff    inet 169.254.75.86/32 brd 169.254.75.86 scope global br1       valid_lft forever preferred_lft forever    inet6 fe80::a497:daff:fe96:cf07/64 scope link       valid_lft forever preferred_lft forever5: vnet0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc pfifo_fast master br1 state UNKNOWN group default qlen 1000    link/ether fe:58:0a:f4:01:08 brd ff:ff:ff:ff:ff:ff    inet6 fe80::fc58:aff:fef4:108/64 scope link       valid_lft forever preferred_lft foreverShowing the bridge br1 member ports.$ kubectl exec -n nodejs-ex -c compute virt-launcher-nodejs-5r59c -- bridge link show3: eth0 state UP @if12: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 master br1 state forwarding priority 32 cost 25: vnet0 state UNKNOWN : &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 master br1 state forwarding priority 32 cost 100DHCPThe virtual machine network is configured by DHCP. You can see virt-launcher has UDP port 67 open on the br1 interface to serve DHCP to the virtual machine.$ kubectl exec -n nodejs-ex -c compute virt-launcher-nodejs-5r59c -- ss -tuapnNetid  State    Recv-Q   Send-Q      Local Address:Port      Peer Address:Portudp    UNCONN   0        0             0.0.0.0%br1:67             0.0.0.0:*      users:((\"virt-launcher\",pid=10,fd=12))libvirtWith virsh domiflist we can also see that the vnet0 interface is a port on the br1 bridge.$ kubectl exec -n nodejs-ex -c compute virt-launcher-nodejs-5r59c -- virsh domiflist nodejs-ex_nodejsInterface  Type       Source     Model       MACvnet0      bridge     br1        e1000       0a:58:0a:f4:01:08VM-levelinterfacesFortunately the vm interfaces are fairly typical. Just the single interface that has been assigned the original pod ip address.  Warning  The MTU of the virtual machine interface is set to 1500. The network interfaces upstream are set to 1450.[fedora@nodejs ~]$ ip a...output...2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000    link/ether 0a:58:0a:f4:01:08 brd ff:ff:ff:ff:ff:ff    inet 10.244.1.8/24 brd 10.244.1.255 scope global dynamic eth0       valid_lft 86299761sec preferred_lft 86299761sec    inet6 fe80::858:aff:fef4:108/64 scope link       valid_lft forever preferred_lft foreverDNSJust quickly wanted to cat the /etc/resolv.conf file to show that DNS is configured so that kube-dns will be properly queried.[fedora@nodejs ~]$ cat /etc/resolv.conf; generated by /usr/sbin/dhclient-scriptsearch nodejs-ex.svc.cluster.local. svc.cluster.local. cluster.local.nameserver 10.96.0.10VM to VM communicationThe virtual machines are on differnet hosts. This was done purposely to show that connectivity between virtual machine and hosts. Here we finally get to use Skydive. The real-time topology below along with arrows annotate the flow of packets between the host, pod and virtual machine network devices.Connectivity TestsTo confirm connectivity we are going to do a few things. First check for DNS resolution for the mongodb service. Next look a established connection to MongoDB and finally check the NodeJS logs looking for confirmation of database connection.DNS resolutionService-based DNS resolution is an important feature of Kubernetes. Since dig,host or nslookup are not installed in our virtual machine a quick python script fills in. This output below shows that the mongodb name is available for resolution.[fedora@nodejs ~]$ python3 -c \"import socket;print(socket.gethostbyname('mongodb.nodejs-ex.svc.cluster.local'))\"10.108.188.170[fedora@nodejs ~]$ python3 -c \"import socket;print(socket.gethostbyname('mongodb'))\"10.108.188.170TCP connectionAfter connecting to the nodejs virtual machine via ssh we can use ss to determine the current TCP connections. We are specifically looking for the established connections to the MongoDB service that is running on the mongodb virtual machine on node kn2.[fedora@nodejs ~]$ ss -tanpState      Recv-Q Send-Q                Local Address:Port                               Peer Address:Port... output ...LISTEN     0      128                               *:8080                                          *:*ESTAB      0      0                        10.244.1.8:47826                            10.108.188.170:27017ESTAB      0      0                        10.244.1.8:47824                            10.108.188.170:27017... output ...Logs[fedora@nodejs ~]$ journalctl -u nodejs...output..Apr 18 20:07:37 nodejs.localdomain node[4303]: Connected to MongoDB at: mongodb://nodejs:nodejspassword@mongodb/nodejs...output...Ingress to VM communicationThe topology image below shows the packet flow when using a ingress kubernetes object. The commands below the image will provide additional details.The kube-proxy has port 30000 open that was defined by the nodePort of the ingress-nginx service. Additional details on kube-proxy and iptables role is available from Service - IPs and VIPs in the Kubernetes documentation.[root@kn1 ~]# ss -tanp | grep 30000LISTEN     0      128         :::30000                   :::*                   users:((\"kube-proxy\",pid=6534,fd=13))[root@kn1 ~]# iptables -n -L -t nat | grep ingress-nginx/ingress-nginx | grep http | grep -v https | grep -v http-mgmtKUBE-MARK-MASQ  tcp  --  0.0.0.0/0            0.0.0.0/0            /* ingress-nginx/ingress-nginx:http */ tcp dpt:30000KUBE-SVC-REQ4FPVT7WYF4VLA  tcp  --  0.0.0.0/0            0.0.0.0/0            /* ingress-nginx/ingress-nginx:http */ tcp dpt:30000KUBE-MARK-MASQ  all  --  10.244.1.4           0.0.0.0/0            /* ingress-nginx/ingress-nginx:http */DNAT       tcp  --  0.0.0.0/0            0.0.0.0/0            /* ingress-nginx/ingress-nginx:http */ tcp to:10.244.1.4:80KUBE-MARK-MASQ  tcp  -- !10.244.0.0/16        10.110.173.97        /* ingress-nginx/ingress-nginx:http cluster IP */ tcp dpt:80KUBE-SVC-REQ4FPVT7WYF4VLA  tcp  --  0.0.0.0/0            10.110.173.97        /* ingress-nginx/ingress-nginx:http cluster IP */ tcp dpt:80KUBE-SEP-BKJT4JXHZ3TCOTKA  all  --  0.0.0.0/0            0.0.0.0/0            /* ingress-nginx/ingress-nginx:http */Since the ingress-nginx pod is on the same host as the nodejs virtual machine we just need to be routed to the cni0 bridge to communicate with the pod and vm.[root@kn1 ~]# ip r...output...10.244.1.0/24 dev cni0 proto kernel scope link src 10.244.1.1...output...Connectivity TestsIn the section where we installed the application we already tested for connectivity but let’s take this is little further to confirm.Nginx Vhost Traffic Statusingress-nginx provides an optional setting to enable traffic status - which we already enabled. The screenshot below shows the requests that Nginx is receiving for nodejs.ingress.virtomation.com.Service NodePort to Nginx PodMy tcpdump fu is lacking so I found an example query that will provide the details we are looking for. I removed a significant amount of the content but you can see my desktop (172.31.51.52) create a GET request to the NodePort 30000. This could have also been done in Skydive but I wanted to provide an alternative if you didn’t want to install it or just stick to the cli.# tcpdump -nni eth0 -A -s 0 'tcp port 30000 and (((ip[2:2] - ((ip[0]&amp;0xf)&lt;&lt;2)) - ((tcp[12]&amp;0xf0)&gt;&gt;2)) != 0)'...output...13:24:52.197092 IP 172.31.51.52.36494 &gt; 172.31.50.231.30000: Flags [P.], seq 2685726663:2685727086, ack 277056091, win 491, options [nop,nop,TS val 267689990 ecr 151714950], length 423E... .@.?.Z...34..2...u0.......[....r............GET / HTTP/1.1Host: nodejs.ingress.virtomation.com:30000User-Agent: Mozilla/5.0 (X11; Fedora; Linux x86_64; rv:59.0) Gecko/20100101 Firefox/59.0Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8Accept-Language: en-US,en;q=0.5Accept-Encoding: gzip, deflateConnection: keep-aliveUpgrade-Insecure-Requests: 1If-None-Match: W/\"9edb-O5JGhneli0eCE6G2kFY5haMKg5k\"Cache-Control: max-age=013:24:52.215284 IP 172.31.50.231.30000 &gt; 172.31.51.52.36494: Flags [P.], seq 1:2362, ack 423, win 236, options [nop,nop,TS val 151723713 ecr 267689990], length 2361E.      m|.@.?.....2...34u0.....[...n...........        .......HTTP/1.1 200 OK        Server: nginx/1.13.12        Date: Fri, 20 Apr 2018 13:24:52 GMT        Content-Type: text/html; charset=utf-8        Transfer-Encoding: chunked        Connection: keep-alive        Vary: Accept-Encoding        X-Powered-By: Express        ETag: W/\"9edb-SZeP35LuygZ9MOrPTIySYOu9sAE\"        Content-Encoding: gzipNginx Pod to NodeJS VMIn (1) we can see flows to and from 10.244.1.4 and 10.244.1.8. .8 is the nodejs virtual machine and .4 is as listed below the nginx-ingress-controller.$ kubectl get pod --all-namespaces -o wideNAMESPACE       NAME                                          READY     STATUS    RESTARTS   AGE       IP              NODE...output...ingress-nginx   nginx-ingress-controller-85c8787886-vf5tp     1/1       Running   0          1d        10.244.1.4      kn1.virtomation.com...output...Final ThoughtsWe have went through quite a bit in this deep dive from installation, KubeVirt specific networking details and kubernetes, host, pod and virtual machine level configurations. Finishing up with the packet flow between virtual machine to virtual machine and ingress to virtual machine.",
        "url": "/2018/KubeVirt-Network-Deep-Dive.html"
      }
      ,
    
      "2018-this-week-in-kube-virt-22-html": {
        "title": "This Week In Kube Virt 22",
        "author": "fabiand",
        "category": "updates",
        "content": "This is a close-to weekly update from the KubeVirt team.In general there is now more work happening outside of the core kubevirtrepository.We are currently driven by      Building a solid user-story around KubeVirt        Caring about end-to-end (backend, core, ui)        Getting dependencies into shape (storage)        Improve the user-experience for users (UI, deployment)        Being easier to be used on Kubernetes and OpenShift  Within the last two weeks we achieved to:      Release KubeVirt v0.4.0(https://github.com/kubevirt/kubevirt/releases/tag/v0.4.0)        Many networking fixes (@mlsorensen @vladikr)(https://github.com/kubevirt/kubevirt/pull/870https://github.com/kubevirt/kubevirt/pull/869https://github.com/kubevirt/kubevirt/pull/847https://github.com/kubevirt/kubevirt/pull/856https://github.com/kubevirt/kubevirt/pull/839https://github.com/kubevirt/kubevirt/pull/830)        Aligned config reading for virtctl (@rmohr)(https://github.com/kubevirt/kubevirt/pull/860)        Subresource Aggregated API server for console endpoints (@vossel)(https://github.com/kubevirt/kubevirt/pull/770)        Enable OpenShift tests in CI (@alukiano @rmohr)(https://github.com/kubevirt/kubevirt/pull/833)        virtctl convenience functions for start/stop of VMs (@sgott)(https://github.com/kubevirt/kubevirt/pull/817)        Ansible - Improved Gluster support for kubevirt-ansible(https://github.com/kubevirt/kubevirt-ansible/pull/174)        POC Device Plugins for KVM and network (@mpolednik @phoracek)https://github.com/kubevirt/kubernetes-device-plugins  In addition to this, we are also working on:      Additional network glue approach (@vladikr)(https://github.com/kubevirt/kubevirt/pull/787)        CRD validation using OpenAPIv3 (@vossel)(https://github.com/kubevirt/kubevirt/pull/850)        Windows VM tests (@alukiano)(https://github.com/kubevirt/kubevirt/pull/809)        Data importer - Functional tests(https://github.com/kubevirt/containerized-data-importer/pull/81)  Take a look at the pulse, to get an overview over all changes of thisweek: https://github.com/kubevirt/kubevirt/pulseFinally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesAnd keep track of events at our calendar18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com“&gt;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&lt;/link&gt;If you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;",
        "url": "/2018/This-Week-in-Kube-Virt-22.html"
      }
      ,
    
      "2018-this-week-in-kube-virt-21-html": {
        "title": "This Week In Kube Virt 21",
        "author": "fabiand",
        "category": "updates",
        "content": "This is a weekly update from the KubeVirt team.In general there is now more work happening outside of the core kubevirtrepository.We are currently driven by      Building a solid user-story around KubeVirt        Caring about end-to-end (backend, core, ui)        Getting dependencies into shape (storage)        Improve the user-experience for users (UI, deployment)        Being easier to be used on Kubernetes and OpenShift  Within the last two weeks we achieved to:      Multi platform (Windows, Mac, Linux) support for virtctl (@slintes)(https://github.com/kubevirt/kubevirt/pull/811)        Stable UUIDs for OfflineVirtualMachines (@fromanirh)(https://github.com/kubevirt/kubevirt/pull/766)        OpenShift support for CI (@alukiano, @rmohr)(https://github.com/kubevirt/kubevirt/pull/792)        v2v improvements - for easier imports of existing VMs (@pkliczewski)(https://github.com/kubevirt/v2v-job)        Data importer - to import existing disk images (@copejon @jeffvance)(https://github.com/kubevirt/containerized-data-importer)        POC Device Plugins for KVM and network (@mpolednik @phoracek)https://github.com/kubevirt/kubernetes-device-plugins  In addition to this, we are also working on:      Subresources for consoles (@davidvossel)(https://github.com/kubevirt/kubevirt/pull/770)        Additional network glue approach (@vladikr)(https://github.com/kubevirt/kubevirt/pull/787)        virtctl convenience functions for start/stop of VMs (@sgott)(https://github.com/kubevirt/kubevirt/pull/817)  Take a look at the pulse, to get an overview over all changes of thisweek: https://github.com/kubevirt/kubevirt/pulseFinally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesAnd keep track of events at our calendar18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com“&gt;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&lt;/link&gt;If you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;",
        "url": "/2018/This-Week-in-Kube-Virt-21.html"
      }
      ,
    
      "2018-this-week-in-kube-virt-20-html": {
        "title": "This Week In Kube Virt 20",
        "author": "fabiand",
        "category": "updates",
        "content": "This is a weekly update from the KubeVirt team.We are currently driven by      Building a solid user-story around KubeVirt        Caring about end-to-end (backend, core, ui)        Getting dependencies into shape (storage)        Improve the user-experience for users (UI, deployment)        Being easier to be used on Kubernetes and OpenShift  Within the last two weeks we achieved to:      Released KubeVirt v0.3.0https://github.com/kubevirt/kubevirt/releases/tag/v0.3.0        Merged VirtualMachinePresets (@stu-gott)(https://github.com/kubevirt/kubevirt/pull/652)        Merged OfflineVirtualMachine (@pkotas)(https://github.com/kubevirt/kubevirt/pull/667)        Merged ephemeral disk support (@alukiano)(https://github.com/kubevirt/kubevirt/pull/728)        Fixes to test KubeVirt on OpenShift (@alukiano)(https://github.com/kubevirt/kubevirt/pull/774)        Scheduler awareness of VM pods (@vladikr)(https://github.com/kubevirt/kubevirt/pull/673)        Plain text inline cloud-init (@alukiano)(https://github.com/kubevirt/kubevirt/pull/757)        Define guest specific labels to be used with presets (@yanirq)(https://github.com/kubevirt/kubevirt/pull/767)        Special note: A ton of automation, CI, and test fixes (@rmohr)  In addition to this, we are also working on:      Stable UUIDs for OfflineVirtualMachines (@fromanirh)(https://github.com/kubevirt/kubevirt/pull/766)        Subresources for consoles (@davidvossel)(https://github.com/kubevirt/kubevirt/pull/770)        Additional network glue approach (@vladikr)(https://github.com/kubevirt/kubevirt/pull/787)        Improvement for testing on OpenShift (@alukiano)(https://github.com/kubevirt/kubevirt/pull/792)  Take a look at the pulse, to get an overview over all changes of thisweek: https://github.com/kubevirt/kubevirt/pulseFinally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesAnd keep track of events at our calendar18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com“&gt;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&lt;/link&gt;If you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;",
        "url": "/2018/This-Week-in-Kube-Virt-20.html"
      }
      ,
    
      "2018-this-week-in-kube-virt-19-html": {
        "title": "This Week In Kube Virt 19",
        "author": "fabiand",
        "category": "updates",
        "content": "This is a bi-weekly update from the KubeVirt team.We are currently driven by      Building a solid user-story around KubeVirt        Caring about end-to-end (backend, core, ui)        Getting dependencies into shape (storage)        Improve the user-experience for users (UI, deployment)        Being easier to be used on Kubernetes and OpenShift  Within the last two weeks we achieved to:      Support for native file-system PVs as disk storage (@alukiano,@davidvossel) (https://github.com/kubevirt/kubevirt/pull/734,https://github.com/kubevirt/kubevirt/pull/671)        Support for native pod networking for VMs (@vladikr)(https://github.com/kubevirt/kubevirt/pull/686)        Many patches to improve kubevirt-ansible usability(https://github.com/kubevirt/kubevirt-ansible/pulse/monthly)        Introduce the kubernetes-device-plugins (@mpolednik)(https://github.com/kubevirt/kubernetes-device-plugins/)        Introduce the kubernetes-device-plugin for bridge networking(@mpolednik)(https://github.com/kubevirt/kubernetes-device-plugins/pull/4)        Add vendor/ tree (@davidvossel)(https://github.com/kubevirt/kubevirt/pull/715)        Expose disk bus (@fromani)(https://github.com/kubevirt/kubevirt/pull/672)        Allow deploying OpenShift in vagrant (@alukiano)(https://github.com/kubevirt/kubevirt/pull/631)        Release of v0.3.0-alpha.3(https://github.com/kubevirt/kubevirt/releases/tag/v0.3.0-alpha.3)  In addition to this, we are also working on:      Implement VirtualMachinePresets (@stu-gott)(https://github.com/kubevirt/kubevirt/pull/652)        Implement OfflineVirtualMachines (@pkotas)(https://github.com/kubevirt/kubevirt/pull/667)        Expose CPU requirements in VM pod (@vladikr)(https://github.com/kubevirt/kubevirt/pull/673)  Take a look at the pulse, to get an overview over all changes of thisweek: https://github.com/kubevirt/kubevirt/pulseFinally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesAnd keep track of events at our calendar18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com“&gt;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&lt;/link&gt;If you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;",
        "url": "/2018/This-Week-in-Kube-Virt-19.html"
      }
      ,
    
      "2018-this-week-in-kube-virt-18-html": {
        "title": "This Week In Kube Virt 18",
        "author": "fabiand",
        "category": "updates",
        "content": "This is a weekly update from the KubeVirt team.We are currently driven by      Building a solid user-story around KubeVirt        Caring about end-to-end (backend, core, ui)        Rework our architecture        Getting dependencies into shape (storage)        Improve the user-experience for users (UI, deployment)        Being easier to be used on Kubernetes and OpenShift  Within the last weeks we achieved to:      Move to a decentralized the architecture (@davidvossel)(https://github.com/kubevirt/kubevirt/pull/663)        Drop live migration for now (@davidvossel)(https://github.com/kubevirt/kubevirt/pull/702)        Change default network provider to flannel (@alukiano)(https://github.com/kubevirt/kubevirt/pull/710)        Adjust uuid API (@mpolednik)(https://github.com/kubevirt/kubevirt/pull/675)        Make cirros and alpine ready for q35 (@rmohr)(https://github.com/kubevirt/kubevirt/pull/688)  In addition to this, we are also working on:      Decentralized pod networking (@vladikr)(https://github.com/kubevirt/kubevirt/pull/686)        Implement VirtualMachinePresets (@stu-gott)(https://github.com/kubevirt/kubevirt/pull/652)        Implement OfflineVirtualMachines (@pkotas)(https://github.com/kubevirt/kubevirt/pull/667)        Allow deploying OpenShift in vagrant (@alukiano)(https://github.com/kubevirt/kubevirt/pull/631)        Expose CPU requirements in VM pod (@vladikr)(https://github.com/kubevirt/kubevirt/pull/673)        Add support for PVs via kubelet (@alukiano)(https://github.com/kubevirt/kubevirt/pull/671)  Take a look at the pulse, to get an overview over all changes of thisweek: https://github.com/kubevirt/kubevirt/pulseFinally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesAnd keep track of events at our calendar18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com“&gt;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&lt;/link&gt;If you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;",
        "url": "/2018/This-Week-in-Kube-Virt-18.html"
      }
      ,
    
      "2018-this-week-in-kube-virt-17-html": {
        "title": "This Week In Kube Virt 17",
        "author": "fabiand",
        "category": "updates",
        "content": "This is a weekly update from the KubeVirt team.We are currently driven by      Building a solid user-story around KubeVirt        Caring about end-to-end (backend, core, ui)        Rework our architecture        Getting dependencies into shape (storage)        Improve the user-experience for users (UI, deployment)        Being easier to be used on Kubernetes and OpenShift  Over the weekend you could have seen our talks at devconf.cz:      “Kubernetes Cloud Autoscaler for IsolatedWorkloads” by @rmohr        “Outcast: Virtualization in a containerworld?” by @fabiand  Within the last weeks we achieved to:      Introduced Fedora Cloud image for testing (@davidvossel)(https://github.com/kubevirt/kubevirt/pull/685)        Switch to q35 by default (@mpolednik)(https://github.com/kubevirt/kubevirt/pull/650)  In addition to this, we are also working on:      Decentralize the architecture (@davidvossel)(https://github.com/kubevirt/kubevirt/pull/663)        Decentralized pod networking (@vladikr)(https://github.com/kubevirt/kubevirt/pull/686)        Implement VirtualMachinePresets (@stu-gott)(https://github.com/kubevirt/kubevirt/pull/652)        Allow deploying OpenShift in vagrant (@alukiano)(https://github.com/kubevirt/kubevirt/pull/631)        Expose CPU requirements in VM pod (@vladikr)(https://github.com/kubevirt/kubevirt/pull/673)        Adjust uuid API (@mpolednik)(https://github.com/kubevirt/kubevirt/pull/675)        Make cirros and alpine ready for q35 (@rmohr)(https://github.com/kubevirt/kubevirt/pull/688)  Take a look at the pulse, to get an overview over all changes of thisweek: https://github.com/kubevirt/kubevirt/pulseFinally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesAnd keep track of events at our calendar18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com“&gt;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&lt;/link&gt;If you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;",
        "url": "/2018/This-Week-in-Kube-Virt-17.html"
      }
      ,
    
      "2018-this-week-in-kube-virt-16-size-xl-html": {
        "title": "This Week In Kube Virt 16 Size Xl",
        "author": "fabiand",
        "category": "updates",
        "content": "This is a weekly update from the KubeVirt team - including the holidaybacklog update.We are currently driven by      Building a solid user-story around KubeVirt        Caring about end-to-end (backend, core, ui)        Rework out architecture        Getting dependencies into shape (storage)        Improve the user-experience for users (UI, deployment)        Being easier to be used on Kubernetes and OpenShift  Within the last weeks we achieved to:      Drop of HAProxy and redeisng of console access (@davidvossel)(https://github.com/kubevirt/kubevirt/pull/618)        Dockerized builds to make sure the build env matches the runtime env(@rmohr and others)(https://github.com/kubevirt/kubevirt/pull/647)        OwnerReference fixes (@alukiano)(https://github.com/kubevirt/kubevirt/pull/642)        OfflineVirtualMachineDesign documentation (@petrkotas)(https://github.com/kubevirt/kubevirt/pull/641)        Further RBAC improvements (@gbenhaim)(https://github.com/kubevirt/kubevirt/pull/640)        User-Guide                  The guide saw many updates also for planned stuff                    Update to reflect v0.2.0 changes (@rmohr)(https://github.com/kubevirt/user-guide/pull/12)                    NodeSelector and affinity (@rmohr)(https://github.com/kubevirt/user-guide/pull/15)                    Hardware configuration (@rmohr)(https://github.com/kubevirt/user-guide/pull/14)                    Volumes and disks (@rmohr)(https://github.com/kubevirt/user-guide/pull/13)                    Cloud-Init (@davidvossel)(https://github.com/kubevirt/user-guide/pull/10)                  API Reference          Now updated regularly (@lukas-bednar)(https://github.com/kubevirt/kubevirt/pull/643)https://kubevirt-incubator.github.io/api-reference/content/index.html            Demo                  Got updated to v0.2.0 (@fabiand)                    But an issue with virtctl was introduced                    https://github.com/kubevirt/demo                  UI                  The WIP KubeVirt provider for ManageIQ was showcased(@masayag @pkliczewski)                    https://github.com/ManageIQ/manageiq-providers-kubevirt/                    Video: https://www.youtube.com/watch?v=9Gf2Nv7h558                    Screenshot:                  UI                  The Cockpit plugin makes some progress (@mlibra)                    https://github.com/cockpit-project/cockpit/wiki/Feature:-Kubernetes:-KubeVirt-support-enhancements                    https://github.com/cockpit-project/cockpit/pull/7830                    Screenshot:                  Ansible                  Move to stable kubevirt release manifests (@gbenhaim)(https://github.com/kubevirt-incubator/kubevirt-ansible/pull/37)                    Many improvements to make it work seamlessly(@gbenhaim @lukas-bednar)            In addition to this, we are also working on:      Decentralize the architecture (@davidvossel)(https://github.com/kubevirt/kubevirt/pull/663)        Implement VirtualMachinePresets (@stu-gott)(https://github.com/kubevirt/kubevirt/pull/652)        virtctl fixes (@davidvossel and @awels)(https://github.com/kubevirt/kubevirt/pull/648)        Move to q35 machine type (@mpolednik)(https://github.com/kubevirt/kubevirt/pull/650)        Allow deploying OpenShift in vagrant (@alukiano)(https://github.com/kubevirt/kubevirt/pull/631)                  User-Guide:                              Offline Virtual Machine docs (@petrkotas)(https://github.com/kubevirt/user-guide/pull/9)                                Persistent Virtual Machines (@stu-gott)(https://github.com/kubevirt/user-guide/pull/11)                                      Storage                              Working on enabling PV cloning using PVannotations (@aglitke)(https://github.com/aglitke/external-storage/tree/clone-poc)                                Working on optimizing Gluster for in-cluster storage                                Working on the ability to simplify VM image uploads                              Take a look at the pulse, to get an overview over all changes of thisweek: https://github.com/kubevirt/kubevirt/pulseFinally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesAnd keep track of events at our calendar18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com“&gt;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&lt;/link&gt;If you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;",
        "url": "/2018/This-Week-in-Kube-Virt-16-size-XL.html"
      }
      ,
    
      "2018-this-week-in-kube-virt-16-holiday-wrap-up-edition-html": {
        "title": "This Week In Kube Virt 16 Holiday Wrap Up Edition",
        "author": "fabiand",
        "category": "updates",
        "content": "This is a weekly update from the KubeVirt team - including the holidaybacklog update.We are currently driven by      Being easier to be used on Kubernetes and OpenShift        Rework out architecture        Getting dependencies into shape (storage)        Improve the user-experience for users (UI, deployment)  Within the last weeks we achieved to:      Drop of HAProxy and redeisng of console access (@davidvossel)(https://github.com/kubevirt/kubevirt/pull/618)        Dockerized builds to make sure the build env matches the runtime env(@rmohr and others)(https://github.com/kubevirt/kubevirt/pull/647)        OwnerReference fixes (@alukiano)(https://github.com/kubevirt/kubevirt/pull/642)        OfflineVirtualMachineDesign documentation (@petrkotas)(https://github.com/kubevirt/kubevirt/pull/641)        Further RBAC improvements (@gbenhaim)(https://github.com/kubevirt/kubevirt/pull/640)        User-Guide                  The guide saw many updates also for planned stuff                    Update to reflect v0.2.0 changes (@rmohr)(https://github.com/kubevirt/user-guide/pull/12)                    NodeSelector and affinity (@rmohr)(https://github.com/kubevirt/user-guide/pull/15)                    Hardware configuration (@rmohr)(https://github.com/kubevirt/user-guide/pull/14)                    Volumes and disks (@rmohr)(https://github.com/kubevirt/user-guide/pull/13)                    Cloud-Init (@davidvossel)(https://github.com/kubevirt/user-guide/pull/10)                  API Reference          Now updated regularly (@lukas-bednar)(https://github.com/kubevirt/kubevirt/pull/643)https://kubevirt-incubator.github.io/api-reference/content/index.html            Demo                  Got updated to v0.2.0 (@fabiand)                    But an issue with virtctl was introduced                    https://github.com/kubevirt/demo                  UI                  The WIP KubeVirt provider for ManageIQ was showcased(@masayag @pkliczewski)                    https://github.com/ManageIQ/manageiq-providers-kubevirt/                    https://www.youtube.com/watch?v=9Gf2Nv7h558                  UI          The Cockpit plugin makes some progress (@mlibra):            Ansible                  Move to stable kubevirt release manifests (@gbenhaim)(https://github.com/kubevirt-incubator/kubevirt-ansible/pull/37)                    Many improvements to make it work seamlessly(@gbenhaim @lukas-bednar)            In addition to this, we are also working on:      Decentralize the architecture (@davidvossel)(https://github.com/kubevirt/kubevirt/pull/663)        Implement VirtualMachinePresets (@stu-gott)(https://github.com/kubevirt/kubevirt/pull/652)        virtctl fixes (@davidvossel and @awels)(https://github.com/kubevirt/kubevirt/pull/648)        Move to q35 machine type (@mpolednik)(https://github.com/kubevirt/kubevirt/pull/650)        Allow deploying OpenShift in vagrant (@alukiano)(https://github.com/kubevirt/kubevirt/pull/631)        User-Guide:        Offline Virtual Machine docs (@petrkotas)(https://github.com/kubevirt/user-guide/pull/9)        Persistent Virtual Machines (@stu-gott)(https://github.com/kubevirt/user-guide/pull/11)  Take a look at the pulse, to get an overview over all changes of thisweek: https://github.com/kubevirt/kubevirt/pulseFinally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesAnd keep track of events at our calendar18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com“&gt;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&lt;/link&gt;If you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;",
        "url": "/2018/This-Week-in-Kube-Virt-16-Holiday-Wrap-Up-Edition.html"
      }
      ,
    
      "2018-some-notes-on-some-highlights-of-v020-html": {
        "title": "Some Notes On Some Highlights Of V020",
        "author": "fabiand",
        "category": "uncategorized",
        "content": "The very first KubeVirt release of KubeVirt in the new year(https://github.com/kubevirt/kubevirt/releases/v0.2.0) had a fewnotable highlights which were brewing over the last few weeks.VirtualMachine API redesign===========================Previously the VirtualMachine API was pretty much aligned, or a 1:1mapping, to libvirt’s domxml. With this change however, we took a stepback and redesigned the API to be more Kubernet-ish than libvirt-ish.Some changes, like the extraction of source volumes, will actually helpus to implement other patterns - like VirtualMachinePresets.Removal of HAProxyThis is another nice one. So far we were using a custom API server forperforming object validation. But the use of this custom API serverrequired that the client was accessing the custom API server, and notthe main one. The multiplexing of redirecting certain requests to ourand other requests to the main API server was done by HA proxy. Somewhatlike a poor mans API server aggregation.However, now we are focusing on CRDs completely (we considered to go toAPI server aggregation, but dropped this approach), which involves doingthe validation of the CRD based on a json scheme.Redesign of VNC/Console accessThe afore mentioned custom API server contained subresources to permitinbound access to the graphical and serial consoel of VMs. But this doesnot work with CRDs and thus we are now using a different approach toprovide access to those.The new implementation leverages the kubectl exec path in order topipe the graphical and serial console from the VM to the client. This ispretty nice, as we are leveraging Kubernetes for doing the piping, wemerely provide a kubectl plugin in order to ease the consumption ofthis. Side note is that the API of the kubectl plugin did actually notchange.",
        "url": "/2018/Some-notes-on-some-highlights-of-v020.html"
      }
      ,
    
      "2018-kube-virt-v020-html": {
        "title": "Kube Virt V020",
        "author": "fabiand",
        "category": "releases",
        "content": "This release follows v0.1.0 and consists of 131 changes, contributed by6 people, leading to 148 files changed, 9096 insertions(+), 5871deletions(-).The source code and selected binaries are available for download at:https://github.com/kubevirt/kubevirt/releases/tag/v0.2.0.The primary release artifact of KubeVirt is the git tree. The releasetag is signed and can be verified using [git-evtag][git-evtag].Pre-built containers are published on Docker Hub and can be viewed at:https://hub.docker.com/u/kubevirt/.Notable changes      VM launch and shutdown flow improvements        VirtualMachine API redesign        Removal of HAProxy        Redesign of VNC/Console access        Initial support for different vagrant providers  Contributors6 people contributed to this release:65    Roman Mohr &lt;rmohr@redhat.com&gt;60  David Vossel &lt;dvossel@redhat.com&gt; 2  Fabian Deutsch &lt;fabiand@redhat.com&gt; 2  Stu Gott &lt;sgott@redhat.com&gt; 1  Marek Libra &lt;mlibra@redhat.com&gt; 1  Martin Kletzander &lt;mkletzan@redhat.com&gt;Test Results                    Ran 40 of 42 Specs in 703.532 seconds SUCCESS! — 40 Passed        0 Failed                                      0 Pending        2 Skipped PASS            Additional Resources      Mailing list: https://groups.google.com/forum/#!forum/kubevirt-dev        IRC: &lt;irc://irc.freenode.net/#kubevirt&gt;        An easy to use demo: https://github.com/kubevirt/demo        [How to contribute][contributing]        [License][license]  [git-evtag]: https://github.com/cgwalters/git-evtag#using-git-evtag[contributing]:https://github.com/kubevirt/kubevirt/blob/master/CONTRIBUTING.md[license]: https://github.com/kubevirt/kubevirt/blob/master/LICENSE",
        "url": "/2018/Kube-Virt-v020.html"
      }
      ,
    
      "2017-this-week-in-kube-virt-15-html": {
        "title": "This Week In Kube Virt 15",
        "author": "fabiand",
        "category": "updates",
        "content": "This is a weekly update from the KubeVirt team.We are currently driven by      Being easier to be used on Kubernetes and OpenShift        Enabling people to contribute        Streamlining and improving the Kubernetes experience  This week we achieved to:  VM Grace period and shutdown improvements (@davidvossel)(https://github.com/kubevirt/kubevirt/pull/526)On another side we were also successful in:      Initiating a Kubernetes WG Virtualization mainlinglist:https://groups.google.com/forum/#!forum/kubernetes-wg-virtualization        Triggering a #virtualization slack channel in the Kubernetes org  In addition to this, we are also working on quite a few things:      Serial console rework (@davidvossel)(https://github.com/kubevirt/kubevirt/pull/613)        VM API Redesign (@rmohr)(https://github.com/kubevirt/kubevirt/pull/606)        Add OpenShift support (@karimb)(https://github.com/kubevirt/kubevirt/pull/608,https://github.com/kubevirt-incubator/kubevirt-ansible/pull/29)        Ansible Broker support (@gbenhaim)(https://github.com/kubevirt-incubator/kubevirt-ansible/pull/30)        Improve development builds (@petrkotas)(https://github.com/kubevirt/kubevirt/pull/609) - Take a look atthe pulse, to get an overview over all changes of this week:https://github.com/kubevirt/kubevirt/pulse  Finally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesAnd keep track of events at our calendar18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com“&gt;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&lt;/link&gt;If you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;",
        "url": "/2017/This-Week-in-Kube-Virt-15.html"
      }
      ,
    
      "2017-this-week-in-kube-virt-14-html": {
        "title": "This Week In Kube Virt 14",
        "author": "fabiand",
        "category": "updates",
        "content": "This is a weekly update from the KubeVirt team.We are currently driven by      Being easier to be used on Kubernetes and OpenShift        Enabling people to contribute  This week you could have met us at:  KubeCon NA: Virtualizing Workloads Saloon(https://kccncna17.sched.com/event/43ebdf89846d7f4939810bbaeb5a3229)Some minutes athttps://docs.google.com/document/d/1B3zbJA0MTQ82yu2JNMREEiVaQJTG3PfGBfdogsFISBE/editThis week we achieved to:      Release KubeVirt v0.1.0(https://github.com/kubevirt/kubevirt/releases/tag/v0.1.0)        Improve the manifest situation (@rmohr)(https://github.com/kubevirt/kubevirt/pull/602)  In addition to this, we are also working on:      OpenAPI improvements (@lukas-bednar)(https://github.com/kubevirt/kubevirt/pull/603)        Describe how device assignment can work (@mpolednik)(https://github.com/kubevirt/kubevirt/pull/593)        VM Unknown state (@rmohr)(https://github.com/kubevirt/kubevirt/issues/543)  Take a look at the pulse, to get an overview over all changes of thisweek: https://github.com/kubevirt/kubevirt/pulseFinally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesAnd keep track of events at our calendar18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com“&gt;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&lt;/link&gt;If you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;",
        "url": "/2017/This-Week-in-Kube-Virt-14.html"
      }
      ,
    
      "2017-kube-virt-v020-html": {
        "title": "Kube Virt V020",
        "author": "fabiand",
        "category": "releases",
        "content": "This release follows v0.1.0 and consists of 131 changes, contributed by6 people, leading to 148 files changed, 9096 insertions(+), 5871deletions(-).The source code and selected binaries are available for download at:https://github.com/kubevirt/kubevirt/releases/tag/v0.2.0.The primary release artifact of KubeVirt is the git tree. The releasetag is signed and can be verified using [git-evtag][git-evtag].Pre-built containers are published on Docker Hub and can be viewed at:https://hub.docker.com/u/kubevirt/.Notable changes      VM launch and shutdown flow improvements        VirtualMachine API redesign        Removal of HAProxy        Redesign of VNC/Console access        Initial support for different vagrant providers  Contributors6 people contributed to this release:65    Roman Mohr &lt;rmohr@redhat.com&gt;60  David Vossel &lt;dvossel@redhat.com&gt; 2  Fabian Deutsch &lt;fabiand@redhat.com&gt; 2  Stu Gott &lt;sgott@redhat.com&gt; 1  Marek Libra &lt;mlibra@redhat.com&gt; 1  Martin Kletzander &lt;mkletzan@redhat.com&gt;Test Results                    Ran 40 of 42 Specs in 703.532 seconds SUCCESS! — 40 Passed        0 Failed                                      0 Pending        2 Skipped PASS            Additional Resources      Mailing list: https://groups.google.com/forum/#!forum/kubevirt-dev        IRC: &lt;irc://irc.freenode.net/#kubevirt&gt;        An easy to use demo: https://github.com/kubevirt/demo        [How to contribute][contributing]        [License][license]  [git-evtag]: https://github.com/cgwalters/git-evtag#using-git-evtag[contributing]:https://github.com/kubevirt/kubevirt/blob/master/CONTRIBUTING.md[license]: https://github.com/kubevirt/kubevirt/blob/master/LICENSE",
        "url": "/2017/Kube-Virt-v020.html"
      }
      ,
    
      "2017-kube-virt-v010-html": {
        "title": "Kube Virt V010",
        "author": "fabiand",
        "category": "releases",
        "content": "This release follows v0.0.4 and consists of 115 changes, contributed by11 people, leading to 121 files changed, 5278 insertions(+), 1916deletions(-).The source code and selected binaries are available for download at:https://github.com/kubevirt/kubevirt/releases/tag/v0.1.0.The primary release artifact of KubeVirt is the git tree. The releasetag is signed and can be verified using [git-evtag][git-evtag].Pre-built containers are published on Docker Hub and can be viewed at:https://hub.docker.com/u/kubevirt/.Notable changes      Many API improvements for a proper OpenAPI reference        Add watchdog support        Drastically improve the deployment on non-vagrant setups        Dropped nodeSelectors        Separated inner component deployment from edge component deployment        Created separate manifests for developer, test, and releasedeployments        Moved komponents to kube-system namespace        Improved and unified flag parsing  Contributors11 people contributed to this release:42    Roman Mohr &lt;rmohr@redhat.com&gt;20  David Vossel &lt;dvossel@redhat.com&gt;18  Lukas Bednar &lt;lbednar@redhat.com&gt;14  Martin Polednik &lt;mpolednik@redhat.com&gt; 7  Fabian Deutsch &lt;fabiand@redhat.com&gt; 6  Lukianov Artyom &lt;alukiano@redhat.com&gt; 3  Vladik Romanovsky &lt;vromanso@redhat.com&gt; 2  Petr Kotas &lt;petr.kotas@gmail.com&gt; 1  Barak Korren &lt;bkorren@redhat.com&gt; 1  Francois Deppierraz &lt;francois@ctrlaltdel.ch&gt; 1  Saravanan KR &lt;skramaja@redhat.com&gt;Test Results                    Ran 44 of 46 Specs in 851.185 seconds SUCCESS! — 44 Passed        0 Failed                                      0 Pending        2 Skipped PASS            Additional Resources      Mailing list: https://groups.google.com/forum/#!forum/kubevirt-dev        IRC: &lt;irc://irc.freenode.net/#kubevirt&gt;        An easy to use demo: https://github.com/kubevirt/demo        [How to contribute][contributing]        [License][license]  [git-evtag]: https://github.com/cgwalters/git-evtag#using-git-evtag[contributing]:https://github.com/kubevirt/kubevirt/blob/master/CONTRIBUTING.md[license]: https://github.com/kubevirt/kubevirt/blob/master/LICENSE",
        "url": "/2017/Kube-Virt-v010.html"
      }
      ,
    
      "2017-this-week-in-kube-virt-13-html": {
        "title": "This Week In Kube Virt 13",
        "author": "fabiand",
        "category": "updates",
        "content": "This is a weekly update from the KubeVirt team.We are currently driven by      Being easier to be used on Kubernetes and OpenShift        Enabling people to contribute  This week you can meet us at:  KubeCon NA: Virtualizing Workloads Saloon(https://kccncna17.sched.com/event/43ebdf89846d7f4939810bbaeb5a3229)This week we still achieved to:  Owner References for VM ReplicaSet (@rmohr)(https://github.com/kubevirt/kubevirt/pull/596)In addition to this, we are also working on:      Manifest refactoring (@rmohr)(https://github.com/kubevirt/kubevirt/pull/602)        OpenAPI improvements (@lukas-bednar)(https://github.com/kubevirt/kubevirt/pull/603)        Describe how device assignment can work (@mpolednik)(https://github.com/kubevirt/kubevirt/pull/593)        VM Unknown state (@rmohr)(https://github.com/kubevirt/kubevirt/issues/543)  Take a look at the pulse, to get an overview over all changes of thisweek: https://github.com/kubevirt/kubevirt/pulseFinally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesAnd keep track of events at our calendar18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com“&gt;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&lt;/link&gt;If you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;",
        "url": "/2017/This-Week-in-Kube-Virt-13.html"
      }
      ,
    
      "2017-this-week-in-kube-virt-12-html": {
        "title": "This Week In Kube Virt 12",
        "author": "fabiand",
        "category": "updates",
        "content": "This is a weekly update from the KubeVirt team.We are currently driven by      Being easier to be used on Kubernetes and OpenShift        Enabling people to contribute  This week we was really slow, but we still achieved to:  Improve vagrant setup (@cynepco3hahue)(https://github.com/kubevirt/kubevirt/pull/586)In addition to this, we are also working on:      GlusterFS support (@humblec)(https://github.com/kubevirt/kubevirt/pull/578)        Describe how device assignment can work (@mpolednik)(https://github.com/kubevirt/kubevirt/pull/593)  Take a look at the pulse, to get an overview over all changes of thisweek: https://github.com/kubevirt/kubevirt/pulseFinally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesAnd keep track of events at our calendar18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com“&gt;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&lt;/link&gt;If you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;",
        "url": "/2017/This-Week-in-Kube-Virt-12.html"
      }
      ,
    
      "2017-this-week-in-kube-virt-11-html": {
        "title": "This Week In Kube Virt 11",
        "author": "fabiand",
        "category": "updates",
        "content": "This is a weekly update from the KubeVirt team.We are currently driven by      Being easier to be used on Kubernetes and OpenShift        Enabling people to contribute  This week we achieved to:      Generation of API documentation (@lukas-bednar)(https://github.com/kubevirt/kubevirt/pull/571)(https://kubevirt-incubator.github.io/api-reference/content/index.html)        Move components to kube-system namespace (@cynepco3hahue)(https://github.com/kubevirt/kubevirt/pull/558)        Use glide again (@davidvossel)(https://github.com/kubevirt/kubevirt/pull/576)  In addition to this, we are also working on:  GlusterFS support (@humblec)(https://github.com/kubevirt/kubevirt/pull/578)Take a look at the pulse, to get an overview over all changes of thisweek: https://github.com/kubevirt/kubevirt/pulseFinally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesAnd keep track of events at our calendar18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com“&gt;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&lt;/link&gt;If you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;",
        "url": "/2017/This-Week-in-Kube-Virt-11.html"
      }
      ,
    
      "2017-this-week-in-kube-virt-10-base-10-html": {
        "title": "This Week In Kube Virt 10 Base 10",
        "author": "fabiand",
        "category": "updates",
        "content": "This is a weekly update from the KubeVirt team.We are currently driven by      Being easier to be used on Kubernetes and OpenShift        Enabling people to contribute        Node Isolator use-case (more informations soon)  Non-code wise this week      The KVM Forum recording was published: “Running Virtual Machines onKubernetes with libvirt &amp; KVM by Fabian Deutsch &amp; Roman Mohr”(https://www.youtube.com/watch?v=Wh-ejUyuHJ0)        Preparing the “virtualization saloon” at KubeCon NA(https://kccncna17.sched.com/event/CU8m)  This week we achieved to:      Further improve API documentation (@lukas-bednar)(https://github.com/kubevirt/kubevirt/pull/549)        Virtual Machine watchdog device support (@davidvossel)(https://github.com/kubevirt/kubevirt/pull/544)        Introduction of virt-dhcp (@vladikr)(https://github.com/kubevirt/kubevirt/pull/525)        Less specific manifests(https://github.com/kubevirt/kubevirt/pull/560) (@fabiand)  In addition to this, we are also working on:      Addition of more tests to pod networking (@vladikr)(https://github.com/kubevirt/kubevirt/pull/525)        Adding helm charts (@cynepco3hahue)(https://github.com/kubernetes/charts/pull/2669)        Move manifests to kube-system namespace (@cynepco3hahue)(https://github.com/kubevirt/kubevirt/pull/558)        Drafting the publishing of API docs (@lukas-bednar)(https://github.com/kubevirt-incubator/api-reference)(https://kubevirt-incubator.github.io/api-reference/definitions.html)  Take a look at the pulse, to get an overview over all changes of thisweek: https://github.com/kubevirt/kubevirt/pulseFinally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesAnd keep track of events at our calendar18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com“&gt;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&lt;/link&gt;If you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;",
        "url": "/2017/This-Week-in-Kube-Virt-10-base-10.html"
      }
      ,
    
      "2017-kube-virt-v004-html": {
        "title": "Kube Virt V004",
        "author": "fabiand",
        "category": "releases",
        "content": "This release follows v0.0.3 and consists of 133 changes, contributed by14 people, leading to 109 files changed, 7093 insertions(+), 2437deletions(-).The source code and selected binaries are available for download at:https://github.com/kubevirt/kubevirt/releases/tag/v0.0.4.The primary release artifact of KubeVirt is the git tree. The releasetag is signed and can be verified using [git-evtag][git-evtag].Pre-built containers are published on Docker Hub and can be viewed at:https://hub.docker.com/u/kubevirt/.Notable changes===============      Add support for node affinity to VM.Spec        Add OpenAPI specification        Drop swagger 1.2 specification        virt-launcher refactoring        Leader election mechanism for virt-controller        Move from glide to dep for dependency management        Improve virt-handler synchronization loops        Add support for running the functional tests on oVirt infrastructure        Several tests fixes (spice, cleanup, …​)        Add console test tool        Improve libvirt event notification  Contributors14 people contributed to this release:46    David Vossel &lt;dvossel@redhat.com&gt;46  Roman Mohr &lt;rmohr@redhat.com&gt;12  Lukas Bednar &lt;lbednar@redhat.com&gt;11  Lukianov Artyom &lt;alukiano@redhat.com&gt; 4  Martin Sivak &lt;msivak@redhat.com&gt; 4  Petr Kotas &lt;pkotas@redhat.com&gt; 2  Fabian Deutsch &lt;fabiand@redhat.com&gt; 2  Milan Zamazal &lt;mzamazal@redhat.com&gt; 1  Artyom Lukianov &lt;alukiano@redhat.com&gt; 1  Barak Korren &lt;bkorren@redhat.com&gt; 1  Clifford Perry &lt;coperry94@gmail.com&gt; 1  Martin Polednik &lt;mpolednik@redhat.com&gt; 1  Stephen Gordon &lt;sgordon@redhat.com&gt; 1  Stu Gott &lt;sgott@redhat.com&gt;Test Results                    Ran 45 of 47 Specs in 797.286 seconds SUCCESS! — 45 Passed        0 Failed                                      0 Pending        2 Skipped PASS            Additional Resources      Mailing list: https://groups.google.com/forum/#!forum/kubevirt-dev        IRC: &lt;irc://irc.freenode.net/#kubevirt&gt;        An easy to use demo: https://github.com/kubevirt/demo        [How to contribute][contributing]        [License][license]  [git-evtag]: https://github.com/cgwalters/git-evtag#using-git-evtag[contributing]:https://github.com/kubevirt/kubevirt/blob/master/CONTRIBUTING.md[license]: https://github.com/kubevirt/kubevirt/blob/master/LICENSE",
        "url": "/2017/Kube-Virt-v004.html"
      }
      ,
    
      "2017-this-week-in-kube-virt-9-html": {
        "title": "This Week In Kube Virt 9",
        "author": "fabiand",
        "category": "updates",
        "content": "This is a weekly update from the KubeVirt team.We are currently driven by      Being easier to be used on Kubernetes and OpenShift        Enabling people to contribute        Node Isolator use-case (more informations soon)  This week we achieved to:      Release KubeVirt v0.0.4(https://github.com/kubevirt/kubevirt/releases/tag/v0.0.4)        virt-handler refactoring (@rmohr)(https://github.com/kubevirt/kubevirt/pull/530)        Add support for running functional tests on oVirt infrastructure(@bkorren) (https://github.com/kubevirt/kubevirt/pull/379)        Add OpenAPI specification (@lbednar)(https://github.com/kubevirt/kubevirt/pull/535)        Consolidate console functional tests (@dvossel)(https://github.com/kubevirt/kubevirt/pull/541)        Improve libvirt event notification (@rmohr)(https://github.com/kubevirt/kubevirt/pull/351)  In addition to this, we are also working on:      Addition of more tests to pod networking (@vladikr)(https://github.com/kubevirt/kubevirt/pull/525)        Watchdog support (@dvossel)(https://github.com/kubevirt/kubevirt/pull/544)        Leveraging ingress (@fabiand)(https://github.com/kubevirt/kubevirt/pull/538)        Adding helm charts (@cynepco3hahue)(https://github.com/kubernetes/charts/pull/2669)  Take a look at the pulse, to get an overview over all changes of thisweek: https://github.com/kubevirt/kubevirt/pulseFinally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesAnd keep track of events at our calendar18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com“&gt;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&lt;/link&gt;If you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;",
        "url": "/2017/This-Week-in-Kube-Virt-9.html"
      }
      ,
    
      "2017-this-week-in-kube-virt-8-html": {
        "title": "This Week In Kube Virt 8",
        "author": "fabiand",
        "category": "updates",
        "content": "This is a weekly update from the KubeVirt team.We are currently driven by      Being easier to be used on Kubernetes and OpenShift        Enabling people to contribute        Node Isolator use-case (more informations soon)  This week we achieved to:      Present at KVM Forum, Prague (@rmohr, @fabiand)http://redhat.slides.com/fdeutsch/running-virtual-machines-on-kubernetes-with-libvirt-and-kvm-at-kvm-forum-2017#/        Proposal on how to construct the VM API (@rmohr, @michalskrivanek)https://github.com/kubevirt/kubevirt/pull/466        Pod deletion improvements (@davidvossel)https://github.com/kubevirt/kubevirt/pull/531  In addition to this, we are also working on:      Addition of more tests to pod networking (@vladikr)(https://github.com/kubevirt/kubevirt/pull/525)        Access to the node control network (@rmohr)(https://github.com/kubevirt/kubevirt/pull/499)        Custom VM metrics discussion (@fromanirh)(https://github.com/kubevirt/kubevirt/pull/487)        Simple persistence mechanism documentation (@mpolednik)(https://github.com/kubevirt/user-guide/pull/6)  Take a look at the pulse, to get an overview over all changes of thisweek: https://github.com/kubevirt/kubevirt/pulseFinally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesAnd keep track of events at our calendar18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com“&gt;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&lt;/link&gt;If you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;",
        "url": "/2017/This-Week-in-Kube-Virt-8.html"
      }
      ,
    
      "2017-this-week-in-kube-virt-7-html": {
        "title": "This Week In Kube Virt 7",
        "author": "fabiand",
        "category": "updates",
        "content": "This is the seventh weekly update from the KubeVirt team.This week you can read more or speak to us at:      KVM Forum, Prague Thursday, October 26, 10:00 - 10:45https://kvmforum2017.sched.com/event/BnoA        “KubeWHAT?” by S Gordon - On KubeVirt and OpenStack (past event)https://www.slideshare.net/sgordon2/kubewhat  We are currently driven by      Being easier to be used on Kubernetes and OpenShift        Enabling people to contribute        Node Isolator use-case (more informations soon)  This week we achieved to:      VMs and components are now running in the host pid namespace(@dvossel) https://github.com/kubevirt/kubevirt/pull/506        Move dependency management from glide to dep ()https://github.com/kubevirt/kubevirt/pull/511        Add a leader election mechanism to virt-controller (@)https://github.com/kubevirt/kubevirt/pull/461        Add OpenAPI specification (@)https://github.com/kubevirt/kubevirt/pull/494        Put work on api server aggregation on hold for now (@stu-gott) To beresolved: API server storage(https://github.com/kubevirt/kubevirt/pull/355)  In addition to this, we are also working on:      Finalization of pod networking (@vladikr)(https://github.com/kubevirt/kubevirt/pull/525)        Access to the node control network (@rmohr)(https://github.com/kubevirt/kubevirt/pull/499)        Custom VM metrics discussion (@fromanirh)(https://github.com/kubevirt/kubevirt/pull/487)        Simple persistence mechanism (@petrkotas)(https://github.com/petrkotas/virt-vmconfig-crd/)  Take a look at the pulse, to get an overview over all changes of thisweek: https://github.com/kubevirt/kubevirt/pulseFinally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesAnd keep track of events at our calendar18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com“&gt;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&lt;/link&gt;If you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;",
        "url": "/2017/This-Week-in-Kube-Virt-7.html"
      }
      ,
    
      "2017-this-week-in-kube-virt-6-html": {
        "title": "This Week In Kube Virt 6",
        "author": "fabiand",
        "category": "updates",
        "content": "This is the sixth weekly update from the KubeVirt team.This week you could watch us at:      Kubernetes Community Meeting introducing and demoing KubeVirt:https://www.youtube.com/watch?v=oBhu1MeGbss        Or follow us at our new blog: https://kubevirt.github.io/blog/  We are currently driven by      Being easier to be used on Kubernetes and OpenShift        Enabling people to contribute        Node Isolator use-case (more informations soon)  This week we achieved to:  Add support for node affinity to VM.Spec (@MarSik)(https://github.com/kubevirt/kubevirt/pull/446)In addition to this, we are also working on:      Access to the node control network (@rmohr)(https://github.com/kubevirt/kubevirt/pull/499)        Custom VM metrics discussion (@fromanirh)(https://github.com/kubevirt/kubevirt/pull/487)        Continued work on api server aggregation (@stu-gott)(https://github.com/kubevirt/kubevirt/pull/355)        Revived VM Config discussion (@mpolednik)(https://github.com/kubevirt/kubevirt/pull/408)  Take a look at the pulse, to get an overview over all changes of thisweek: https://github.com/kubevirt/kubevirt/pulseFinally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesAnd keep track of events at our calendar18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com“&gt;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&lt;/link&gt;If you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;",
        "url": "/2017/This-Week-in-Kube-Virt-6.html"
      }
      ,
    
      "2017-this-week-in-kube-virt-5-html": {
        "title": "This Week In Kube Virt 5",
        "author": "fabiand",
        "category": "updates",
        "content": "This is the fith weekly update from the KubeVirt team.We are currently driven by      Being easier to be used on Kubernetes and OpenShift        Enabling people to contribute        Node Isolator use-case (more informations soon)  This week we achieved to:      Improved sagger documentation (for SDK generation) (@lukas-bednar)(https://github.com/kubevirt/kubevirt/pull/476)        Kubernetes 1.8 fixes (@cynepco3hahue)(https://github.com/kubevirt/kubevirt/pull/479https://github.com/kubevirt/kubevirt/pull/484)        Ephemeral disk rewrite (@davidvossel)(https://github.com/kubevirt/kubevirt/pull/460)        Custom VM metrics proposal (@fromanirh )(https://github.com/kubevirt/kubevirt/pull/487)        [WIP] Add API server PKI tool (@jhernand)(https://github.com/kubevirt/kubevirt/pull/498)        KubeVirt provider for the cluster autoscaler (@rmohr)(https://github.com/rmohr/autoscaler/pull/1)  In addition to this, we are also working on:      Finally some good progress with layer 3 network connectivity(@vladikr) (https://github.com/kubevirt/kubevirt/pull/450https://github.com/vladikr/kubevirt/tree/veth-bridge-taphttps://github.com/vladikr/kubevirt/tree/veth-macvtap)        Continued work on api server aggregation (@stu-gott)(https://github.com/kubevirt/kubevirt/pull/355)  Take a look at the pulse, to get an overview over all changes of thisweek: https://github.com/kubevirt/kubevirt/pulseFinally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesAnd keep track of events at our calendar18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com“&gt;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&lt;/link&gt;If you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;",
        "url": "/2017/This-Week-in-Kube-Virt-5.html"
      }
      ,
    
      "2017-this-week-in-kube-virt-4-html": {
        "title": "This Week In Kube Virt 4",
        "author": "fabiand",
        "category": "updates",
        "content": "This is the fourth weekly update from the KubeVirt team.We are currently driven by      Being easier to be used on Kubernetes and OpenShift        Enabling people to contribute        Node Isolator use-case (more informations soon)  This week you can find us at:  Ohio Linux Fest (@stu-gott) “KubeVirt, Virtual Machine ManagementUsing Kubernetes” https://ohiolinux.org/schedule/This week we achieved to:      ReplicaSet for VirtualMachines (@rmohr)(https://github.com/kubevirt/kubevirt/pull/453)        Swagger documentation improvements (@rmohr, @lukas-bednar)(https://github.com/kubevirt/kubevirt/pull/475)        Hot-standby for our controller (@cynepco3hahue)(https://github.com/kubevirt/kubevirt/pull/461)        domxml/VM Spec mapping rules proposal (@rmohr, @michalskrivanek)(https://github.com/kubevirt/kubevirt/pull/466)        Launch flow improvement proposal (@davidvossel)(https://github.com/kubevirt/kubevirt/pull/469)  In addition to this, we are also working on:      Debug layer 3 network connectivity issues for VMs (@vladikr)(https://github.com/kubevirt/kubevirt/pull/450)        Review of the draft code for the api server aggregation (@stu-gott)(https://github.com/kubevirt/kubevirt/pull/355)  Take a look at the pulse, to get an overview over all changes of thisweek: https://github.com/kubevirt/kubevirt/pulseFinally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesAnd keep track of events at our calendar18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com“&gt;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&lt;/link&gt;If you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;",
        "url": "/2017/This-Week-in-Kube-Virt-4.html"
      }
      ,
    
      "2017-this-week-in-kube-virt-3-html": {
        "title": "This Week In Kube Virt 3",
        "author": "fabiand",
        "category": "updates",
        "content": "This is the third weekly update from the KubeVirt team.We are currently driven by      Being easier to be used on Kubernetes and OpenShift        Enabling people to contribute        Node Isolator use-case (more informations soon)  This week we achieved to:      Renamed VM kind to VirtualMachine (@cynepco3hahue)(https://github.com/kubevirt/kubevirt/pull/452)        Proposal for VirtualMachineReplicaSet to scale VMs (@rmohr)(https://github.com/kubevirt/kubevirt/pull/453)        Ephemeral Registry Disk Rewrite (@vossel)(https://github.com/kubevirt/kubevirt/pull/460)        Fix some race in our CI (@rmohr)(https://github.com/kubevirt/kubevirt/pull/459)  In addition to this, we are also working on:      Review of the draft code to get layer 3 network connectivity for VMs(@vladikr) (https://github.com/kubevirt/kubevirt/pull/450)        Review of the draft code for the api server aggregation (@stu-gott)(https://github.com/kubevirt/kubevirt/pull/355)        Review of the proposal integrate with host networking (@rmohr)(https://github.com/kubevirt/kubevirt/pull/367)        Converging multiple ansible playbooks for deployment on OpenShift(@petrkotas, @cynepco3hahue, @lukas-bednar)(https://github.com/kubevirt-incubator/kubevirt-ansible)        Continued discussion of VM persistence and ABI stability(https://groups.google.com/d/topic/kubevirt-dev/G0FpxJYFhf4/discussion)  Take a look at the pulse, to get an overview over all changes of thisweek: https://github.com/kubevirt/kubevirt/pulseFinally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesAnd keep track of events at our calendar18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com“&gt;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&lt;/link&gt;If you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;",
        "url": "/2017/This-Week-in-Kube-Virt-3.html"
      }
      ,
    
      "2017-this-week-in-kube-virt-2-html": {
        "title": "This Week In Kube Virt 2",
        "author": "fabiand",
        "category": "updates",
        "content": "This is the second weekly update from the KubeVirt team.We are currently driven by      Being easier to be used on Kubernetes and OpenShift        Enabling people to contribute  This week we achieved to:      Keep cloud-init data in Secrets (@vossel)(https://github.com/kubevirt/kubevirt/pull/433)        First draft code to get layer 3 network connectivity for VMs(@vladikr) (https://github.com/kubevirt/kubevirt/pull/450)        First draft code for the api server aggregation (@stu-gott)(https://github.com/kubevirt/kubevirt/pull/355)        Add further migration documentation (@rmohr)(https://github.com/kubevirt/user-guide/pull/1)  In addition to this, we are also working on:      Progress on how to integrate with host networking (@rmohr)(https://github.com/kubevirt/kubevirt/pull/367)        Converging multiple ansible playbooks for deployment on OpenShift(@petrkotas, @cynepco3hahue, @lukas-bednar)(https://github.com/kubevirt-incubator/kubevirt-ansible)        Initial support for Anti- &amp; Affinity for VMs (@MarSik)(https://github.com/kubevirt/kubevirt/issues/438)        Initial support for memory and cpu mapping (@MarSik)(https://github.com/kubevirt/kubevirt/pull/388)        Discussing VM persistence and ABI stability(https://groups.google.com/d/topic/kubevirt-dev/G0FpxJYFhf4/discussion)  Take a look at the pulse, to get an overview over all changes of thisweek: https://github.com/kubevirt/kubevirt/pulseFinally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesAnd keep track of events at our calendar18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com“&gt;https://calendar.google.com/embed?src=18pc0jur01k8f2cccvn5j04j1g@group.calendar.google.com&lt;/link&gt;If you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;",
        "url": "/2017/This-Week-in-Kube-Virt-2.html"
      }
      ,
    
      "2017-this-week-in-kube-virt-1-html": {
        "title": "This Week In Kube Virt 1",
        "author": "fabiand",
        "category": "updates",
        "content": "This is the first weekly update from the KubeVirt team.We are currently driven by  Being easier to consume on Kubernetes and OpenShiftThis week we achieved to      merge a design for cloud-init support(https://github.com/kubevirt/kubevirt/pull/372)        release KubeVirt v0.0.2(https://github.com/kubevirt/kubevirt/releases/tag/v0.0.2)        Minikube based demo (https://github.com/kubevirt/demo)        OpenShift Community presentation(https://www.youtube.com/watch?v=IfuL2rYhMKY)  In addition to this, we are also working on:      Support stock Kubernetes networking(https://github.com/kubevirt/kubevirt/issues/261)        Move to a custom API Server suitable for API Server aggregation(https://github.com/kubevirt/kubevirt/issues/205)        Writing a user facing getting started guide(https://github.com/kubevirt/kubevirt/issues/410)        Ansible playbooks for deployment on OpenShift  Take a look at the pulse, to get an overview over all changes of thisweek: https://github.com/kubevirt/kubevirt/pulseFinally you can view our open issues athttps://github.com/kubevirt/kubevirt/issuesIf you need some help or want to chat you can find us on&lt;irc://irc.freenode.net/#kubevirt&gt;.",
        "url": "/2017/This-Week-in-Kube-Virt-1.html"
      }
      
    
  };
</script>
<script src="/js/lunr.min.js"></script>
<script src="/js/search.js"></script>

    </main>

    <footer class="footer" role="footer">
      <div class="container-fluid">
  <div class="row justify-content-center">
    <div class="col-sm-12 col-md-5">
      <p class="privacy-statement text-sm-left" style="text-align: center;">
        &copy; <script type="text/javascript"> document.write(new Date().getFullYear()); </script> KubeVirt | <a href="/privacy" class="privacy-statement-link">Privacy Statement</a>
      </p>
    </div>
    <div class="col-sm-12 col-md-5" style="text-align: center;">
      <p class="text-md-right">
        <a href="https://twitter.com/kubevirt" aria-label="Visit us on Twitter" class="link-social-twitter">
          <i class="fab fa-twitter fa-lg"></i>
        </a>
        <a href="https://github.com/kubevirt" aria-label="View our repo on GitHub" class="link-social-github">
          <i class="fab fa-github fa-lg"></i>
        </a>
        <a href="https://groups.google.com/forum/#!forum/kubevirt-dev" aria-label="Send us an email" class="link-social-mail">
          <i class="fas fa-envelope fa-lg"></i>
        </a>
        <a href="https://calendar.google.com/calendar/embed?src=18pc0jur01k8f2cccvn5j04j1g%40group.calendar.google.com&ctz=Etc%2FGMT"
            aria-label="See our calendar"
            class="link-social-calendar">
          <i class="fas fa-calendar fa-lg"></i>
        </a>
      </p>
    </div>
  </div>
  <div class="row">
    <div class="col offset-md-1 text-sm-left footer-licensing" style="text-align: center;">
      Code licensed under <a href="https://github.com/kubevirt/kubevirt/blob/master/LICENSE">Apache 2.0</a>,
      site under <a href="https://github.com/kubevirt/kubevirt.github.io/blob/master/LICENSE">MIT</a>.
    </div>
  </div>
</div>

    </footer>

    <script defer src="https://use.fontawesome.com/releases/v5.1.0/js/all.js" integrity="sha384-3LK/3kTpDE/Pkp8gTNp2gR/2gOiwQ6QaO7Td0zV76UFJVhqLl4Vl3KL1We6q6wR9" crossorigin="anonymous"></script>
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.0/umd/popper.min.js" integrity="sha384-cs/chFZiN24E4KMATLdqdvsezGxaGsi4hLGOzlXwp5UZB1LY//20VyM2taTB4QvJ" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.0/js/bootstrap.min.js" integrity="sha384-uefMccjFJAIv6A+rW+L4AHf99KvxDjWSu1z9VI8SKNVmz4sk7buKt/6v9KI65qnm" crossorigin="anonymous"></script>
    <script src="/js/kubevirt-io.js"></script>
    <script src="/js/lunr.min.js"></script>
    <script src="/js/search.js"></script>
  </body>
</html>
