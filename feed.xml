<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kubevirt.io</title>
    <description>Virtual Machine Management on Kubernetes
</description>
    <link>https://www.kubevirt.io//</link>
    <atom:link href="https://www.kubevirt.io//feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Fri, 13 Jul 2018 17:22:36 -0400</pubDate>
    <pubYear>Fri, 13 Jul 2018 17:22:36 -0400</pubYear>
    <lastBuildDate>Fri, 13 Jul 2018 17:22:36 -0400</lastBuildDate>
    <generator>Jekyll v3.8.3</generator>
    
      <item>
        <title>Unit Test Howto</title>
        <description>&lt;p&gt;There are &lt;a href=&quot;https://blog.codinghorror.com/i-pity-the-fool-who-doesnt-write-unit-tests/&quot;&gt;way too many reasons&lt;/a&gt; to write unit tests, but my favorite one is: the freedom to hack, modify and improve the code without fear, and get quick feedback that you are on the right track.&lt;/p&gt;

&lt;p&gt;Of course, writing good integration tests (the stuff under the &lt;a href=&quot;https://github.com/kubevirt/kubevirt/tree/master/tests&quot;&gt;tests&lt;/a&gt; directory) is the best way to validate that everything works, but unit tests has great value as:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;They are much faster to run (~30 seconds in our case)&lt;/li&gt;
  &lt;li&gt;You get nice coverage reports with &lt;a href=&quot;https://coveralls.io/github/kubevirt/kubevirt&quot;&gt;coveralls&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;No need to: &lt;code class=&quot;highlighter-rouge&quot;&gt;make cluster up/sync&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Cover corner cases and easier to debug&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Some Notes:&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;We use same frameworks (ginkgo, gomega) for unit testing and integration testing, which means that with the same learning curve, you can develop much more!&lt;/li&gt;
    &lt;li&gt;“Bang for the Buck” - it usually takes 20% of the time to get to 80% coverage, and 80% of the time to get to 100%. Which mean that you have to use common sense when improving coverage - some code is just fine with 80% coverage (e.g. large files calling some other APIs with little logic), and other would benefit from getting close to 100% (e.g. complex core functionality handling lots of error cases)&lt;/li&gt;
    &lt;li&gt;Follow the &lt;a href=&quot;http://programmer.97things.oreilly.com/wiki/index.php/The_Boy_Scout_Rule&quot;&gt;“boy (or girl) scout rule”&lt;/a&gt; - every time you enhance/fix some code, add more testing around the existing code as well&lt;/li&gt;
    &lt;li&gt;Avoid “white box testing”, as this will cause endless maintenance of the test code. Best way to assure that, is to put the test code under a different package than the code under test&lt;/li&gt;
    &lt;li&gt;Explore coveralls. Not only it will show you the coverage and the overall trend, it will also help you understand which tests are missing. When drilling down into a file, you can see hits per line, and make better decision on what needs to be covered next&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;frameworks&quot;&gt;Frameworks&lt;/h1&gt;
&lt;p&gt;There are several frameworks we use to write unit tests:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The tests themselves are written using &lt;a href=&quot;https://github.com/onsi/ginkgo&quot;&gt;ginkgo&lt;/a&gt;, which is a &lt;a href=&quot;https://en.wikipedia.org/wiki/Behavior-driven_development&quot;&gt;Behavior-Driven Development (BDD)&lt;/a&gt; framework&lt;/li&gt;
  &lt;li&gt;The library used for assertions in the tests is &lt;a href=&quot;https://github.com/onsi/gomega&quot;&gt;gomega&lt;/a&gt;. It has a very rich set of matchers, so, before you write you own code around the “equal” matcher, check &lt;a href=&quot;http://onsi.github.io/gomega/#provided-matchers&quot;&gt;here&lt;/a&gt; to see if there is a more expressive assertion you can use&lt;/li&gt;
  &lt;li&gt;We use &lt;a href=&quot;https://github.com/golang/mock&quot;&gt;GoMock&lt;/a&gt; to generate mocks for the different kubevirt interfaces and objects. The command &lt;code class=&quot;highlighter-rouge&quot;&gt;make generate&lt;/code&gt; will (among other things) create a &lt;a href=&quot;https://github.com/kubevirt/kubevirt/blob/master/pkg/kubecli/generated_mock_kubevirt.go&quot;&gt;file&lt;/a&gt; holding the mocked version of our objects and interfaces
    &lt;ul&gt;
      &lt;li&gt;Many examples exist in our code on how to use this framework&lt;/li&gt;
      &lt;li&gt;Also see &lt;a href=&quot;https://github.com/golang/mock/tree/master/sample&quot;&gt;here&lt;/a&gt; for sample code from GoMock&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;If you need mocks for k8s objects and interfaces, use their framework. They have a tool called &lt;a href=&quot;https://github.com/kubernetes/code-generator&quot;&gt;client-gen&lt;/a&gt;, which generates both the code and the mocks based on the defined APIs
    &lt;ul&gt;
      &lt;li&gt;The generated mock interfaces and objects of the k8s client are &lt;a href=&quot;https://github.com/kubernetes/client-go/blob/master/kubernetes/fake/clientset_generated.go&quot;&gt;here&lt;/a&gt;. Note that they a use a different mechanism to control the mocked behavior than the one used in GoMock&lt;/li&gt;
      &lt;li&gt;Mocked actions are more are &lt;a href=&quot;https://github.com/kubernetes/client-go/tree/master/testing&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Unit test utilities are placed under &lt;a href=&quot;https://github.com/kubevirt/kubevirt/tree/master/pkg/testutils&quot;&gt;testutils&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Some integration test utilities are also useful for unit testing, see this &lt;a href=&quot;https://github.com/kubevirt/kubevirt/blob/master/tests/utils.go&quot;&gt;file&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;When testing interfaces, a mock HTTP server is usually needed. For that we use the &lt;a href=&quot;https://golang.org/pkg/net/http/httptest/&quot;&gt;golang httptest package&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;gomega also has a package called &lt;a href=&quot;http://onsi.github.io/gomega/#ghttp-testing-http-clients&quot;&gt;ghttp&lt;/a&gt; that could be used for same purpose&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;best-practices-and-tips&quot;&gt;Best Practices and Tips&lt;/h1&gt;
&lt;h2 id=&quot;ginkgo&quot;&gt;ginkgo&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Don’t mix setup and tests, use BeforeEach/JustBeforeEach for setup and It/Specify for tests&lt;/li&gt;
  &lt;li&gt;Don’t write setup/cleanup code under Describe/Context clause, which is not inside BeforeEach/AfterEach etc.&lt;/li&gt;
  &lt;li&gt;Make sure that any state change inside an “It” clause, that may impact other tests, is reverted in “AfterEach”&lt;/li&gt;
  &lt;li&gt;Don’t assume the “It” clauses, which are at the same level, are invoked in any specific order
    &lt;h2 id=&quot;gomega&quot;&gt;gomega&lt;/h2&gt;
    &lt;p&gt;Be verbose and use specific matchers. For example, to check that an array has N elements, you can use:&lt;/p&gt;
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Expect(len(arr)).To(Equal(N))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;But a better way would be:&lt;/p&gt;
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Expect(arr).To(HaveLen(N))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;h2 id=&quot;function-override&quot;&gt;Function Override&lt;/h2&gt;
    &lt;p&gt;Sometimes the code under test is invoking a function which is not mocked. In most cases, this is an indication that the code needs to be refactored, so this function, or its return values, will be passed as part of the API of the code being tested.
However, if this refactoring is not possible (or too costly), you can inject your own implementation of this function. The original function should be defined as a closure, and assigned to a global variable. Since functions are 1st class citizens in go, you can assign your implementation to that function variable. More detailed example is &lt;a href=&quot;https://gist.github.com/yuvalif/006c48c563f264041f4ada5f90ddfd0c&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 03 Jul 2018 00:00:00 -0400</pubDate>
        <pubYear>Wed, 31 Dec 1969 19:33:38 -0500</pubYear>
        <link>https://www.kubevirt.io//2018/Unit-Test-Howto.html</link>
        <guid isPermaLink="true">https://www.kubevirt.io//2018/Unit-Test-Howto.html</guid>
        
        
        <category>news</category>
        
      </item>
    
      <item>
        <title>Run Istio With Kubevirt</title>
        <description>&lt;p&gt;On this blog post, we are going to deploy virtual machines with the KubeVirt project and insert them into the Istio service mesh.&lt;/p&gt;

&lt;p&gt;Some information about the technologies we are going to use in this blog post.&lt;/p&gt;

&lt;h2 id=&quot;kubernetes&quot;&gt;Kubernetes&lt;/h2&gt;

&lt;p&gt;Production-Grade Container Orchestration.&lt;/p&gt;

&lt;p&gt;Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.&lt;/p&gt;

&lt;h3 id=&quot;kubeadm&quot;&gt;Kubeadm&lt;/h3&gt;

&lt;p&gt;kubeadm helps you bootstrap a minimum viable Kubernetes cluster that conforms to best practices.&lt;/p&gt;

&lt;h2 id=&quot;calico&quot;&gt;Calico&lt;/h2&gt;

&lt;p&gt;Calico provides secure network connectivity for containers and virtual machine workloads.&lt;/p&gt;

&lt;p&gt;Calico creates and manages a flat layer 3 network, assigning each workload a fully routable IP address. Workloads can communicate without IP encapsulation or network address translation for bare metal performance, easier troubleshooting, and better interoperability. In environments that require an overlay, Calico uses IP-in-IP tunneling or can work with other overlay networking such as flannel.&lt;/p&gt;

&lt;h2 id=&quot;kubevirt&quot;&gt;KubeVirt&lt;/h2&gt;

&lt;p&gt;Virtualization API for kubernetes in order to manage virtual machines&lt;/p&gt;

&lt;p&gt;KubeVirt technology addresses the needs of development teams that have adopted or want to adopt Kubernetes but possess existing Virtual Machine-based workloads that cannot be easily containerized. More specifically, the technology provides a unified development platform where developers can build, modify, and deploy applications residing in both Application Containers as well as Virtual Machines in a common, shared environment.&lt;/p&gt;

&lt;p&gt;Benefits are broad and significant. Teams with a reliance on existing virtual machine-based workloads are empowered to rapidly containerize applications. With virtualized workloads placed directly in development workflows, teams can decompose them over time while still leveraging remaining virtualized components as is comfortably desired.&lt;/p&gt;

&lt;h2 id=&quot;istio&quot;&gt;Istio&lt;/h2&gt;

&lt;p&gt;An open platform to connect, manage, and secure microservices.&lt;/p&gt;

&lt;p&gt;Istio provides an easy way to create a network of deployed services with load balancing, service-to-service authentication, monitoring, and more, without requiring any changes in service code. You add Istio support to services by deploying a special sidecar proxy throughout your environment that intercepts all network communication between microservices, configured and managed using Istio’s control plane functionality.&lt;/p&gt;

&lt;h2 id=&quot;bookinfo-application&quot;&gt;Bookinfo application&lt;/h2&gt;

&lt;p&gt;A simple application that displays information about a book, similar to a single catalog entry of an online book store. Displayed on the page is a description of the book, book details (ISBN, number of pages, and so on), and a few book reviews.&lt;/p&gt;

&lt;p&gt;The Bookinfo application is broken into four separate microservices:&lt;/p&gt;

&lt;p&gt;productpage. The productpage microservice calls the details and reviews microservices to populate the page.&lt;/p&gt;

&lt;p&gt;details. The details microservice contains book information.&lt;/p&gt;

&lt;p&gt;reviews. The reviews microservice contains book reviews. It also calls the ratings microservice.&lt;/p&gt;

&lt;p&gt;ratings. The ratings microservice contains book ranking information that accompanies a book review.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/2019-06-21-Run-Istio-with-kubevirt/Bookinfo.png&quot; alt=&quot;Bookinfo-application&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This demo is going to be deployed on a kubernetes 1.10 cluster.&lt;/p&gt;

&lt;h1 id=&quot;requirements&quot;&gt;Requirements&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;docker&lt;/li&gt;
  &lt;li&gt;kubeadm&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Follow this &lt;a href=&quot;https://kubernetes.io/docs/tasks/tools/install-kubeadm/&quot;&gt;document&lt;/a&gt; to install everything we need for the POC&lt;/p&gt;

&lt;h1 id=&quot;deployment&quot;&gt;Deployment&lt;/h1&gt;

&lt;p&gt;For the POC we clone &lt;a href=&quot;https://github.com/SchSeba/kubevirt-istio-poc&quot;&gt;this repo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The repo contains all the configuration we need to deploy KubeVirt and Istio.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;kubevirt.yaml&lt;/li&gt;
  &lt;li&gt;istio-demo-auth.yaml&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It also contains the deployment configuration of our sample application.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;bookinfo.yaml&lt;/li&gt;
  &lt;li&gt;bookinfo-gateway.yaml&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Run the bash script&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd kubevirt-istio-poc
./deploy-istio-poc.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;demo-application&quot;&gt;Demo application&lt;/h1&gt;

&lt;p&gt;We are going to use the &lt;a href=&quot;https://istio.io/docs/guides/bookinfo/#overview&quot;&gt;bookinfo sample application&lt;/a&gt; from the istio webpage.&lt;/p&gt;

&lt;p&gt;The follow yaml will deploy the bookinfo application with a ‘small’ change the details service will run on a virtual machine inside our kubernetes cluster!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; it will take like 5 minutes for the application to by running inside the virtual machine because we install git and ruby, then clone the istio repo and start the application.&lt;/p&gt;

&lt;h1 id=&quot;poc-details&quot;&gt;POC details&lt;/h1&gt;

&lt;p&gt;Lets start with the bash script:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-x&lt;/span&gt;

kubeadm init &lt;span class=&quot;nt&quot;&gt;--pod-network-cidr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;192.168.0.0/16

&lt;span class=&quot;nb&quot;&gt;yes&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;cp&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; /etc/kubernetes/admin.conf &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/.kube/config

kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://docs.projectcalico.org/v3.0/getting-started/kubernetes/installation/hosted/kubeadm/1.7/calico.yaml

&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[[&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;kubectl get po &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;kube-dns | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;Running | &lt;span class=&quot;nb&quot;&gt;wc&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-l&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-eq&lt;/span&gt; 0 &lt;span class=&quot;o&quot;&gt;]]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;do
        &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo &lt;/span&gt;Calico deployment is no ready yet.
        &lt;span class=&quot;nb&quot;&gt;sleep &lt;/span&gt;5
&lt;span class=&quot;k&quot;&gt;done

&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo &lt;/span&gt;Calico is ready.

&lt;span class=&quot;nb&quot;&gt;echo &lt;/span&gt;Taint the master node.

kubectl taint nodes &lt;span class=&quot;nt&quot;&gt;--all&lt;/span&gt; node-role.kubernetes.io/master-

&lt;span class=&quot;nb&quot;&gt;echo &lt;/span&gt;Deploy kubevirt.

kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://github.com/kubevirt/kubevirt/releases/download/v0.7.0/kubevirt.yaml

&lt;span class=&quot;nb&quot;&gt;echo &lt;/span&gt;Deploy istio.

kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; istio-demo-auth.yaml

&lt;span class=&quot;nb&quot;&gt;echo &lt;/span&gt;Add istio-injection to the default namespace.

kubectl label namespace default istio-injection&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;enabled

&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[[&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;kubectl get po &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; istio-system | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;sidecar-injector | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;Running | &lt;span class=&quot;nb&quot;&gt;wc&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-l&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-eq&lt;/span&gt; 0 &lt;span class=&quot;o&quot;&gt;]]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;do
        &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo &lt;/span&gt;Istio deployment is no ready yet.
        &lt;span class=&quot;nb&quot;&gt;sleep &lt;/span&gt;5
&lt;span class=&quot;k&quot;&gt;done

&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo &lt;/span&gt;Istio is ready.

&lt;span class=&quot;nb&quot;&gt;sleep &lt;/span&gt;20

&lt;span class=&quot;nb&quot;&gt;echo &lt;/span&gt;Deploy the bookinfo example application

kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; bookinfo.yaml

kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; bookinfo-gateway.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The follow script create a kubernetes cluster using the kubeadm command, deploy calico as a network CNI and taint the master node (have only one node in the cluster).&lt;/p&gt;

&lt;p&gt;After the cluster is up the script deploy both istio with mutual TLS and kubevirt projects, it also add the auto injection to the default namespace.&lt;/p&gt;

&lt;p&gt;At last the script deploy the bookinfo demo application that we change a bit.&lt;/p&gt;

&lt;p&gt;Lets take a closer look in the virtual machine part inside the bookinfo.yaml file&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;##################################################################################################
# Details service
##################################################################################################
apiVersion: v1
kind: Service
metadata:
  name: details
  labels:
    app: details
spec:
  ports:
  - port: 9080
    name: http
  selector:
    app: details
---
apiVersion: kubevirt.io/v1alpha2
kind: VirtualMachineInstance
metadata:
  creationTimestamp: null
  labels:
    special: vmi-details
    app: details
    version: v1
  name: vmi-details
spec:
  domain:
    devices:
      disks:
      - disk:
          bus: virtio
        name: registrydisk
        volumeName: registryvolume
      - disk:
          bus: virtio
        name: cloudinitdisk
        volumeName: cloudinitvolume
      interfaces:
      - name: testSlirp
        slirp: {}
        ports:
        - name: http
          port: 9080
    machine:
      type: &quot;&quot;
    resources:
      requests:
        memory: 1024M
  networks:
  - name: testSlirp
    pod: {}
  terminationGracePeriodSeconds: 0
  volumes:
  - name: registryvolume
    registryDisk:
      image: kubevirt/fedora-cloud-registry-disk-demo:latest
  - cloudInitNoCloud:
      userData: |-
        #!/bin/bash
        echo &quot;fedora&quot; |passwd fedora --stdin
        yum install git ruby -y
        git clone https://github.com/istio/istio.git
        cd istio/samples/bookinfo/src/details/
        ruby details.rb 9080 &amp;amp;
    name: cloudinitvolume
status: {}
---
..........
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;details&quot;&gt;Details:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Create a network of type pod&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;networks:
- name: testSlirp
  pod: {}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Create an interface of type slirp and connect it to the pod network by matching the pod network name&lt;/li&gt;
  &lt;li&gt;Add our application port&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;interfaces:
- name: testSlirp
  slirp: {}
  ports:
  - name: http
    port: 9080
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Use the cloud init script to download install and run the details application&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- cloudInitNoCloud:
    userData: |-
        #!/bin/bash
        echo &quot;fedora&quot; |passwd fedora --stdin
        yum install git ruby -y
        git clone https://github.com/istio/istio.git
        cd istio/samples/bookinfo/src/details/
        ruby details.rb 9080 &amp;amp;
    name: cloudinitvolume
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;poc-check&quot;&gt;POC Check&lt;/h1&gt;

&lt;p&gt;After running the bash script the environment should look like this&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;NAME                              READY     STATUS    RESTARTS   AGE
productpage-v1-7bbdd59459-w6nwq   2/2       Running   0          1h
ratings-v1-76dc7f6b9-6n6s9        2/2       Running   0          1h
reviews-v1-64545d97b4-tvgl2       2/2       Running   0          1h
reviews-v2-8cb9489c6-wjp9x        2/2       Running   0          1h
reviews-v3-6bc884b456-hr5bm       2/2       Running   0          1h
virt-launcher-vmi-details-94pb6   3/3       Running   0          1h
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Lets find the istio ingress service port&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# kubectl get service -n istio-system  | grep istio-ingressgateway
istio-ingressgateway       LoadBalancer   10.97.163.91     &amp;lt;pending&amp;gt;     80:31380/TCP,443:31390/TCP,31400:31400/TCP                            3h
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then browse the follow url&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;http://&amp;lt;k8s-node-ip-address&amp;gt;:&amp;lt;istio-ingress-service-port-exposed-by-k8s&amp;gt;/productpage
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Example:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;http://10.0.0.1:31380/productpage
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;This POC show how we can use KubeVirt with Istio to integrate the Istio service mesh to virtual machine workloads running inside our kubernetes cluster.&lt;/p&gt;
</description>
        <pubDate>Thu, 21 Jun 2018 00:00:00 -0400</pubDate>
        <pubYear>Wed, 31 Dec 1969 19:33:38 -0500</pubYear>
        <link>https://www.kubevirt.io//2018/Run-Istio-with-kubevirt.html</link>
        <guid isPermaLink="true">https://www.kubevirt.io//2018/Run-Istio-with-kubevirt.html</guid>
        
        
        <category>news</category>
        
      </item>
    
      <item>
        <title>Kvm Using Device Plugins</title>
        <description>&lt;p&gt;As of Kubernetes 1.10, the Device Plugins API is now in beta! KubeVirt is now
using this framework to provide hardware acceleration and network devices to
virtual machines. The motivation behind this is that virt-launcher pods are no
longer responsible for creating their own device nodes. Or stated another way:
virt-launcher pods no longer require excess privileges just for the purpose of
creating device nodes.&lt;/p&gt;

&lt;h2 id=&quot;kubernetes-device-plugin-basics&quot;&gt;Kubernetes Device Plugin Basics&lt;/h2&gt;

&lt;p&gt;Device Plugins consist of two main parts: a server that provides devices and
pods that consume them. Each plugin server is used to share a preconfigured
list of devices local to the node with pods scheduled on that node. Kubernetes
marks each node with the devices it’s capable of sharing, and uses the presence
of such devices when scheduling pods.&lt;/p&gt;

&lt;h2 id=&quot;device-plugins-in-kubevirt&quot;&gt;Device Plugins In KubeVirt&lt;/h2&gt;

&lt;h3 id=&quot;providing-devices&quot;&gt;Providing Devices&lt;/h3&gt;

&lt;p&gt;In KubeVirt virt-handler takes on the role of the device plugin server. When it
starts up on each node, it registers with the Kubernetes Device Plugin API and
advertises KVM and TUN devices.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: v1
kind: Node
metadata:
  ...
spec:
  ...
status:
  allocatable:
    cpu: &quot;2&quot;
    devices.kubevirt.io/kvm: &quot;110&quot;
    devices.kubevirt.io/tun: &quot;110&quot;
    pods: &quot;110&quot;
    ...
  capacity:
    cpu: &quot;2&quot;
    devices.kubevirt.io/kvm: &quot;110&quot;
    devices.kubevirt.io/tun: &quot;110&quot;
    pods: &quot;110&quot;
    ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this case advertising 110 KVM or TUN devices is simply an arbitrary default
based on the number of pods that node is limited to.&lt;/p&gt;

&lt;h3 id=&quot;consuming-devices&quot;&gt;Consuming Devices&lt;/h3&gt;

&lt;p&gt;Now any pod that requests a &lt;code class=&quot;highlighter-rouge&quot;&gt;devices.kubevirt.io/kvm&lt;/code&gt; or
&lt;code class=&quot;highlighter-rouge&quot;&gt;devices.kubevirt.io/tun&lt;/code&gt; device can only be scheduled on nodes which provide
them. On clusters where KubeVirt is deployed this conveniently happens to be
all nodes in the cluster that have these physical devices, which normally means
all nodes in the cluster.&lt;/p&gt;

&lt;p&gt;Here’s an excerpt of what the pod spec looks like in this case.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  ...
spec:
  containers:
  - command:
    - /entrypoint.sh
      ...
    name: compute
      ...
    resources:
      limits:
        devices.kubevirt.io/kvm: &quot;1&quot;
        devices.kubevirt.io/tun: &quot;1&quot;
      requests:
        devices.kubevirt.io/kvm: &quot;1&quot;
        devices.kubevirt.io/tun: &quot;1&quot;
        memory: &quot;161679432&quot;
    securityContext:
      capabilities:
        add:
        - NET_ADMIN
      privileged: false
      runAsUser: 0
    ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Of special note is the securityContext stanza. The only special privilege
required is the &lt;code class=&quot;highlighter-rouge&quot;&gt;NET_ADMIN&lt;/code&gt; capability! This is needed by libvirt to set up the
domain’s networking stack.&lt;/p&gt;
</description>
        <pubDate>Wed, 20 Jun 2018 00:00:00 -0400</pubDate>
        <pubYear>Wed, 31 Dec 1969 19:33:38 -0500</pubYear>
        <link>https://www.kubevirt.io//2018/KVM-Using-Device-Plugins.html</link>
        <guid isPermaLink="true">https://www.kubevirt.io//2018/KVM-Using-Device-Plugins.html</guid>
        
        
        <category>news</category>
        
      </item>
    
      <item>
        <title>Proxy VM Conclusion</title>
        <description>&lt;p&gt;This blog post follow my previous reseach on how to allow vms inside a k8s cluster tp play nice with istio and other sidecars.&lt;/p&gt;

&lt;h1 id=&quot;research-conclusions-and-network-roadmap&quot;&gt;Research conclusions and network roadmap&lt;/h1&gt;

&lt;p&gt;After the deep research about different options/ways to connect VM to pods, we find that all the solution have different pros and cons.
All the represented solution need access to kernel modules and have the risk of conflicting with other networking tools.&lt;/p&gt;

&lt;p&gt;We decided to implement a 100% Kubernetes compatible network approach on the kubevirt project by using the slirp interface qemu provides.
This approach let the VM (from a networking perspective) behave like a process. Thus all traffic is going in and out of TCP or UDP sockets. The approach especially needs to avoid to rely on any specific Kernel configurations (like iptables, ebtables, tc, …) in order to not conflict with other Kubernetes networking tools like Istio or multus.&lt;/p&gt;

&lt;p&gt;This is just an intermediate solution, because it’s shortcomings (unmaintained, unsafe, not performing well)&lt;/p&gt;

&lt;h3 id=&quot;slirp-interface&quot;&gt;Slirp interface&lt;/h3&gt;

&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;vm ack like a process&lt;/li&gt;
  &lt;li&gt;No external modules needed&lt;/li&gt;
  &lt;li&gt;No external process needed&lt;/li&gt;
  &lt;li&gt;Works with any sidecar solution&lt;/li&gt;
  &lt;li&gt;no rely on any specific Kernel configurations&lt;/li&gt;
  &lt;li&gt;pod can run without privilege&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;poor performance&lt;/li&gt;
  &lt;li&gt;use userspace network stack&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;iptables-only&quot;&gt;Iptables only&lt;/h3&gt;

&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;No external modules needed&lt;/li&gt;
  &lt;li&gt;No external process needed&lt;/li&gt;
  &lt;li&gt;All the traffic is handled by the kernel user space not involved&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:red;&quot;&gt;Istio dedicated solution!&lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;Not other process can change the iptables rules&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;iptables-with-a-nat-proxy&quot;&gt;Iptables with a nat-proxy&lt;/h3&gt;

&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;No external modules needed&lt;/li&gt;
  &lt;li&gt;Works with any sidecar solution&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Not other process can change the iptables rules&lt;/li&gt;
  &lt;li&gt;External process needed&lt;/li&gt;
  &lt;li&gt;The traffic is passed to user space&lt;/li&gt;
  &lt;li&gt;Only support ingress TCP connection&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;iptables-with-a-trasperent-proxy&quot;&gt;Iptables with a trasperent-proxy&lt;/h3&gt;

&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;other process can change the nat table (this solution works on the mangle table)&lt;/li&gt;
  &lt;li&gt;better preformance comparing to nat-proxy&lt;/li&gt;
  &lt;li&gt;Works with any sidecar solution&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Need NET_ADMIN capability for the docker&lt;/li&gt;
  &lt;li&gt;External process needed&lt;/li&gt;
  &lt;li&gt;The traffic is passed to user space&lt;/li&gt;
  &lt;li&gt;Only support ingress TCP connection&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 13 Jun 2018 00:00:00 -0400</pubDate>
        <pubYear>Wed, 31 Dec 1969 19:33:38 -0500</pubYear>
        <link>https://www.kubevirt.io//2018/Proxy-vm-conclusion.html</link>
        <guid isPermaLink="true">https://www.kubevirt.io//2018/Proxy-vm-conclusion.html</guid>
        
        
        <category>uncategorized</category>
        
      </item>
    
      <item>
        <title>Non Dockerized Build</title>
        <description>&lt;p&gt;In this post we will set up an alternative to the existing containerized build system used in KubeVirt.&lt;/p&gt;

&lt;p&gt;A &lt;a href=&quot;../assets/2018-06-07-Non-Dockerized-Build/Makefile.nocontainer&quot;&gt;new makefile&lt;/a&gt; will be presented here, which you can for experimenting (if you are brave enough…)&lt;/p&gt;

&lt;h1 id=&quot;why&quot;&gt;Why?&lt;/h1&gt;

&lt;p&gt;Current build system for KubeVirt is done inside docker. This ensures a robust and consistent build environment:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;No need to install system dependencies&lt;/li&gt;
  &lt;li&gt;Controlled versions of these dependencies&lt;/li&gt;
  &lt;li&gt;Agnostic of local golang environment&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, in general, &lt;strong&gt;you should just use the dockerized build system&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Still, there are some drawbacks there:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Tool integration:
    &lt;ul&gt;
      &lt;li&gt;Since your tools are not running in the dockerized environment, they may give different outcome than the ones running in the dockerized environment&lt;/li&gt;
      &lt;li&gt;Invoking any of the dockerized scripts (under &lt;code class=&quot;highlighter-rouge&quot;&gt;hack&lt;/code&gt; directory) may be inconsistent with the outside environment (e.g. file path is different than the one on your machine)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Build time: the dockerized build has some small overheads, and some improvements are still needed to make sure that caching work properly and build is optimized&lt;/li&gt;
  &lt;li&gt;And last, but not least, &lt;em&gt;sometimes it is just hard to resist the tinkering…&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how&quot;&gt;How?&lt;/h2&gt;

&lt;p&gt;Currently, the Makefile includes targets that address different things: building, dependencies, cluster management, testing etc. - here I tried to modify the minimum which is required for non-containerized build. Anything not related to it, should just be done using the existing Makefile.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note that cross compilation is not covered here (e.g. building virtctl for mac and windows)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h3&gt;

&lt;p&gt;Best place to look for that is in the docker file definition for the build environment: &lt;a href=&quot;https://github.com/kubevirt/kubevirt/blob/master/hack/docker-builder/Dockerfile&quot;&gt;hack/docker-builder/Dockerfile&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Note that not everything from there is needed for building, so the bare minimum on Fedora27 would be:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo dnf install -y git
sudo dnf install -y libvirt-devel
sudo dnf install -y golang
sudo dnf install -y docker
sudo dnf install -y qemu-img
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Similarly to the containerized case&lt;/em&gt;, docker is still needed (e.g. all the cluster stuff is done via docker),  and therefore, any docker related preparations are needed as well. This would include running docker on startup and making sure that docker commands does not need root privileges. On Fedora27 this would mean:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo groupadd docker
sudo usermod -aG docker $USER
sudo systemctl enable docker
sudo systemctl start docker
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Now, getting the actual code could be done either via &lt;code class=&quot;highlighter-rouge&quot;&gt;go get&lt;/code&gt; (don’t forget to set the &lt;code class=&quot;highlighter-rouge&quot;&gt;GOPATH&lt;/code&gt; environment variable):&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;go get -d kubevirt.io/kubevirt/...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Or &lt;code class=&quot;highlighter-rouge&quot;&gt;git clone&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir -p $GOPATH/src/kubevirt.io/ &amp;amp;&amp;amp; cd $GOPATH/src/kubevirt.io/
git clone https://github.com/kubevirt/kubevirt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;makefilenocontainer&quot;&gt;&lt;a href=&quot;../assets/2018-06-07-Non-Dockerized-Build/Makefile.nocontainer&quot;&gt;Makefile.nocontainer&lt;/a&gt;&lt;/h3&gt;
&lt;div class=&quot;language-makefile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nl&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;build&lt;/span&gt;

&lt;span class=&quot;nl&quot;&gt;bootstrap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;go&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;get&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;-u&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;github.com/onsi/ginkgo/ginkgo&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;go&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;get&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;-u&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;mvdan.cc/sh/cmd/shfmt&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;go&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;get&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;-u&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;k8s.io/code-generator/cmd/deepcopy-gen&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;go&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;get&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;-u&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;k8s.io/code-generator/cmd/defaulter-gen&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;go&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;get&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;-u&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;k8s.io/code-generator/cmd/openapi-gen&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;cd&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;${GOPATH}/src/k8s.io/code-generator/cmd/deepcopy-gen&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;git&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;checkout&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;release-1.9&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;go&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;install&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;cd&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;${GOPATH}/src/k8s.io/code-generator/cmd/defaulter-gen&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;git&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;checkout&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;release-1.9&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;go&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;install&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;cd&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;${GOPATH}/src/k8s.io/code-generator/cmd/openapi-gen&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;git&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;checkout&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;release-1.9&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;go&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;install&lt;/span&gt;

&lt;span class=&quot;nl&quot;&gt;generate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;./hack/generate.sh&lt;/span&gt;

&lt;span class=&quot;nl&quot;&gt;apidocs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generate&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;./hack/gen-swagger-doc/gen-swagger-docs.sh&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;v1&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;html&lt;/span&gt;

&lt;span class=&quot;nl&quot;&gt;build&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;check&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;go&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;-v&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;./cmd/...&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;./pkg/...&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;./hack/copy-cmd.sh&lt;/span&gt;

&lt;span class=&quot;nl&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;build&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;go&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;-v&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;-cover&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;./pkg/...&lt;/span&gt;

&lt;span class=&quot;nl&quot;&gt;check&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;./hack/check.sh&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;OUT_DIR&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;./_out
&lt;span class=&quot;nv&quot;&gt;TESTS_OUT_DIR&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;OUT_DIR&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/tests

&lt;span class=&quot;nl&quot;&gt;functest&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;build&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;go&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;build&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;-v&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;./tests/...&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;ginkgo&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;build&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;./tests&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;mkdir&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;${TESTS_OUT_DIR}/&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;mv&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;./tests/tests.test&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;${TESTS_OUT_DIR}/&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;./hack/functests.sh&lt;/span&gt;

&lt;span class=&quot;nl&quot;&gt;cluster-sync&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;build&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;./hack/build-copy-artifacts.sh&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;./hack/build-manifests.sh&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;./hack/build-docker.sh&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;build&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;./cluster/clean.sh&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;./cluster/deploy.sh&lt;/span&gt;

&lt;span class=&quot;nl&quot;&gt;.PHONY&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bootstrap generate apidocs build test check functest cluster-sync&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;targets&quot;&gt;Targets&lt;/h3&gt;

&lt;p&gt;To execute any of the targets use:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;make -f Makefile.nocontainer &amp;lt;target&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;File has the following targets:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;bootstrap&lt;/strong&gt;: this is actually part of the prerequisites, but added all golang tool dependencies here, since this is agnostic of the running platform Should be called once
    &lt;ul&gt;
      &lt;li&gt;Note that the k8s code generators use specific version&lt;/li&gt;
      &lt;li&gt;Note that these are not code dependencies, as they are handled by using a &lt;code class=&quot;highlighter-rouge&quot;&gt;vendor&lt;/code&gt; directory, as well as the distclean,  deps-install and deps-update targets in the &lt;a href=&quot;ttps://github.com/kubevirt/kubevirt/blob/master/Makefile&quot;&gt;standard makefile&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;generate&lt;/strong&gt;: Calling &lt;a href=&quot;https://github.com/kubevirt/kubevirt/blob/master/hack/generate.sh&quot;&gt;hack/generate.sh&lt;/a&gt; script similarly to the &lt;a href=&quot;https://github.com/kubevirt/kubevirt/blob/master/Makefile&quot;&gt;standard makefile&lt;/a&gt;. It builds all generators (under the &lt;code class=&quot;highlighter-rouge&quot;&gt;tools&lt;/code&gt; directory) and use them to generate: test mocks, KubeVirt resources and test yamls&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;apidocs&lt;/strong&gt;: this is similar to apidocs target in the &lt;a href=&quot;ttps://github.com/kubevirt/kubevirt/blob/master/Makefile&quot;&gt;standard makefile&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;build&lt;/strong&gt;: this is building all product binaries, and then using a script (&lt;a href=&quot;../assets/2018-06-07-Non-Dockerized-Build/copy-cmd.sh&quot;&gt;copy-cmd.sh&lt;/a&gt;, should be placed under: &lt;code class=&quot;highlighter-rouge&quot;&gt;hack&lt;/code&gt;) to copy the binaries from their standard location into the &lt;code class=&quot;highlighter-rouge&quot;&gt;_out&lt;/code&gt; directory, where the cluster management scripts expect them&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;test&lt;/strong&gt;: building and running unit tests
check: using similar code to the one used in the standard makefile: formatting files, fixing package imports and calling go vet&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;functest&lt;/strong&gt;: building and running integration tests. After tests are built , they are moved to the &lt;code class=&quot;highlighter-rouge&quot;&gt;_out&lt;/code&gt; directory so that the standard script for running integration tests would find them&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;cluster-sync&lt;/strong&gt;: this is the only “cluster management” target that had to be modified from the standard makefile&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 07 Jun 2018 00:00:00 -0400</pubDate>
        <pubYear>Wed, 31 Dec 1969 19:33:38 -0500</pubYear>
        <link>https://www.kubevirt.io//2018/Non-Dockerized-Build.html</link>
        <guid isPermaLink="true">https://www.kubevirt.io//2018/Non-Dockerized-Build.html</guid>
        
        
        <category>uncategorized</category>
        
      </item>
    
      <item>
        <title>Research Run Vms With Istio Service Mesh</title>
        <description>&lt;p&gt;In this blog post we are going to talk about istio and virtual machines on top of Kubernetes. Some of the components we are going to use are &lt;a href=&quot;https://istio.io/docs/concepts/what-is-istio/overview/&quot;&gt;istio&lt;/a&gt;, &lt;a href=&quot;https://libvirt.org/index.html&quot;&gt;libvirt&lt;/a&gt;, &lt;a href=&quot;http://ebtables.netfilter.org/&quot;&gt;ebtables&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Iptables&quot;&gt;iptables&lt;/a&gt;, and &lt;a href=&quot;https://github.com/LiamHaworth/go-tproxy&quot;&gt;tproxy&lt;/a&gt;. Please review the links provided for an overview and deeper dive into each technology&lt;/p&gt;

&lt;h1 id=&quot;research-explanation&quot;&gt;Research explanation&lt;/h1&gt;
&lt;p&gt;Our research goal was to give virtual machines running inside pods (kubevirt project) all the benefits kubernetes have to offer, one of them is a service mesh like istio.&lt;/p&gt;

&lt;h2 id=&quot;iptables-only-with-dnat-and-source-nat-configuration&quot;&gt;Iptables only with dnat and source nat configuration&lt;/h2&gt;
&lt;p&gt;&lt;span style=&quot;color:red;&quot;&gt;This configuration is istio only!&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;For this solution we created the following architecture&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/2018-06-03-Research-run-VMs-with-istio-service-mesh/Iptables-diagram.png&quot; alt=&quot;Iptables-Diagram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With the follow yaml configuration&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: application-devel
  labels:
    app: libvirtd-devel
spec:
  ports:
  - port: 9080
    name: http
  selector:
    app: libvirtd-devel

---
apiVersion: v1
kind: Service
metadata:
  name: libvirtd-client-devel
  labels:
    app: libvirtd-devel
spec:
  ports:
  - port: 16509
    name: client-connection
  - port: 5900
    name: spice
  - port: 22
    name: ssh
  selector:
    app: libvirtd-devel
  type: LoadBalancer
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  creationTimestamp: null
  name: libvirtd-devel
spec:
  replicas: 1
  strategy: {}
  template:
    metadata:
      annotations:
        sidecar.istio.io/status: '{&quot;version&quot;:&quot;43466efda2266e066fb5ad36f2d1658de02fc9411f6db00ccff561300a2a3c78&quot;,&quot;initContainers&quot;:[&quot;istio-init&quot;,&quot;enable-core-dump&quot;],&quot;containers&quot;:[&quot;istio-proxy&quot;],&quot;volumes&quot;:[&quot;istio-envoy&quot;,&quot;istio-certs&quot;]}'
      creationTimestamp: null
      labels:
        app: libvirtd-devel
    spec:
      containers:
      - image: docker.io/sebassch/mylibvirtd:devel
        imagePullPolicy: Always
        name: compute
        ports:
        - containerPort: 9080
        - containerPort: 16509
        - containerPort: 5900
        - containerPort: 22
        securityContext:
          capabilities:
            add:
            - ALL
          privileged: true
          runAsUser: 0
        volumeMounts:
          - mountPath: /var/lib/libvirt/images
            name: test-volume
          - mountPath: /host-dev
            name: host-dev
          - mountPath: /host-sys
            name: host-sys
        resources: {}
        env:
          - name: LIBVIRTD_DEFAULT_NETWORK_DEVICE
            value: &quot;eth0&quot;
      - args:
        - proxy
        - sidecar
        - --configPath
        - /etc/istio/proxy
        - --binaryPath
        - /usr/local/bin/envoy
        - --serviceCluster
        - productpage
        - --drainDuration
        - 45s
        - --parentShutdownDuration
        - 1m0s
        - --discoveryAddress
        - istio-pilot.istio-system:15005
        - --discoveryRefreshDelay
        - 1s
        - --zipkinAddress
        - zipkin.istio-system:9411
        - --connectTimeout
        - 10s
        - --statsdUdpAddress
        - istio-mixer.istio-system:9125
        - --proxyAdminPort
        - &quot;15000&quot;
        - --controlPlaneAuthPolicy
        - MUTUAL_TLS
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: INSTANCE_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: docker.io/istio/proxy:0.7.1
        imagePullPolicy: IfNotPresent
        name: istio-proxy
        resources: {}
        securityContext:
          privileged: false
          readOnlyRootFilesystem: true
          runAsUser: 1337
        volumeMounts:
        - mountPath: /etc/istio/proxy
          name: istio-envoy
        - mountPath: /etc/certs/
          name: istio-certs
          readOnly: true
      initContainers:
      - args:
        - -p
        - &quot;15001&quot;
        - -u
        - &quot;1337&quot;
        image: docker.io/istio/proxy_init:0.7.1
        imagePullPolicy: IfNotPresent
        name: istio-init
        resources: {}
        securityContext:
          capabilities:
            add:
            - NET_ADMIN
      - args:
        - -c
        - sysctl -w kernel.core_pattern=/etc/istio/proxy/core.%e.%p.%t &amp;amp;&amp;amp; ulimit -c
          unlimited
        command:
        - /bin/sh
        image: alpine
        imagePullPolicy: IfNotPresent
        name: enable-core-dump
        resources: {}
        securityContext:
          privileged: true
      volumes:
      - emptyDir:
          medium: Memory
        name: istio-envoy
      - name: istio-certs
        secret:
          optional: true
          secretName: istio.default
      - name: host-dev
        hostPath:
          path: /dev
          type: Directory
      - name: host-sys
        hostPath:
          path: /sys
          type: Directory
      - name: test-volume
        hostPath:
          # directory location on host
          path: /bricks/brick1/volume/Images
          # this field is optional
          type: Directory
status: {}

---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: gateway-devel
  annotations:
    kubernetes.io/ingress.class: &quot;istio&quot;
spec:
  rules:
  - http:
      paths:
      - path: /devel-myvm
        backend:
          serviceName: application-devel
          servicePort: 9080
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When the my-libvirt container starts it runs an entry point script for iptables configuration.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1. iptables -t nat -D PREROUTING 1
2. iptables -t nat -A PREROUTING -p tcp -m comment --comment &quot;Kubevirt Spice&quot;  --dport 5900 -j ACCEPT
3. iptables -t nat -A PREROUTING -p tcp -m comment --comment &quot;Kubevirt virt-manager&quot;  --dport 16509 -j ACCEPT
4. iptables -t nat  -A PREROUTING -d 10.96.0.0/12 -m comment --comment &quot;istio/redirect-ip-range-10.96.0.0/12-service cidr&quot; -j ISTIO_REDIRECT
5. iptables -t nat  -A PREROUTING -d 192.168.0.0/16 -m comment --comment &quot;istio/redirect-ip-range-192.168.0.0/16-Pod cidr&quot; -j ISTIO_REDIRECT
6. iptables -t nat  -A OUTPUT -d 127.0.0.1/32 -p tcp -m comment --comment &quot;Kubevirt mesh application port&quot; --dport 9080 -j DNAT --to-destination 10.0.0.2
7. iptables -t nat  -A POSTROUTING -s 127.0.0.1/32 -d 10.0.0.2/32 -m comment --comment &quot;Kubevirt VM Forward&quot; -j SNAT --to-source `ifconfig eth0 | grep inet | awk '{print $2}'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now lets explain every one of this lines:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Remove istio ingress connection rule that send all the ingress traffic directly to the envoy proxy (our vm traffic is ingress traffic for our pod)&lt;/li&gt;
  &lt;li&gt;Allow ingress connection with spice port to get our libvirt process running in the pod&lt;/li&gt;
  &lt;li&gt;Allow ingress connection with virt-manager port to get our libvirt process running in the pod&lt;/li&gt;
  &lt;li&gt;Redirect all the traffic that came from the k8s clusters services to the envoy process&lt;/li&gt;
  &lt;li&gt;Redirect all the traffic that came from the k8s clusters pods to the envoy process&lt;/li&gt;
  &lt;li&gt;Send all the traffic that came from envoy process to our vm by changing the destination ip address to ur vm ip address&lt;/li&gt;
  &lt;li&gt;Change the source ip address of the packet send by envoy from localhost to the pod ip address so the virtual machine can return the connection&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;iptables-configuration-conclusions&quot;&gt;Iptables configuration conclusions&lt;/h3&gt;
&lt;p&gt;With this configuration all the traffic that exit the virtual machine to a k8s service will pass the envoy process and will enter the istio service mash.
Also all the traffic that came into the pod will be pass to envoy and after that it will be send to our virtual machine&lt;/p&gt;

&lt;p&gt;Egress data flow in this solution:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/2018-06-03-Research-run-VMs-with-istio-service-mesh/iptables-egress.png&quot; alt=&quot;iptables-egress-traffic&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ingress data flow in this solution:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/2018-06-03-Research-run-VMs-with-istio-service-mesh/iptables-ingress.png&quot; alt=&quot;iptables-ingress-traffic&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;No external modules needed&lt;/li&gt;
  &lt;li&gt;No external process needed&lt;/li&gt;
  &lt;li&gt;All the traffic is handled by the kernel user space not involved&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:red;&quot;&gt;Istio dedicated solution!&lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;Not other process can change the iptables rules&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;iptables-with-a-nat-proxy-process&quot;&gt;Iptables with a nat-proxy process&lt;/h2&gt;
&lt;p&gt;For this solution a created the following architecture&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/2018-06-03-Research-run-VMs-with-istio-service-mesh/nat-proxy.png&quot; alt=&quot;nat-proxy-Diagram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With the follow yaml configuration&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: application-nat-proxt
  labels:
    app: libvirtd-nat-proxt
spec:
  ports:
  - port: 9080
    name: http
  selector:
    app: libvirtd-nat-proxt
  type: LoadBalancer

---
apiVersion: v1
kind: Service
metadata:
  name: libvirtd-client-nat-proxt
  labels:
    app: libvirtd-nat-proxt
spec:
  ports:
  - port: 16509
    name: client-connection
  - port: 5900
    name: spice
  - port: 22
    name: ssh
  selector:
    app: libvirtd-nat-proxt
  type: LoadBalancer
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  creationTimestamp: null
  name: libvirtd-nat-proxt
spec:
  replicas: 1
  strategy: {}
  template:
    metadata:
      annotations:
        sidecar.istio.io/status: '{&quot;version&quot;:&quot;43466efda2266e066fb5ad36f2d1658de02fc9411f6db00ccff561300a2a3c78&quot;,&quot;initContainers&quot;:[&quot;istio-init&quot;,&quot;enable-core-dump&quot;],&quot;containers&quot;:[&quot;istio-proxy&quot;],&quot;volumes&quot;:[&quot;istio-envoy&quot;,&quot;istio-certs&quot;]}'
      creationTimestamp: null
      labels:
        app: libvirtd-nat-proxt
    spec:
      containers:
      - image: docker.io/sebassch/mylibvirtd:devel
        imagePullPolicy: Always
        name: compute
        ports:
        - containerPort: 9080
        - containerPort: 16509
        - containerPort: 5900
        - containerPort: 22
        securityContext:
          capabilities:
            add:
            - ALL
          privileged: true
          runAsUser: 0
        volumeMounts:
          - mountPath: /var/lib/libvirt/images
            name: test-volume
          - mountPath: /host-dev
            name: host-dev
          - mountPath: /host-sys
            name: host-sys
        resources: {}
        env:
          - name: LIBVIRTD_DEFAULT_NETWORK_DEVICE
            value: &quot;eth0&quot;
      - image: docker.io/sebassch/mynatproxy:devel
        imagePullPolicy: Always
        name: proxy
        resources: {}
        securityContext:
          privileged: true
          capabilities:
            add:
            - NET_ADMIN
      - args:
        - proxy
        - sidecar
        - --configPath
        - /etc/istio/proxy
        - --binaryPath
        - /usr/local/bin/envoy
        - --serviceCluster
        - productpage
        - --drainDuration
        - 45s
        - --parentShutdownDuration
        - 1m0s
        - --discoveryAddress
        - istio-pilot.istio-system:15005
        - --discoveryRefreshDelay
        - 1s
        - --zipkinAddress
        - zipkin.istio-system:9411
        - --connectTimeout
        - 10s
        - --statsdUdpAddress
        - istio-mixer.istio-system:9125
        - --proxyAdminPort
        - &quot;15000&quot;
        - --controlPlaneAuthPolicy
        - MUTUAL_TLS
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: INSTANCE_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: docker.io/istio/proxy:0.7.1
        imagePullPolicy: IfNotPresent
        name: istio-proxy
        resources: {}
        securityContext:
          privileged: false
          readOnlyRootFilesystem: true
          runAsUser: 1337
        volumeMounts:
        - mountPath: /etc/istio/proxy
          name: istio-envoy
        - mountPath: /etc/certs/
          name: istio-certs
          readOnly: true
      initContainers:
      - args:
        - -p
        - &quot;15001&quot;
        - -u
        - &quot;1337&quot;
        - -i
        - 10.96.0.0/12,192.168.0.0/16
        image: docker.io/istio/proxy_init:0.7.1
        imagePullPolicy: IfNotPresent
        name: istio-init
        resources: {}
        securityContext:
          capabilities:
            add:
            - NET_ADMIN
      - args:
        - -c
        - sysctl -w kernel.core_pattern=/etc/istio/proxy/core.%e.%p.%t &amp;amp;&amp;amp; ulimit -c
          unlimited
        command:
        - /bin/sh
        image: alpine
        imagePullPolicy: IfNotPresent
        name: enable-core-dump
        resources: {}
        securityContext:
          privileged: true
      volumes:
      - emptyDir:
          medium: Memory
        name: istio-envoy
      - name: istio-certs
        secret:
          optional: true
          secretName: istio.default
      - name: host-dev
        hostPath:
          path: /dev
          type: Directory
      - name: host-sys
        hostPath:
          path: /sys
          type: Directory
      - name: test-volume
        hostPath:
          # directory location on host
          path: /bricks/brick1/volume/Images
          # this field is optional
          type: Directory
status: {}

---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: gateway-nat-proxt
  annotations:
    kubernetes.io/ingress.class: &quot;istio&quot;
spec:
  rules:
  - http:
      paths:
      - path: /nat-proxt-myvm
        backend:
          serviceName: application-nat-proxt
          servicePort: 9080
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When the mynatproxy container starts it runs an entry point script for iptables configuration.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1. iptables -t nat -I PREROUTING 1 -p tcp -s 10.0.1.2 -m comment --comment &quot;nat-proxy redirect&quot; -j REDIRECT --to-ports 8080
2. iptables -t nat -I OUTPUT 1 -p tcp -s 10.0.1.2 -j ACCEPT
3. iptables -t nat -I POSTROUTING 1 -s 10.0.1.2 -p udp -m comment --comment &quot;nat udp connections&quot; -j MASQUERADE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now lets explain every one of this lines:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Redirect all the tcp traffic that came from the virtual machine to our proxy on port 8080&lt;/li&gt;
  &lt;li&gt;Accept all the traffic that go from the pod to the virtual machine&lt;/li&gt;
  &lt;li&gt;Nat all the udp praffic that came from the virtual machine&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This solution uses a container I created that has two processes inside, one for the egress traffic of the virtual machine and one for the ingress traffic.
For the egress traffic i used a program writed in golang, and for the ingress traffic I used haproxy.&lt;/p&gt;

&lt;p&gt;The nat-proxy used a system call to get the original destination address and port that its been redirected to us from the iptable rules I created.&lt;/p&gt;

&lt;p&gt;The extract function:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;func getOriginalDst(clientConn *net.TCPConn) (ipv4 string, port uint16, newTCPConn *net.TCPConn, err error) {
    if clientConn == nil {
        log.Printf(&quot;copy(): oops, dst is nil!&quot;)
        err = errors.New(&quot;ERR: clientConn is nil&quot;)
        return
    }

    // test if the underlying fd is nil
    remoteAddr := clientConn.RemoteAddr()
    if remoteAddr == nil {
        log.Printf(&quot;getOriginalDst(): oops, clientConn.fd is nil!&quot;)
        err = errors.New(&quot;ERR: clientConn.fd is nil&quot;)
        return
    }

    srcipport := fmt.Sprintf(&quot;%v&quot;, clientConn.RemoteAddr())

    newTCPConn = nil
    // net.TCPConn.File() will cause the receiver's (clientConn) socket to be placed in blocking mode.
    // The workaround is to take the File returned by .File(), do getsockopt() to get the original
    // destination, then create a new *net.TCPConn by calling net.Conn.FileConn().  The new TCPConn
    // will be in non-blocking mode.  What a pain.
    clientConnFile, err := clientConn.File()
    if err != nil {
        log.Printf(&quot;GETORIGINALDST|%v-&amp;gt;?-&amp;gt;FAILEDTOBEDETERMINED|ERR: could not get a copy of the client connection's file object&quot;, srcipport)
        return
    } else {
        clientConn.Close()
    }

    // Get original destination
    // this is the only syscall in the Golang libs that I can find that returns 16 bytes
    // Example result: &amp;amp;{Multiaddr:[2 0 31 144 206 190 36 45 0 0 0 0 0 0 0 0] Interface:0}
    // port starts at the 3rd byte and is 2 bytes long (31 144 = port 8080)
    // IPv4 address starts at the 5th byte, 4 bytes long (206 190 36 45)
    addr, err := syscall.GetsockoptIPv6Mreq(int(clientConnFile.Fd()), syscall.IPPROTO_IP, SO_ORIGINAL_DST)
    log.Printf(&quot;getOriginalDst(): SO_ORIGINAL_DST=%+v\n&quot;, addr)
    if err != nil {
        log.Printf(&quot;GETORIGINALDST|%v-&amp;gt;?-&amp;gt;FAILEDTOBEDETERMINED|ERR: getsocketopt(SO_ORIGINAL_DST) failed: %v&quot;, srcipport, err)
        return
    }
    newConn, err := net.FileConn(clientConnFile)
    if err != nil {
        log.Printf(&quot;GETORIGINALDST|%v-&amp;gt;?-&amp;gt;%v|ERR: could not create a FileConn fron clientConnFile=%+v: %v&quot;, srcipport, addr, clientConnFile, err)
        return
    }
    if _, ok := newConn.(*net.TCPConn); ok {
        newTCPConn = newConn.(*net.TCPConn)
        clientConnFile.Close()
    } else {
        errmsg := fmt.Sprintf(&quot;ERR: newConn is not a *net.TCPConn, instead it is: %T (%v)&quot;, newConn, newConn)
        log.Printf(&quot;GETORIGINALDST|%v-&amp;gt;?-&amp;gt;%v|%s&quot;, srcipport, addr, errmsg)
        err = errors.New(errmsg)
        return
    }

    ipv4 = itod(uint(addr.Multiaddr[4])) + &quot;.&quot; +
        itod(uint(addr.Multiaddr[5])) + &quot;.&quot; +
        itod(uint(addr.Multiaddr[6])) + &quot;.&quot; +
        itod(uint(addr.Multiaddr[7]))
    port = uint16(addr.Multiaddr[2])&amp;lt;&amp;lt;8 + uint16(addr.Multiaddr[3])

    return
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After we get the original destination address and port we start a connection to it and copy all the packets.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;var streamWait sync.WaitGroup
streamWait.Add(2)

streamConn := func(dst io.Writer, src io.Reader) {
    io.Copy(dst, src)
    streamWait.Done()
}

go streamConn(remoteConn, VMconn)
go streamConn(VMconn, remoteConn)

streamWait.Wait()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The Haproxy help us with the ingress traffic with the follow configuration&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;defaults
  mode tcp
frontend main
  bind *:9080
  default_backend guest
backend guest
  server guest 10.0.1.2:9080 maxconn 2048
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It send all the traffic to our virtual machine on the service port the machine is listening.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/SchSeba/NatProxy&quot;&gt;Code repository&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;nat-proxy-conclusions&quot;&gt;nat proxy conclusions&lt;/h3&gt;
&lt;p&gt;This solution is a general solution, not a dedicated solution to istio only. Its make the vm traffic look like a regular process inside the pod so it will work with any sidecars projects&lt;/p&gt;

&lt;p&gt;Egress data flow in this solution:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/2018-06-03-Research-run-VMs-with-istio-service-mesh/nat-proxy-egress-traffic.png&quot; alt=&quot;nat-proxy-egress-traffic&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ingress data flow in this solution:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/2018-06-03-Research-run-VMs-with-istio-service-mesh/nat-proxy-ingress.png&quot; alt=&quot;nat-proxy-ingress-traffic&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;No external modules needed&lt;/li&gt;
  &lt;li&gt;Works with any sidecar solution&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Not other process can change the iptables rules&lt;/li&gt;
  &lt;li&gt;External process needed&lt;/li&gt;
  &lt;li&gt;The traffic is passed to user space&lt;/li&gt;
  &lt;li&gt;Only support ingress TCP connection&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;iptables-with-a-trasperent-proxy-process&quot;&gt;Iptables with a trasperent-proxy process&lt;/h2&gt;
&lt;p&gt;This is the last solution I used in my research, it use a kernel module named TPROXY The &lt;a href=&quot;https://www.kernel.org/doc/Documentation/networking/tproxy.txt&quot;&gt;official documentation&lt;/a&gt; from the linux kernel documentation.&lt;/p&gt;

&lt;p&gt;For this solution a created the following architecture&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/2018-06-03-Research-run-VMs-with-istio-service-mesh/semi-tproxy-diagram.png&quot; alt=&quot;semi-tproxy-Diagram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With the follow yaml configuration&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: application-devel
  labels:
    app: libvirtd-devel
spec:
  ports:
  - port: 9080
    name: http
  selector:
    app: libvirtd-devel
  type: LoadBalancer

---
apiVersion: v1
kind: Service
metadata:
  name: libvirtd-client-devel
  labels:
    app: libvirtd-devel
spec:
  ports:
  - port: 16509
    name: client-connection
  - port: 5900
    name: spice
  - port: 22
    name: ssh
  selector:
    app: libvirtd-devel
  type: LoadBalancer
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  creationTimestamp: null
  name: libvirtd-devel
spec:
  replicas: 1
  strategy: {}
  template:
    metadata:
      annotations:
        sidecar.istio.io/status: '{&quot;version&quot;:&quot;43466efda2266e066fb5ad36f2d1658de02fc9411f6db00ccff561300a2a3c78&quot;,&quot;initContainers&quot;:[&quot;istio-init&quot;,&quot;enable-core-dump&quot;],&quot;containers&quot;:[&quot;istio-proxy&quot;],&quot;volumes&quot;:[&quot;istio-envoy&quot;,&quot;istio-certs&quot;]}'
      creationTimestamp: null
      labels:
        app: libvirtd-devel
    spec:
      containers:
      - image: docker.io/sebassch/mylibvirtd:devel
        imagePullPolicy: Always
        name: compute
        ports:
        - containerPort: 9080
        - containerPort: 16509
        - containerPort: 5900
        - containerPort: 22
        securityContext:
          capabilities:
            add:
            - ALL
          privileged: true
          runAsUser: 0
        volumeMounts:
          - mountPath: /var/lib/libvirt/images
            name: test-volume
          - mountPath: /host-dev
            name: host-dev
          - mountPath: /host-sys
            name: host-sys
        resources: {}
        env:
          - name: LIBVIRTD_DEFAULT_NETWORK_DEVICE
            value: &quot;eth0&quot;
      - image: docker.io/sebassch/mytproxy:devel
        imagePullPolicy: Always
        name: proxy
        resources: {}
        securityContext:
          privileged: true
          capabilities:
            add:
            - NET_ADMIN
      - args:
        - proxy
        - sidecar
        - --configPath
        - /etc/istio/proxy
        - --binaryPath
        - /usr/local/bin/envoy
        - --serviceCluster
        - productpage
        - --drainDuration
        - 45s
        - --parentShutdownDuration
        - 1m0s
        - --discoveryAddress
        - istio-pilot.istio-system:15005
        - --discoveryRefreshDelay
        - 1s
        - --zipkinAddress
        - zipkin.istio-system:9411
        - --connectTimeout
        - 10s
        - --statsdUdpAddress
        - istio-mixer.istio-system:9125
        - --proxyAdminPort
        - &quot;15000&quot;
        - --controlPlaneAuthPolicy
        - MUTUAL_TLS
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: INSTANCE_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: docker.io/istio/proxy:0.7.1
        imagePullPolicy: IfNotPresent
        name: istio-proxy
        resources: {}
        securityContext:
          privileged: false
          readOnlyRootFilesystem: true
          runAsUser: 1337
        volumeMounts:
        - mountPath: /etc/istio/proxy
          name: istio-envoy
        - mountPath: /etc/certs/
          name: istio-certs
          readOnly: true
      initContainers:
      - args:
        - -p
        - &quot;15001&quot;
        - -u
        - &quot;1337&quot;
        - -i
        - 10.96.0.0/12,192.168.0.0/16
        image: docker.io/istio/proxy_init:0.7.1
        imagePullPolicy: IfNotPresent
        name: istio-init
        resources: {}
        securityContext:
          capabilities:
            add:
            - NET_ADMIN
      - args:
        - -c
        - sysctl -w kernel.core_pattern=/etc/istio/proxy/core.%e.%p.%t &amp;amp;&amp;amp; ulimit -c
          unlimited
        command:
        - /bin/sh
        image: alpine
        imagePullPolicy: IfNotPresent
        name: enable-core-dump
        resources: {}
        securityContext:
          privileged: true
      volumes:
      - emptyDir:
          medium: Memory
        name: istio-envoy
      - name: istio-certs
        secret:
          optional: true
          secretName: istio.default
      - name: host-dev
        hostPath:
          path: /dev
          type: Directory
      - name: host-sys
        hostPath:
          path: /sys
          type: Directory
      - name: test-volume
        hostPath:
          # directory location on host
          path: /bricks/brick1/volume/Images
          # this field is optional
          type: Directory
status: {}

---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: gateway-devel
  annotations:
    kubernetes.io/ingress.class: &quot;istio&quot;
spec:
  rules:
  - http:
      paths:
      - path: /devel-myvm
        backend:
          serviceName: application-devel
          servicePort: 9080
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When the tproxy container starts it runs an entry point script for iptables configuration but this time the proxy redirect came in the mangle table and not in the nat table that because TPROXY module avilable only in the mangle table.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;TPROXY
This target is only valid in the mangle table, in the
PREROUTING chain and user-defined chains which are only
called from this chain.  It redirects the packet to a local
socket without changing the packet header in any way. It can
also change the mark value which can then be used in
advanced routing rules.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;iptables rules:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;iptables -t mangle -vL
iptables -t mangle -N KUBEVIRT_DIVERT
iptables -t mangle -A KUBEVIRT_DIVERT -j MARK --set-mark 8
iptables -t mangle -A KUBEVIRT_DIVERT -j ACCEPT

table=mangle
iptables -t ${table} -N KUBEVIRT_INBOUND
iptables -t ${table} -A PREROUTING -p tcp -m comment --comment &quot;Kubevirt Spice&quot;  --dport 5900 -j RETURN
iptables -t ${table} -A PREROUTING -p tcp -m comment --comment &quot;Kubevirt virt-manager&quot;  --dport 16509 -j RETURN
iptables -t ${table} -A PREROUTING -p tcp -i vnet0 -j KUBEVIRT_INBOUND

iptables -t ${table} -N KUBEVIRT_TPROXY
iptables -t ${table} -A KUBEVIRT_TPROXY ! -d 127.0.0.1/32 -p tcp -j TPROXY --tproxy-mark 8/0xffffffff --on-port 9401
#iptables -t mangle -A KUBEVIRT_TPROXY ! -d 127.0.0.1/32 -p udp -j TPROXY --tproxy-mark 8/0xffffffff --on-port 8080

# If an inbound packet belongs to an established socket, route it to the
# loopback interface.
iptables -t ${table} -A KUBEVIRT_INBOUND -p tcp -m socket -j KUBEVIRT_DIVERT
#iptables -t mangle -A KUBEVIRT_INBOUND -p udp -m socket -j KUBEVIRT_DIVERT

# Otherwise, it's a new connection. Redirect it using TPROXY.
iptables -t ${table} -A KUBEVIRT_INBOUND -p tcp -j KUBEVIRT_TPROXY
#iptables -t mangle -A KUBEVIRT_INBOUND -p udp -j KUBEVIRT_TPROXY
iptables -t ${table} -I OUTPUT 1 -d 10.0.1.2 -j ACCEPT

table=nat
# Remove vm Connection from iptables rules
iptables -t ${table} -I PREROUTING 1 -s 10.0.1.2 -j ACCEPT
iptables -t ${table} -I OUTPUT 1 -d 10.0.1.2 -j ACCEPT

# Allow guest -&amp;gt; world -- using nat for UDP
iptables -t ${table} -I POSTROUTING 1 -s 10.0.1.2 -p udp -j MASQUERADE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For this solution we also need to load the bridge kernel module&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;modprobe bridge
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And create some ebtables rules so egress and ingress traffict from the virtial machine will exit the l2 rules and pass to the l3 rules:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  ebtables -t broute -F # Flush the table
    # inbound traffic
    ebtables -t broute -A BROUTING -p IPv4 --ip-dst 10.0.1.2 \
    -j redirect --redirect-target DROP
    # returning outbound traffic
    ebtables -t broute -A BROUTING -p IPv4 --ip-src 10.0.1.2 \
    -j redirect --redirect-target DROP
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We also need to disable rp_filter on the virtual machine interface and the libvirt bridge interface&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;echo 0 &amp;gt; /proc/sys/net/ipv4/conf/virbr0/rp_filter
echo 0 &amp;gt; /proc/sys/net/ipv4/conf/virbr0-nic/rp_filter
echo 0 &amp;gt; /proc/sys/net/ipv4/conf/vnet0/rp_filter
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After this configuration the container start the semi-tproxy process for egress traffic and the haproxy process for the ingress traffic.&lt;/p&gt;

&lt;p&gt;The semi-tproxy program is a golag program,binding a listener socket with the IP_TRANSPARENT socket option
Preparing a socket to receive connections with TProxy is really no different than what is normally done when setting up a socket to listen for connections. The only difference in the process is before the socket is bound, the IP_TRANSPARENT socket option.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;syscall.SetsockoptInt(fileDescriptor, syscall.SOL_IP, syscall.IP_TRANSPARENT, 1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;About IP_TRANSPARENT&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;IP_TRANSPARENT (since Linux 2.6.24)
Setting this boolean option enables transparent proxying on
this socket.  This socket option allows the calling applica‐
tion to bind to a nonlocal IP address and operate both as a
client and a server with the foreign address as the local
end‐point.  NOTE: this requires that routing be set up in
a way that packets going to the foreign address are routed
through the TProxy box (i.e., the system hosting the
application that employs the IP_TRANSPARENT socket option).
Enabling this socket option requires superuser privileges
(the CAP_NET_ADMIN capability).

TProxy redirection with the iptables TPROXY target also
requires that this option be set on the redirected socket.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then we set the IP_TRANSPARENT socket option on outbound connections
Same goes for making connections to a remote host pretending to be the client, the IP_TRANSPARENT socket option is set and the Linux kernel will allow the bind so along as a connection was intercepted with those details being used for the bind.&lt;/p&gt;

&lt;p&gt;When the process get a new connection we start a connection to the real destination address and copy the traffic between both sockets&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;var streamWait sync.WaitGroup
streamWait.Add(2)

streamConn := func(dst io.Writer, src io.Reader) {
    io.Copy(dst, src)
    streamWait.Done()
}

go streamConn(remoteConn, VMconn)
go streamConn(VMconn, remoteConn)

streamWait.Wait()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The Haproxy helps us with the ingress traffic with the follow configuration&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;defaults
  mode tcp
frontend main
  bind *:9080
  default_backend guest
backend guest
  server guest 10.0.1.2:9080 maxconn 2048
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It send all the traffic to our virtual machine on the service port the machine is listening.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/SchSeba/SemiTrasperentProxy&quot;&gt;Code repository&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;tproxy-conclusions&quot;&gt;tproxy conclusions&lt;/h3&gt;
&lt;p&gt;This solution is a general solution, not a dedicated solution to istio only. Its make the vm traffic look like a regular process inside the pod so it will work with any sidecars projects&lt;/p&gt;

&lt;p&gt;Egress data flow in this solution:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/2018-06-03-Research-run-VMs-with-istio-service-mesh/semi-tproxy-egress.png&quot; alt=&quot;tproxy-egress-traffic&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ingress data flow in this solution:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/2018-06-03-Research-run-VMs-with-istio-service-mesh/nat-proxy-ingress.png&quot; alt=&quot;tproxy-ingress-traffic&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;other process can change the nat table (this solution works on the mangle table)&lt;/li&gt;
  &lt;li&gt;better preformance comparing to nat-proxy&lt;/li&gt;
  &lt;li&gt;Works with any sidecar solution&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Need NET_ADMIN capability for the docker&lt;/li&gt;
  &lt;li&gt;External process needed&lt;/li&gt;
  &lt;li&gt;The traffic is passed to user space&lt;/li&gt;
  &lt;li&gt;Only support ingress TCP connection&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;research-conclustion&quot;&gt;Research Conclustion&lt;/h1&gt;
&lt;p&gt;Kubevirt shows it is possible to run virtual machines inside a kubernetes cluster, and this post shows that the virtual machine can also get the benefit of it.&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Jun 2018 00:00:00 -0400</pubDate>
        <pubYear>Wed, 31 Dec 1969 19:33:38 -0500</pubYear>
        <link>https://www.kubevirt.io//2018/Research-run-VMs-with-istio-service-mesh.html</link>
        <guid isPermaLink="true">https://www.kubevirt.io//2018/Research-run-VMs-with-istio-service-mesh.html</guid>
        
        
        <category>uncategorized</category>
        
      </item>
    
      <item>
        <title>Use Vs Code For Kube Virt Development</title>
        <description>&lt;p&gt;In this post we will install and configure Visual studio code (vscode) for KubeVirt development and debug.&lt;/p&gt;

&lt;p&gt;Visual Studio Code is a source code editor developed by Microsoft for Windows, Linux and macOS.&lt;/p&gt;

&lt;p&gt;It includes support for debugging, embedded Git control, syntax highlighting, intelligent code completion, snippets, and code refactoring.&lt;/p&gt;

&lt;h1 id=&quot;golang-installation&quot;&gt;Golang Installation&lt;/h1&gt;

&lt;p&gt;GO installation is required, We can find the binaries in &lt;a href=&quot;https://golang.org/dl/&quot;&gt;golang page&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;golang-linux-installation&quot;&gt;Golang Linux Installation&lt;/h2&gt;

&lt;p&gt;After downloading the binaries extract them with the following command:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tar -C /usr/local -xzf go$VERSION.$OS-$ARCH.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now lets Add /usr/local/go/bin to the PATH environment variable.&lt;/p&gt;

&lt;p&gt;You can do this by adding this line to your /etc/profile (for a system-wide installation) or $HOME/.profile:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export PATH=$PATH:/usr/local/go/bin
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;golang-windows-installation&quot;&gt;Golang Windows Installation&lt;/h2&gt;

&lt;p&gt;Open the MSI file and follow the prompts to install the Go tools.&lt;/p&gt;

&lt;p&gt;By default, the installer puts the Go distribution in C:\Go.&lt;/p&gt;

&lt;p&gt;The installer should put the C:\Go\bin directory in your PATH environment variable.&lt;/p&gt;

&lt;p&gt;You may need to restart any open command prompts for the change to take effect.&lt;/p&gt;

&lt;h1 id=&quot;vscode-installation&quot;&gt;VSCODE Installation&lt;/h1&gt;
&lt;p&gt;Now we will install Visual Studio Code in our system.&lt;/p&gt;

&lt;h2 id=&quot;for-linux-machines&quot;&gt;For linux machines&lt;/h2&gt;
&lt;p&gt;We need to choose our linux distribution.&lt;/p&gt;

&lt;h3 id=&quot;for-rhelcentosfedora&quot;&gt;For RHEL/Centos/Fedora&lt;/h3&gt;
&lt;p&gt;The following script will install the key and repository:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc
sudo sh -c 'echo -e &quot;[code]\nname=Visual Studio Code\nbaseurl=https://packages.microsoft.com/yumrepos/vscode\nenabled=1\ngpgcheck=1\ngpgkey=https://packages.microsoft.com/keys/microsoft.asc&quot; &amp;gt; /etc/yum.repos.d/vscode.repo'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then update the package cache and install the package using dnf (Fedora 22 and above):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dnf check-update
sudo dnf install code
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Or on older versions using yum:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yum check-update
sudo yum install code
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;for-debianubuntu&quot;&gt;For Debian/Ubuntu&lt;/h3&gt;
&lt;p&gt;We need to download the .deb package from the &lt;a href=&quot;https://code.visualstudio.com/Download&quot;&gt;vscode download page&lt;/a&gt;,
and from the command line run the package management.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo dpkg -i &amp;lt;file&amp;gt;.deb
sudo apt-get install -f # Install dependencies
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;for-windows-machines&quot;&gt;For Windows machines&lt;/h1&gt;
&lt;p&gt;Download the &lt;a href=&quot;https://go.microsoft.com/fwlink/?LinkID=534107&quot;&gt;Visual Studio Code installer&lt;/a&gt;, and then run the installer (VSCodeSetup-version.exe)&lt;/p&gt;

&lt;h1 id=&quot;go-project-struct&quot;&gt;Go Project struct&lt;/h1&gt;
&lt;p&gt;Lets create the following structure for our kubevirt project development environment:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;├── &amp;lt;Go-projects-folder&amp;gt; # Your Golang projects root folder
│   ├── bin
│   ├── pkg
│   ├── src
│   │   ├── kubevirt.io
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now navigate to kubevirt.io folder and run:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone &amp;lt;kubevirt-fork&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;install-vscode-extensions&quot;&gt;Install VSCODE Extensions&lt;/h1&gt;
&lt;p&gt;Now we are going to install some extensions for a better development experience with the IDE.&lt;/p&gt;

&lt;p&gt;Open vscode and select your go project root folder you created in the last step.&lt;/p&gt;

&lt;p&gt;On the extensions tab (Ctrl+Shift+X), search for golang and install it.&lt;/p&gt;

&lt;p&gt;Now open the command palette (Ctrl+Shift+P) view-&amp;gt;Command Palette and type “Go: install/update tools”, this will install all the requirements for example: delve debugger, etc…&lt;/p&gt;

&lt;p&gt;(optional) We can install docker extension for syntax highlighting, commands, etc..&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/2018-05-22-Use-VS-Code-for-Kube-Virt-Development/extension-install.png&quot; alt=&quot;Extension-Install&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;gopath-and-goroot-configuration&quot;&gt;GOPATH and GOROOT configuration&lt;/h1&gt;
&lt;p&gt;Open the vscode configuration file (ctrl+,) file-&amp;gt;preferences-&amp;gt;settings.&lt;/p&gt;

&lt;p&gt;Now on the right file we need to add this configuration:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;go.gopath&quot;: &quot;&amp;lt;Go-projects-folder&amp;gt;&quot;,
&quot;go.goroot&quot;: &quot;/usr/local/go&quot;,
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;../assets/2018-05-22-Use-VS-Code-for-Kube-Virt-Development/settings.png&quot; alt=&quot;settings&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;create-debug-configuration&quot;&gt;Create debug configuration&lt;/h1&gt;
&lt;p&gt;For the last part we are going to configure the debugger file, open it by Debug-&amp;gt;Open Configurations and add to the configuration list the following structure&lt;/p&gt;

&lt;p&gt;** Change the &lt;Go-projects-folder&gt; parameter to your golang projects root directory&lt;/Go-projects-folder&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
            &quot;name&quot;: &quot;Kubevirt&quot;,
            &quot;type&quot;: &quot;go&quot;,
            &quot;request&quot;: &quot;launch&quot;,
            &quot;mode&quot;: &quot;debug&quot;,
            &quot;remotePath&quot;: &quot;&quot;,
            &quot;port&quot;: 2345,
            &quot;host&quot;: &quot;127.0.0.1&quot;,
            &quot;program&quot;: &quot;${fileDirname}&quot;,
            &quot;env&quot;: {},
            &quot;args&quot;: [&quot;--kubeconfig&quot;, &quot;cluster/k8s-1.9.3/.kubeconfig&quot;,
                     &quot;--port&quot;, &quot;1234&quot;],
            &quot;showLog&quot;: true,
            &quot;cwd&quot;: &quot;${workspaceFolder}/src/kubevirt.io/kubevirt&quot;,
            &quot;output&quot;: &quot;&amp;lt;Go-projects-folder&amp;gt;/bin/${fileBasenameNoExtension}&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;../assets/2018-05-22-Use-VS-Code-for-Kube-Virt-Development/debug-config.png&quot; alt=&quot;debug-config&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;debug-process&quot;&gt;Debug Process&lt;/h1&gt;
&lt;p&gt;For debug we need to open the main package we want to debug.&lt;/p&gt;

&lt;p&gt;For example if we want to debug the virt-api component, open the main package:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubevirt.io/cmd/virt-api/virt-api.go
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;../assets/2018-05-22-Use-VS-Code-for-Kube-Virt-Development/debug-file.png&quot; alt=&quot;debug-file&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now change to debug view (ctrl+shift+D), check that we are using the kubevirt configuration and hit the play button&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/2018-05-22-Use-VS-Code-for-Kube-Virt-Development/debug-view.png&quot; alt=&quot;debug-view&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;more-information&quot;&gt;More Information&lt;/h2&gt;
&lt;p&gt;For more information, keyboard shortcuts and advance vscode usage please refer the following link&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://code.visualstudio.com/docs/editor/codebasics&quot;&gt;editor code basics&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 22 May 2018 00:00:00 -0400</pubDate>
        <pubYear>Wed, 31 Dec 1969 19:33:38 -0500</pubYear>
        <link>https://www.kubevirt.io//2018/Use-VS-Code-for-Kube-Virt-Development.html</link>
        <guid isPermaLink="true">https://www.kubevirt.io//2018/Use-VS-Code-for-Kube-Virt-Development.html</guid>
        
        
        <category>uncategorized</category>
        
      </item>
    
      <item>
        <title>Ovn Multi Network Plugin For Kubernetes Kubetron</title>
        <description>&lt;p&gt;Kubernetes networking model is suited for containerized applications, based mostly around L4 and L7 services, where all pods are connected to one big network. This is perfectly ok for most use cases. However, sometimes there is a need for fine-grained network configuration with better control. Use-cases such as L2 networks, static IP addresses, interfaces dedicated for storage traffic etc. For such needs there is ongoing effort in Kubernetes sig-network to support multiple networks (see &lt;a href=&quot;https://docs.google.com/document/d/1Ny03h6IDVy_e_vmElOqR7UdTPAG_RNydhVE1Kx54kFQ&quot;&gt;Kubernetes Network CRD De-Facto Standard&lt;/a&gt;. There exist many prototypes of plugins providing such functionality. You are reading about one of them.&lt;/p&gt;

&lt;p&gt;Kubetron (working name, &lt;code class=&quot;highlighter-rouge&quot;&gt;kubernetes + neutron&lt;/code&gt;, quite misleading since we want to support setup without Neutron involved too), allows users to connect their pods to multiple networks configured on OVN. Important part here is, that such networks are configured by an external tool, be it OVN Northbound Database client or higher level tool such as Neutron or oVirt Provider OVN. This allows administrators to configure complicated networks, Kubernetes then only knows enough about the known networks to be able to connect to them - but not all the complexity involved to manage them. Kubetron does not affect default Kubernetes networking at all, default networks will be left intact.&lt;/p&gt;

&lt;p&gt;In order to enable the use-cases outlined above, Kubetron can be used to provide multiple interfaces to a pod, further more KubeVirt will then use those interfaces to pass them to its virtual machines via the in progress &lt;a href=&quot;https://docs.google.com/document/d/10rXr91aqn8MvVcLgHw33WX8BaQwHPZERp25PHxoZGgw/edit?usp=sharing&quot;&gt;VirtualMachine networking API&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can find source code in &lt;a href=&quot;https://github.com/phoracek/kubetron&quot;&gt;Kubetron GitHub repository&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;Contents&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Desired Model and Usage&lt;/li&gt;
  &lt;li&gt;Proof of Concept&lt;/li&gt;
  &lt;li&gt;Demo&lt;/li&gt;
  &lt;li&gt;Try it Yourself&lt;/li&gt;
  &lt;li&gt;Looking for Help&lt;/li&gt;
  &lt;li&gt;Disclaimer&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;desired-model-and-usage&quot;&gt;Desired Model and Usage&lt;/h2&gt;

&lt;p&gt;Let’s talk about how Kubetron looks from administrator’s and user’s point of view. Please note that following examples are still for the desired state and some of them might not be implemented in PoC yet. If you want to learn more about deployment and architecture, check &lt;a href=&quot;https://docs.google.com/presentation/d/1KiHQyZngdaL8gtreL9Tmy7S1XiY5Sbnn0YuNCqhggF8/edit?usp=sharing&quot;&gt;Kubetron slide deck&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;configure-ovn-networks&quot;&gt;Configure OVN Networks&lt;/h3&gt;

&lt;p&gt;First of all, administrator must create and configure networks in OVN. That could be done either directly on OVN northbound database (e.g. using &lt;code class=&quot;highlighter-rouge&quot;&gt;ovn-nbctl&lt;/code&gt;) or via OVN manager (e.g. Neutron or oVirt Provider OVN, using ansible).&lt;/p&gt;

&lt;h3 id=&quot;expose-available-networks&quot;&gt;Expose Available Networks&lt;/h3&gt;

&lt;p&gt;Once the networks are configured, there are two options how to expose available networks to a user. First one is providing some form of access to OVN or Neutron API, this one is completely out of Kubernetes’ and Kubetron’s
scope. Second option is to enable Network object support (as described in Kubernetes Network CRD De-Facto standard). With this option, administrator must create a Network object per each OVN network is allowed to be used by a user. This object allows administrator to expose only limited subset of networks or to limit access per Namespace. This process could be automated, e.g. via a service that monitors available logical switches and exposes them as Networks.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# List networks (Logical Switches) directly from OVN Northbound database&lt;/span&gt;
ovn-nbctl ls-list

&lt;span class=&quot;c&quot;&gt;# List networks available on Neutron&lt;/span&gt;
neutron net-list

&lt;span class=&quot;c&quot;&gt;# List networks as Network objects created in Kubernetes&lt;/span&gt;
kubectl get networks
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;attach-pod-to-a-network&quot;&gt;Attach pod to a Network&lt;/h3&gt;

&lt;p&gt;Once user selects a desired network based on options described in previous section, he or she can request them for a pod using an annotation. This annotation is compatible with earlier mentioned Kubernetes Network CRD De-Facto Standard.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pod&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;network-consumer&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;annotations&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;kubernetes.v1.cni.cncf.io/networks&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;red&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# requested networks&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;busybox&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;busybox&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;access-the-network-from-the-pod&quot;&gt;Access the Network from the pod&lt;/h3&gt;

&lt;p&gt;Once the pod is created, a user can list its interfaces and their assigned IP addresses:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; network-consumer &lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; ip address
...
10: red-bcxoeffrsw: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1442 qdisc noqueue state UNKNOWN qlen 1000
    &lt;span class=&quot;nb&quot;&gt;link&lt;/span&gt;/ether 4e:71:3b:ee:a5:f4 brd ff:ff:ff:ff:ff:ff
    inet 10.1.0.3/24 brd 10.1.0.255 scope global dynamic red-bcxoeffrsw
       valid_lft 86371sec preferred_lft 86371sec
    inet6 fe80::4c71:3bff:feee:a5f4/64 scope &lt;span class=&quot;nb&quot;&gt;link
       &lt;/span&gt;valid_lft forever preferred_lft forever
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In order to make it easier to obtain the network’s interface name inside pod’s containers, environment variables with network-interface mapping are created:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$NETWORK_INTERFACE_RED&lt;/span&gt;
red-bcxoeffrsw
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;proof-of-concept&quot;&gt;Proof of Concept&lt;/h2&gt;

&lt;p&gt;As for now, current implementation does not completely implement the desired model yet:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Only Neutron mode is implemented, Kubetron can not be used with OVN alone&lt;/li&gt;
  &lt;li&gt;Network object handling is not implemented, Kubetron obtains networks directly from Neutron&lt;/li&gt;
  &lt;li&gt;Interface names are not exposed as environment variables&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It might be unstable and there are some missing parts. However, basic scenario works, at least in development environment.&lt;/p&gt;

&lt;h2 id=&quot;demo&quot;&gt;Demo&lt;/h2&gt;

&lt;p&gt;In the following recording we create two networks &lt;code class=&quot;highlighter-rouge&quot;&gt;red&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;blue&lt;/code&gt; using Neutron API via Ansible. Then we create two pods and connect them to both mentioned networks. And then we &lt;code class=&quot;highlighter-rouge&quot;&gt;ping&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://asciinema.org/a/7nB3vgIJcz05TxRNiaD2vLLdE&quot;&gt;&lt;img src=&quot;https://asciinema.org/a/7nB3vgIJcz05TxRNiaD2vLLdE.png&quot; alt=&quot;asciicast&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;try-it-yourself&quot;&gt;Try it Yourself&lt;/h2&gt;

&lt;p&gt;I encourage you to try Kubetron yourself. It has not yet been tested on regular Kubernetes deployment (and it likely won’t work without some tuning). Fortunately, Kubetron repository contains Vagrant file and set of scripts that will help you deploy multi-node Kubernetes
with OVN and Kubetron installed. On top of that it describes how to create networks and connect pods to them. Check out &lt;a href=&quot;https://github.com/phoracek/kubetron/blob/master/README.md&quot;&gt;Kubetron README.md&lt;/a&gt; and give it a try!&lt;/p&gt;

&lt;h2 id=&quot;looking-for-help&quot;&gt;Looking for Help&lt;/h2&gt;

&lt;p&gt;If you are interested in contributing to Kubetron, please follow its GitHub repository. There are many missing features and possible improvements, I will open issues to track them soon. Stay tuned!&lt;/p&gt;

&lt;h2 id=&quot;disclaimer&quot;&gt;Disclaimer&lt;/h2&gt;

&lt;p&gt;Kubetron is in early development stage, both it’s architecture and tools to use it will change.&lt;/p&gt;
</description>
        <pubDate>Wed, 16 May 2018 00:00:00 -0400</pubDate>
        <pubYear>Wed, 31 Dec 1969 19:33:38 -0500</pubYear>
        <link>https://www.kubevirt.io//2018/ovn-multi-network-plugin-for-kubernetes-kubetron.html</link>
        <guid isPermaLink="true">https://www.kubevirt.io//2018/ovn-multi-network-plugin-for-kubernetes-kubetron.html</guid>
        
        
        <category>uncategorized</category>
        
      </item>
    
      <item>
        <title>Use Glusterfs Cloning With Kubevirt</title>
        <description>&lt;p&gt;Gluster seems like a good fit for storage in kubernetes and in particular in kubevirt. Still, as for other storage backends, we will likely need to use a golden set of images and deploy vms from them.&lt;/p&gt;

&lt;p&gt;That’s where cloning feature of gluster comes at rescue!&lt;/p&gt;

&lt;h2 id=&quot;contents&quot;&gt;Contents&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Prerequisites&lt;/li&gt;
  &lt;li&gt;Installing Gluster provisioner&lt;/li&gt;
  &lt;li&gt;Using The cloning feature&lt;/li&gt;
  &lt;li&gt;Conclusion&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h2&gt;

&lt;p&gt;I assume you already have a running instance of openshift and kubevirt along with gluster and an already existing pvc where you copied a base operating system ( you can get those from &lt;a href=&quot;https://docs.openstack.org/image-guide/obtain-images.html&quot;&gt;here&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;For reference, I used the following components and versions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;3 baremetal servers with Rhel 7.4 as base OS&lt;/li&gt;
  &lt;li&gt;Openshift and Cns 3.9&lt;/li&gt;
  &lt;li&gt;KubeVirt latest&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;installing-gluster-provisioner&quot;&gt;Installing Gluster provisioner&lt;/h2&gt;

&lt;h3 id=&quot;initial-deployment&quot;&gt;initial deployment&lt;/h3&gt;

&lt;p&gt;We will deploy the custom provisioner using &lt;a href=&quot;../assets/2018-05-16-use-glustercloning-with-kubevirt/glusterfile-provisioner-template.yml&quot;&gt;this template&lt;/a&gt;, along with cluster rules located in &lt;a href=&quot;../assets/2018-05-16-use-glustercloning-with-kubevirt/openshift-clusterrole.yaml&quot;&gt;this file&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Note that we also patch the image to use an existing one from gluster org located at docker.io instead of quay.io, as the corresponding repository is private by the time of this writing, and the heketi one, to make sure it has the code required to handle cloning&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;NAMESPACE=&quot;app-storage&quot;
oc create -f openshift-clusterrole.yaml
oc process -f glusterfile-provisioner-template.yml | oc apply -f - -n $NAMESPACE
oc adm policy add-cluster-role-to-user cluster-admin -z glusterfile-provisioner -n $NAMESPACE
oc adm policy add-scc-to-user privileged -z glusterfile-provisioner
oc set image dc/heketi-storage heketi=gluster/heketiclone:latest  -n $NAMESPACE
oc set image dc/glusterfile-provisioner glusterfile-provisioner=gluster/glusterfileclone:latest  -n $NAMESPACE
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And you will see something similar to this in your storage namespace&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[root@master01 ~]# NAMESPACE=&quot;app-storage&quot;
[root@master01 ~]# kubectl get pods -n $NAMESPACE
NAME                              READY     STATUS    RESTARTS   AGE
glusterfile-provisioner-3-vhkx6   1/1       Running   0          1d
glusterfs-storage-b82x4           1/1       Running   1          23d
glusterfs-storage-czthc           1/1       Running   0          23d
glusterfs-storage-z68hm           1/1       Running   0          23d
heketi-storage-2-qdrks            1/1       Running   0          6h
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;additional-configuration&quot;&gt;additional configuration&lt;/h3&gt;

&lt;p&gt;for the custom provisioner to work, we need two additional things:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a storage class pointing to it, but also containing the details of the current heketi installation&lt;/li&gt;
  &lt;li&gt;a secret similar to the one used by the current heketi installation, but using a different &lt;em&gt;type&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can use the following&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;NAMESPACE=&quot;app-storage&quot;
oc get sc glusterfs-storage -o yaml
oc get secret heketi-storage-admin-secret -n $NAMESPACE-o yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;then, create the following objects:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;glustercloning-heketi-secret secret in your storage namespace&lt;/li&gt;
  &lt;li&gt;glustercloning storage class&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;for reference, here are samples of those files.&lt;/p&gt;

&lt;p&gt;Note how we change the type for the secret and add extra options for our storage class (in particular, enabling smartclone).&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: v1
data:
  key: eEt0NUJ4cklPSmpJb2RZcFpqVExSSjUveFV5WHI4L0NxcEtMME1WVlVjQT0=
kind: Secret
metadata:
  name: glustercloning-heketi-secret
  namespace: app-storage
type: gluster.org/glusterfile
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: glustercloning
parameters:
  restsecretname: glustercloning-heketi-secret
  restsecretnamespace: app-storage
  resturl: http://heketi-storage.192.168.122.10.xip.io
  restuser: admin
  smartclone: &quot;true&quot;
  snapfactor: &quot;10&quot;
  volumeoptions: group virt
provisioner: gluster.org/glusterfile
reclaimPolicy: Delete
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The full set of supported parameters can be found &lt;a href=&quot;https://github.com/kubernetes-incubator/external-storage/blob/master/gluster/file/README.md&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;using-the-cloning-feature&quot;&gt;Using the cloning feature&lt;/h2&gt;

&lt;p&gt;Once deployed, you can now provision pvcs from a base origin&lt;/p&gt;

&lt;h3 id=&quot;cloning-single-pvcs&quot;&gt;Cloning single pvcs&lt;/h3&gt;

&lt;p&gt;For instance, provided you have an existing pvc named &lt;em&gt;cirros&lt;/em&gt; containing this base operating system, and that this PVC contains an annotion of the following&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(...)
metadata:
 annotations:
  gluster.org/heketi-volume-id: f0cbbb29ef4202c5226f87708da57e5c
(...)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;you can create a cloned pvc with the following yaml ( note that we simply indicate a clone request in the annotations)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: testclone1
  namespace: default
  annotations:
    k8s.io/CloneRequest: cirros
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: glustercloning
status:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 1Gi
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once provisioned, the pvc will contain this additional annotation created by the provisioner&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(...)
metadata:
 annotations:
      k8s.io/CloneOf: cirros

(...)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;leveraging-the-feature-in-openshift-templates&quot;&gt;Leveraging the feature in openshift templates&lt;/h3&gt;

&lt;p&gt;We can make direct use of the feature in &lt;a href=&quot;../assets/2018-05-16-use-glustercloning-with-kubevirt/template.yml&quot;&gt;this openshift template&lt;/a&gt; which would create the following objects:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a persistent volume claim as a clone of an existing pvc (cirros by default)&lt;/li&gt;
  &lt;li&gt;an offline virtual machine object&lt;/li&gt;
  &lt;li&gt;additional services for ssh and http access&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;you can use it with something like&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;oc process -f template.yml -p Name=myvm | oc process -f - -n default
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;cloning features in the storage backend allow us to simply use a given set of pvcs as base os for the deployment of our vms. this feature is growing in gluster, worth giving it a try!&lt;/p&gt;
</description>
        <pubDate>Wed, 16 May 2018 00:00:00 -0400</pubDate>
        <pubYear>Wed, 31 Dec 1969 19:33:38 -0500</pubYear>
        <link>https://www.kubevirt.io//2018/Use-GlusterFS-Cloning-with-KubeVirt.html</link>
        <guid isPermaLink="true">https://www.kubevirt.io//2018/Use-GlusterFS-Cloning-with-KubeVirt.html</guid>
        
        
        <category>uncategorized</category>
        
      </item>
    
      <item>
        <title>Kubevirt Api Access Control</title>
        <description>&lt;p&gt;Access to KubeVirt resources are controlled entirely by Kubernete’s Resource
Based Access Control (RBAC) system. This system allows KubeVirt to tie directly
into the existing authentication and authorization mechanisms Kubernetes
already provides to its core api objects.&lt;/p&gt;

&lt;h2 id=&quot;kubevirt-rbac-role-basics&quot;&gt;KubeVirt RBAC Role Basics&lt;/h2&gt;

&lt;p&gt;Typically when people think of Kubernetes RBAC system, they’re thinking about
granting users access to create/delete kubernetes objects (like Pods,
deployments, etc), however those same RBAC mechanisms work naturally with
KubeVirt objects as well.&lt;/p&gt;

&lt;p&gt;When we look at KubeVirt’s objects, we can see they are structured just like
the objects that come predefined in the Kubernetes core.&lt;/p&gt;

&lt;p&gt;For example, look here’s an example of a VirtualMachine spec.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: kubevirt.io/v1alpha1
kind: VirtualMachine
metadata:
  name: vm-ephemeral
spec:
  domain:
    devices:
      disks:
      - disk:
          bus: virtio
        name: registrydisk
        volumeName: registryvolume
    resources:
      requests:
        memory: 64M
  volumes:
  - name: registryvolume
    registryDisk:
      image: kubevirt/cirros-registry-disk-demo:devel
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the spec above, we see the KubeVirt VirtualMachine object has an &lt;em&gt;apiVersion&lt;/em&gt;
field and a &lt;em&gt;kind&lt;/em&gt; field just like a Pod spec does. The &lt;strong&gt;kubevirt.io&lt;/strong&gt; portion
of the apiVersion field represents KubeVirt apiGroup the resource is a part of.
The &lt;strong&gt;kind&lt;/strong&gt; field reflects the resource type.&lt;/p&gt;

&lt;p&gt;Using that information, we can create an RBAC role that gives a user permission
to create, delete, and view all VirtualMachine objects.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: vm-access
  labels:
    kubevirt.io: &quot;&quot;
rules:
  - apiGroups:
      - kubevirt.io
    resources:
      - virtualmachines
    verbs:
      - get
      - delete
      - create
      - update
      - patch
      - list
      - watch
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This same logic can be applied when creating RBAC roles for other KubeVirt
objects as well. If we wanted to extend this RBAC role to grant similar
permissions for VirtualMachinePreset objects, we’d just have to add a second
resource kubevirt.io resource list. The result would look like this.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: vm-access
  labels:
    kubevirt.io: &quot;&quot;
rules:
  - apiGroups:
      - kubevirt.io
    resources:
      - virtualmachines
      - virtualmachinepresets
    verbs:
      - get
      - delete
      - create
      - update
      - patch
      - list
      - watch
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;kubevirt-subresource-rbac-roles&quot;&gt;KubeVirt Subresource RBAC Roles&lt;/h2&gt;

&lt;p&gt;Access to a VirtualMachines’s VNC and console stream using KubeVirt’s
&lt;strong&gt;virtctl&lt;/strong&gt; tool is managed by the Kubernetes RBAC system as well. Permissions
for these resources work slightly different than the other KubeVirt objects
though.&lt;/p&gt;

&lt;p&gt;Console and VNC access is performed using the KubeVirt Stream API, which has
its own api group called &lt;strong&gt;subresources.kubevirt.io&lt;/strong&gt;. Below is an example of
how to create a role that grants access to the VNC and console streams APIs.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: vm-vnc-access
  labels:
    kubevirt.io: &quot;&quot;
rules:
  - apiGroups:
      - subresources.kubevirt.io
    resources:
      - virtualmachines/console
      - virtualmachines/vnc
    verbs:
      - get
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;limiting-rbac-to-a-single-namespace&quot;&gt;Limiting RBAC To a Single Namespace.&lt;/h2&gt;

&lt;p&gt;A ClusterRole can be bound to a user in two different ways.&lt;/p&gt;

&lt;p&gt;When a ClusterRoleBinding is used, a user is permitted access to all resources
defined in the ClusterRole across all namespaces in the cluster.&lt;/p&gt;

&lt;p&gt;When a RoleBinding is used, a user is limited to accessing only the resources
defined in the ClusterRole within the namespace RoleBinding exists in.&lt;/p&gt;

&lt;h2 id=&quot;limiting-rbac-to-a-single-resource&quot;&gt;Limiting RBAC To a Single Resource.&lt;/h2&gt;

&lt;p&gt;A user can also be limit to accessing only a single resource within a resource
type. Below is an example that only grants VNC access to the VirtualMachine
named ‘bobs-vm’&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: vm-vnc-access
  labels:
    kubevirt.io: &quot;&quot;
rules:
  - apiGroups:
      - subresources.kubevirt.io
    resources:
      - virtualmachines/console
      - virtualmachines/vnc
    resourceName:
      - bobs-vm
    verbs:
      - get
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;default-kubevirt-rbac-roles&quot;&gt;Default Kubevirt RBAC Roles&lt;/h2&gt;

&lt;p&gt;The next release of KubeVirt is coming with three default ClusterRoles that
admins can use to grant users access to KubeVirt resources. In most cases,
these roles will prevent admins from ever having to create their own custom
KubeVirt RBAC roles.&lt;/p&gt;

&lt;p&gt;More information about these default roles can be found in the KubeVirt
user guide &lt;a href=&quot;https://www.kubevirt.io/user-guide/#/authorization&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 16 May 2018 00:00:00 -0400</pubDate>
        <pubYear>Wed, 31 Dec 1969 19:33:38 -0500</pubYear>
        <link>https://www.kubevirt.io//2018/KubeVirt-API-Access-Control.html</link>
        <guid isPermaLink="true">https://www.kubevirt.io//2018/KubeVirt-API-Access-Control.html</guid>
        
        
        <category>uncategorized</category>
        
      </item>
    
  </channel>
</rss>
